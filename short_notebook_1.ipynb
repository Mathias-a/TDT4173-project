{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.3.2-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting autogluon\n",
      "  Using cached autogluon-0.8.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.1.3-cp310-cp310-win_amd64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.26.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Collecting autogluon.core==0.8.2 (from autogluon.core[all]==0.8.2->autogluon)\n",
      "  Using cached autogluon.core-0.8.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting autogluon.features==0.8.2 (from autogluon)\n",
      "  Using cached autogluon.features-0.8.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting autogluon.tabular==0.8.2 (from autogluon.tabular[all]==0.8.2->autogluon)\n",
      "  Using cached autogluon.tabular-0.8.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting autogluon.multimodal==0.8.2 (from autogluon)\n",
      "  Using cached autogluon.multimodal-0.8.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting autogluon.timeseries==0.8.2 (from autogluon.timeseries[all]==0.8.2->autogluon)\n",
      "  Using cached autogluon.timeseries-0.8.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.2.2-cp310-cp310-win_amd64.whl (8.3 MB)\n",
      "Requirement already satisfied: networkx<4,>=3.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon) (3.2.1)\n",
      "Collecting pandas\n",
      "  Using cached pandas-1.5.3-cp310-cp310-win_amd64.whl (10.4 MB)\n",
      "Requirement already satisfied: tqdm<5,>=4.38 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon) (4.65.2)\n",
      "Requirement already satisfied: requests in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon) (2.28.2)\n",
      "Collecting matplotlib (from autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon)\n",
      "  Using cached matplotlib-3.8.1-cp310-cp310-win_amd64.whl.metadata (5.9 kB)\n",
      "Collecting boto3<2,>=1.10 (from autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon)\n",
      "  Using cached boto3-1.28.84-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting autogluon.common==0.8.2 (from autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon)\n",
      "  Using cached autogluon.common-0.8.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting hyperopt<0.2.8,>=0.2.7 (from autogluon.core[all]==0.8.2->autogluon)\n",
      "  Using cached hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting ray<2.4,>=2.3 (from ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon)\n",
      "  Using cached ray-2.3.1-cp310-cp310-win_amd64.whl (21.7 MB)\n",
      "Requirement already satisfied: pydantic<2.0,>=1.10.4 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (1.10.13)\n",
      "Requirement already satisfied: grpcio<=1.50.0,>=1.42.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (1.50.0)\n",
      "Requirement already satisfied: Pillow<9.6,>=9.3 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (9.5.0)\n",
      "Collecting jsonschema<4.18,>=4.14 (from autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached jsonschema-4.17.3-py3-none-any.whl (90 kB)\n",
      "Collecting seqeval<1.3.0,>=1.2.2 (from autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached seqeval-1.2.2-py3-none-any.whl\n",
      "Collecting evaluate<0.4.0,>=0.2.2 (from autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached evaluate-0.3.0-py3-none-any.whl (72 kB)\n",
      "Collecting accelerate<0.17,>=0.9 (from autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached accelerate-0.16.0-py3-none-any.whl (199 kB)\n",
      "Collecting timm<0.10.0,>=0.9.2 (from autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached timm-0.9.10-py3-none-any.whl.metadata (59 kB)\n",
      "Requirement already satisfied: torch<1.14,>=1.9 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (1.13.1)\n",
      "Collecting torchvision<0.15.0 (from autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached torchvision-0.14.1-cp310-cp310-win_amd64.whl (1.1 MB)\n",
      "Collecting scikit-image<0.20.0,>=0.19.1 (from autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached scikit_image-0.19.3-cp310-cp310-win_amd64.whl (12.0 MB)\n",
      "Collecting pytorch-lightning<1.10.0,>=1.9.0 (from autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached pytorch_lightning-1.9.5-py3-none-any.whl (829 kB)\n",
      "Requirement already satisfied: text-unidecode<1.4,>=1.3 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (1.3)\n",
      "Collecting torchmetrics<0.12.0,>=0.11.0 (from autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
      "Collecting transformers<4.27.0,>=4.23.0 (from transformers[sentencepiece]<4.27.0,>=4.23.0->autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "Collecting nptyping<2.5.0,>=1.4.4 (from autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached nptyping-2.4.1-py3-none-any.whl (36 kB)\n",
      "Collecting omegaconf<2.3.0,>=2.1.1 (from autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached omegaconf-2.2.3-py3-none-any.whl (79 kB)\n",
      "Collecting pytorch-metric-learning<2.0,>=1.3.0 (from autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached pytorch_metric_learning-1.7.3-py3-none-any.whl (112 kB)\n",
      "Collecting nlpaug<1.2.0,>=1.1.10 (from autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
      "Collecting nltk<4.0.0,>=3.4.5 (from autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting openmim<0.4.0,>=0.3.7 (from autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached openmim-0.3.9-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: defusedxml<0.7.2,>=0.7.1 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.7.1)\n",
      "Collecting jinja2<3.2,>=3.0.3 (from autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting tensorboard<3,>=2.9 (from autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached tensorboard-2.15.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: pytesseract<0.3.11,>=0.3.9 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.3.10)\n",
      "Collecting lightgbm<3.4,>=3.3 (from autogluon.tabular[all]==0.8.2->autogluon)\n",
      "  Using cached lightgbm-3.3.5-py3-none-win_amd64.whl (1.0 MB)\n",
      "Collecting xgboost<1.8,>=1.6 (from autogluon.tabular[all]==0.8.2->autogluon)\n",
      "  Using cached xgboost-1.7.6-py3-none-win_amd64.whl.metadata (1.9 kB)\n",
      "Collecting fastai<2.8,>=2.3.1 (from autogluon.tabular[all]==0.8.2->autogluon)\n",
      "  Using cached fastai-2.7.13-py3-none-any.whl.metadata (9.6 kB)\n",
      "Collecting catboost<1.3,>=1.1 (from autogluon.tabular[all]==0.8.2->autogluon)\n",
      "  Using cached catboost-1.2.2-cp310-cp310-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting statsmodels<0.15,>=0.13.0 (from autogluon.timeseries==0.8.2->autogluon.timeseries[all]==0.8.2->autogluon)\n",
      "  Using cached statsmodels-0.14.0-cp310-cp310-win_amd64.whl (9.2 MB)\n",
      "Collecting gluonts<0.14,>=0.13.1 (from autogluon.timeseries==0.8.2->autogluon.timeseries[all]==0.8.2->autogluon)\n",
      "  Using cached gluonts-0.13.7-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting statsforecast<1.5,>=1.4.0 (from autogluon.timeseries==0.8.2->autogluon.timeseries[all]==0.8.2->autogluon)\n",
      "  Using cached statsforecast-1.4.0-py3-none-any.whl (91 kB)\n",
      "Collecting mlforecast<0.7.4,>=0.7.0 (from autogluon.timeseries==0.8.2->autogluon.timeseries[all]==0.8.2->autogluon)\n",
      "  Using cached mlforecast-0.7.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: ujson<6,>=5 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from autogluon.timeseries==0.8.2->autogluon.timeseries[all]==0.8.2->autogluon) (5.8.0)\n",
      "Requirement already satisfied: psutil<6,>=5.7.3 in c:\\users\\simen\\appdata\\roaming\\python\\python310\\site-packages (from autogluon.common==0.8.2->autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon) (5.9.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from autogluon.common==0.8.2->autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon) (60.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\simen\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\simen\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\simen\\appdata\\roaming\\python\\python310\\site-packages (from accelerate<0.17,>=0.9->autogluon.multimodal==0.8.2->autogluon) (23.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate<0.17,>=0.9->autogluon.multimodal==0.8.2->autogluon) (6.0.1)\n",
      "Collecting botocore<1.32.0,>=1.31.84 (from boto3<2,>=1.10->autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon)\n",
      "  Using cached botocore-1.31.84-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from boto3<2,>=1.10->autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon) (0.10.0)\n",
      "Collecting s3transfer<0.8.0,>=0.7.0 (from boto3<2,>=1.10->autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon)\n",
      "  Using cached s3transfer-0.7.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: graphviz in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==0.8.2->autogluon) (0.20.1)\n",
      "Requirement already satisfied: plotly in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==0.8.2->autogluon) (5.18.0)\n",
      "Collecting datasets>=2.0.0 (from evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached datasets-2.14.6-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: dill in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (0.3.7)\n",
      "Requirement already satisfied: xxhash in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (3.4.1)\n",
      "Collecting multiprocess (from evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (2023.10.0)\n",
      "Collecting huggingface-hub>=0.7.0 (from evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached huggingface_hub-0.19.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting responses<0.19 (from evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: pip in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (23.3.1)\n",
      "Collecting fastdownload<2,>=0.0.5 (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon)\n",
      "  Using cached fastdownload-0.0.7-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: fastcore<1.6,>=1.5.29 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (1.5.29)\n",
      "Requirement already satisfied: fastprogress>=0.2.4 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (1.0.3)\n",
      "Collecting spacy<4 (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon)\n",
      "  Using cached spacy-3.7.2-cp310-cp310-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: toolz~=0.10 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gluonts<0.14,>=0.13.1->autogluon.timeseries==0.8.2->autogluon.timeseries[all]==0.8.2->autogluon) (0.12.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gluonts<0.14,>=0.13.1->autogluon.timeseries==0.8.2->autogluon.timeseries[all]==0.8.2->autogluon) (4.8.0)\n",
      "Requirement already satisfied: future in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==0.8.2->autogluon) (0.18.2)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==0.8.2->autogluon) (3.0.0)\n",
      "Requirement already satisfied: py4j in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==0.8.2->autogluon) (0.10.9.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2<3.2,>=3.0.3->autogluon.multimodal==0.8.2->autogluon) (2.1.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema<4.18,>=4.14->autogluon.multimodal==0.8.2->autogluon) (23.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema<4.18,>=4.14->autogluon.multimodal==0.8.2->autogluon) (0.20.0)\n",
      "Requirement already satisfied: wheel in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightgbm<3.4,>=3.3->autogluon.tabular[all]==0.8.2->autogluon) (0.41.3)\n",
      "Collecting numba (from mlforecast<0.7.4,>=0.7.0->autogluon.timeseries==0.8.2->autogluon.timeseries[all]==0.8.2->autogluon)\n",
      "  Using cached numba-0.58.1-cp310-cp310-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting window-ops (from mlforecast<0.7.4,>=0.7.0->autogluon.timeseries==0.8.2->autogluon.timeseries[all]==0.8.2->autogluon)\n",
      "  Using cached window_ops-0.0.14-py3-none-any.whl (14 kB)\n",
      "Collecting gdown>=4.0.0 (from nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached gdown-4.7.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: click in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk<4.0.0,>=3.4.5->autogluon.multimodal==0.8.2->autogluon) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk<4.0.0,>=3.4.5->autogluon.multimodal==0.8.2->autogluon) (2023.10.3)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from omegaconf<2.3.0,>=2.1.1->autogluon.multimodal==0.8.2->autogluon) (4.9.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\simen\\appdata\\roaming\\python\\python310\\site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (0.4.6)\n",
      "Collecting model-index (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached model_index-0.1.11-py3-none-any.whl (34 kB)\n",
      "Collecting opendatalab (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached opendatalab-0.0.10-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting rich (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached rich-13.6.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: tabulate in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (0.9.0)\n",
      "Collecting lightning-utilities>=0.6.0.post0 (from pytorch-lightning<1.10.0,>=1.9.0->autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached lightning_utilities-0.9.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ray<2.4,>=2.3->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (3.13.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ray<2.4,>=2.3->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (1.0.7)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ray<2.4,>=2.3->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (3.20.2)\n",
      "Collecting aiosignal (from ray<2.4,>=2.3->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: frozenlist in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ray<2.4,>=2.3->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (1.4.0)\n",
      "Requirement already satisfied: virtualenv>=20.0.24 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ray<2.4,>=2.3->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (20.24.6)\n",
      "Collecting aiohttp>=3.7 (from ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon)\n",
      "  Using cached aiohttp-3.8.6-cp310-cp310-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting aiohttp-cors (from ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon)\n",
      "  Using cached aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: colorful in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (0.5.5)\n",
      "Requirement already satisfied: py-spy>=0.2.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (0.3.14)\n",
      "Collecting gpustat>=1.0.0 (from ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon)\n",
      "  Using cached gpustat-1.1.1-py3-none-any.whl\n",
      "Collecting opencensus (from ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon)\n",
      "  Using cached opencensus-0.11.3-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: prometheus-client>=0.7.1 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (0.18.0)\n",
      "Requirement already satisfied: smart-open in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (6.4.0)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ray[tune]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (2.6.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon) (2023.7.22)\n",
      "Collecting imageio>=2.4.1 (from scikit-image<0.20.0,>=0.19.1->autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached imageio-2.32.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image<0.20.0,>=0.19.1->autogluon.multimodal==0.8.2->autogluon) (2023.9.26)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image<0.20.0,>=0.19.1->autogluon.multimodal==0.8.2->autogluon) (1.4.1)\n",
      "Collecting patsy>=0.5.2 (from statsmodels<0.15,>=0.13.0->autogluon.timeseries==0.8.2->autogluon.timeseries[all]==0.8.2->autogluon)\n",
      "  Using cached patsy-0.5.3-py2.py3-none-any.whl (233 kB)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (2.0.0)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached google_auth-2.23.4-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached google_auth_oauthlib-1.1.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (3.5.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (3.0.1)\n",
      "Requirement already satisfied: safetensors in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from timm<0.10.0,>=0.9.2->autogluon.multimodal==0.8.2->autogluon) (0.4.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<4.27.0,>=4.23.0->transformers[sentencepiece]<4.27.0,>=4.23.0->autogluon.multimodal==0.8.2->autogluon) (0.13.3)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[sentencepiece]<4.27.0,>=4.23.0->autogluon.multimodal==0.8.2->autogluon) (0.1.99)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon)\n",
      "  Using cached contourpy-1.2.0-cp310-cp310-win_amd64.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon) (4.44.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon) (3.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp>=3.7->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp>=3.7->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp>=3.7->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (1.9.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets>=2.0.0->evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (14.0.1)\n",
      "Collecting beautifulsoup4 (from gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (4.9)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: nvidia-ml-py>=11.450.129 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gpustat>=1.0.0->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (12.535.133)\n",
      "Collecting blessed>=1.17.1 (from gpustat>=1.0.0->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon)\n",
      "  Using cached blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from numba->mlforecast<0.7.4,>=0.7.0->autogluon.timeseries==0.8.2->autogluon.timeseries[all]==0.8.2->autogluon) (0.41.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (3.0.9)\n",
      "Collecting thinc<8.3.0,>=8.1.8 (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon)\n",
      "  Using cached thinc-8.2.1-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (2.0.10)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon)\n",
      "  Using cached weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (0.9.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (3.3.0)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from virtualenv>=20.0.24->ray<2.4,>=2.3->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (0.3.7)\n",
      "Requirement already satisfied: platformdirs<4,>=3.9.1 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from virtualenv>=20.0.24->ray<2.4,>=2.3->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (3.11.0)\n",
      "Requirement already satisfied: ordered-set in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from model-index->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (4.1.0)\n",
      "Requirement already satisfied: opencensus-context>=0.1.3 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opencensus->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (0.1.3)\n",
      "Collecting google-api-core<3.0.0,>=1.0.0 (from opencensus->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon)\n",
      "  Using cached google_api_core-2.14.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: pycryptodome in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (3.19.0)\n",
      "Collecting openxlab (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached openxlab-0.0.28-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: pywin32 in c:\\users\\simen\\appdata\\roaming\\python\\python310\\site-packages (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (306)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from plotly->catboost<1.3,>=1.1->autogluon.tabular[all]==0.8.2->autogluon) (8.2.3)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\simen\\appdata\\roaming\\python\\python310\\site-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (2.16.1)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in c:\\users\\simen\\appdata\\roaming\\python\\python310\\site-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (0.2.9)\n",
      "Requirement already satisfied: jinxed>=1.1.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (1.2.0)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon)\n",
      "  Using cached googleapis_common_protos-1.61.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (3.2.2)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.1.8->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon)\n",
      "  Using cached blis-0.7.11-cp310-cp310-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.1.8->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon)\n",
      "  Using cached confection-0.1.3-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon)\n",
      "  Using cached cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==0.8.2->autogluon) (2.5)\n",
      "Collecting oss2~=2.17.0 (from openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached oss2-2.17.0-py3-none-any.whl\n",
      "Collecting rich (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached rich-13.4.2-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests[socks]->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==0.8.2->autogluon) (1.7.1)\n",
      "Requirement already satisfied: ansicon in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinxed>=1.1.0->blessed>=1.17.1->gpustat>=1.0.0->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (1.89.0)\n",
      "Requirement already satisfied: crcmod>=1.7 in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (1.7)\n",
      "Collecting aliyun-python-sdk-kms>=2.4.1 (from oss2~=2.17.0->openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached aliyun_python_sdk_kms-2.16.2-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting aliyun-python-sdk-core>=2.13.12 (from oss2~=2.17.0->openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached aliyun_python_sdk_core-2.14.0-py3-none-any.whl\n",
      "Collecting cryptography>=2.6.0 (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached cryptography-41.0.5-cp37-abi3-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting cffi>=1.12 (from cryptography>=2.6.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon)\n",
      "  Using cached cffi-1.16.0-cp310-cp310-win_amd64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\simen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cffi>=1.12->cryptography>=2.6.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (2.21)\n",
      "Using cached autogluon-0.8.2-py3-none-any.whl (9.7 kB)\n",
      "Using cached autogluon.core-0.8.2-py3-none-any.whl (224 kB)\n",
      "Using cached autogluon.features-0.8.2-py3-none-any.whl (62 kB)\n",
      "Using cached autogluon.multimodal-0.8.2-py3-none-any.whl (372 kB)\n",
      "Using cached autogluon.tabular-0.8.2-py3-none-any.whl (285 kB)\n",
      "Using cached autogluon.timeseries-0.8.2-py3-none-any.whl (116 kB)\n",
      "Using cached autogluon.common-0.8.2-py3-none-any.whl (61 kB)\n",
      "Using cached boto3-1.28.84-py3-none-any.whl (135 kB)\n",
      "Using cached catboost-1.2.2-cp310-cp310-win_amd64.whl (101.0 MB)\n",
      "Using cached fastai-2.7.13-py3-none-any.whl (232 kB)\n",
      "Using cached gluonts-0.13.7-py3-none-any.whl (1.5 MB)\n",
      "Using cached mlforecast-0.7.3-py3-none-any.whl (43 kB)\n",
      "Using cached openmim-0.3.9-py2.py3-none-any.whl (52 kB)\n",
      "Using cached tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
      "Using cached timm-0.9.10-py3-none-any.whl (2.2 MB)\n",
      "Using cached xgboost-1.7.6-py3-none-win_amd64.whl (70.9 MB)\n",
      "Using cached matplotlib-3.8.1-cp310-cp310-win_amd64.whl (7.6 MB)\n",
      "Using cached aiohttp-3.8.6-cp310-cp310-win_amd64.whl (325 kB)\n",
      "Using cached botocore-1.31.84-py3-none-any.whl (11.3 MB)\n",
      "Using cached contourpy-1.2.0-cp310-cp310-win_amd64.whl (186 kB)\n",
      "Using cached datasets-2.14.6-py3-none-any.whl (493 kB)\n",
      "Using cached google_auth-2.23.4-py2.py3-none-any.whl (183 kB)\n",
      "Using cached google_auth_oauthlib-1.1.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached huggingface_hub-0.19.0-py3-none-any.whl (311 kB)\n",
      "Using cached imageio-2.32.0-py3-none-any.whl (313 kB)\n",
      "Using cached lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
      "Using cached numba-0.58.1-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "Using cached s3transfer-0.7.0-py3-none-any.whl (79 kB)\n",
      "Using cached spacy-3.7.2-cp310-cp310-win_amd64.whl (12.1 MB)\n",
      "Using cached multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "Using cached opencensus-0.11.3-py2.py3-none-any.whl (128 kB)\n",
      "Using cached opendatalab-0.0.10-py3-none-any.whl (29 kB)\n",
      "Using cached google_api_core-2.14.0-py3-none-any.whl (122 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached thinc-8.2.1-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "Using cached weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "Using cached openxlab-0.0.28-py3-none-any.whl (297 kB)\n",
      "Using cached rich-13.4.2-py3-none-any.whl (239 kB)\n",
      "Using cached blis-0.7.11-cp310-cp310-win_amd64.whl (6.6 MB)\n",
      "Using cached cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "Using cached confection-0.1.3-py3-none-any.whl (34 kB)\n",
      "Using cached googleapis_common_protos-1.61.0-py2.py3-none-any.whl (230 kB)\n",
      "Using cached aliyun_python_sdk_kms-2.16.2-py2.py3-none-any.whl (94 kB)\n",
      "Using cached cryptography-41.0.5-cp37-abi3-win_amd64.whl (2.7 MB)\n",
      "Using cached cffi-1.16.0-cp310-cp310-win_amd64.whl (181 kB)\n",
      "Installing collected packages: patsy, omegaconf, numba, nptyping, multiprocess, markdown-it-py, lightning-utilities, jsonschema, jinja2, imageio, googleapis-common-protos, contourpy, cloudpathlib, cffi, blis, beautifulsoup4, aiosignal, xgboost, window-ops, torchvision, torchmetrics, scikit-learn, scikit-image, rich, responses, requests-oauthlib, ray, pandas, nltk, model-index, matplotlib, hyperopt, huggingface-hub, google-auth, fastdownload, cryptography, confection, botocore, blessed, aiohttp, accelerate, weasel, transformers, timm, thinc, statsmodels, seqeval, s3transfer, pytorch-metric-learning, mlforecast, lightgbm, gpustat, google-auth-oauthlib, google-api-core, gluonts, gdown, catboost, aliyun-python-sdk-core, aiohttp-cors, tensorboard, statsforecast, spacy, pytorch-lightning, opencensus, nlpaug, datasets, boto3, aliyun-python-sdk-kms, oss2, fastai, evaluate, autogluon.common, openxlab, autogluon.features, autogluon.core, opendatalab, autogluon.tabular, openmim, autogluon.timeseries, autogluon.multimodal, autogluon\n",
      "Successfully installed accelerate-0.16.0 aiohttp-3.8.6 aiohttp-cors-0.7.0 aiosignal-1.3.1 aliyun-python-sdk-core-2.14.0 aliyun-python-sdk-kms-2.16.2 autogluon-0.8.2 autogluon.common-0.8.2 autogluon.core-0.8.2 autogluon.features-0.8.2 autogluon.multimodal-0.8.2 autogluon.tabular-0.8.2 autogluon.timeseries-0.8.2 beautifulsoup4-4.12.2 blessed-1.20.0 blis-0.7.11 boto3-1.28.84 botocore-1.31.84 catboost-1.2.2 cffi-1.16.0 cloudpathlib-0.16.0 confection-0.1.3 contourpy-1.2.0 cryptography-41.0.5 datasets-2.14.6 evaluate-0.3.0 fastai-2.7.13 fastdownload-0.0.7 gdown-4.7.1 gluonts-0.13.7 google-api-core-2.14.0 google-auth-2.23.4 google-auth-oauthlib-1.1.0 googleapis-common-protos-1.61.0 gpustat-1.1.1 huggingface-hub-0.19.0 hyperopt-0.2.7 imageio-2.32.0 jinja2-3.1.2 jsonschema-4.17.3 lightgbm-3.3.5 lightning-utilities-0.9.0 markdown-it-py-3.0.0 matplotlib-3.8.1 mlforecast-0.7.3 model-index-0.1.11 multiprocess-0.70.15 nlpaug-1.1.11 nltk-3.8.1 nptyping-2.4.1 numba-0.58.1 omegaconf-2.2.3 opencensus-0.11.3 opendatalab-0.0.10 openmim-0.3.9 openxlab-0.0.28 oss2-2.17.0 pandas-1.5.3 patsy-0.5.3 pytorch-lightning-1.9.5 pytorch-metric-learning-1.7.3 ray-2.3.1 requests-oauthlib-1.3.1 responses-0.18.0 rich-13.4.2 s3transfer-0.7.0 scikit-image-0.19.3 scikit-learn-1.2.2 seqeval-1.2.2 spacy-3.7.2 statsforecast-1.4.0 statsmodels-0.14.0 tensorboard-2.15.1 thinc-8.2.1 timm-0.9.10 torchmetrics-0.11.4 torchvision-0.14.1 transformers-4.26.1 weasel-0.3.4 window-ops-0.0.14 xgboost-1.7.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn autogluon pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "from autogluon.tabular import TabularPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constans and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = [\"A\", \"B\", \"C\"]\n",
    "features_order = []\n",
    "\n",
    "LAGGED_COLUMNS_TO_KEEP = [\n",
    "    # 'direct_rad:W_lag_1h', \n",
    "    'direct_rad:W_lag_forward_1h', \n",
    "    # 'clear_sky_rad:W_lag_1h', \n",
    "    'clear_sky_rad:W_lag_forward_1h', \n",
    "    # 'diffuse_rad:W_lag_1h', \n",
    "    'diffuse_rad:W_lag_forward_1h', \n",
    "    # 'direct_rad_1h:J_lag_1h', \n",
    "    'direct_rad_1h:J_lag_forward_1h', \n",
    "    # 'is_in_shadow:idx_lag_1h', \n",
    "    'is_in_shadow:idx_lag_forward_1h', \n",
    "    # 'clear_sky_energy_1h:J_lag_1h', \n",
    "    'clear_sky_energy_1h:J_lag_forward_1h', \n",
    "    # 'effective_cloud_cover:p_lag_1h', \n",
    "    'effective_cloud_cover:p_lag_forward_1h', \n",
    "    # 'visibility:m_lag_1h', \n",
    "    'visibility:m_lag_forward_1h', \n",
    "    # 'total_cloud_cover:p_lag_1h', \n",
    "    'total_cloud_cover:p_lag_forward_1h', \n",
    "\n",
    "\n",
    "    # 'direct_rad:W_lag_2h', \n",
    "    # 'direct_rad:W_lag_forward_2h', \n",
    "    # 'clear_sky_rad:W_lag_2h', \n",
    "    # 'clear_sky_rad:W_lag_forward_2h', \n",
    "    # 'diffuse_rad:W_lag_2h', \n",
    "    # 'diffuse_rad:W_lag_forward_2h', \n",
    "    # 'direct_rad_1h:J_lag_2h', \n",
    "    # 'direct_rad_1h:J_lag_forward_2h', \n",
    "    # 'is_in_shadow:idx_lag_2h', \n",
    "    # 'is_in_shadow:idx_lag_forward_2h', \n",
    "    # 'clear_sky_energy_1h:J_lag_2h', \n",
    "    # 'clear_sky_energy_1h:J_lag_forward_2h', \n",
    "    # 'effective_cloud_cover:p_lag_2h', \n",
    "    # 'effective_cloud_cover:p_lag_forward_2h', \n",
    "    # 'visibility:m_lag_2h', \n",
    "    # 'visibility:m_lag_forward_2h', \n",
    "    # 'total_cloud_cover:p_lag_2h', \n",
    "    # 'total_cloud_cover:p_lag_forward_2h', \n",
    "\n",
    "    # 'direct_rad:W_lag_3h', \n",
    "    # 'direct_rad:W_lag_forward_3h', \n",
    "    # 'clear_sky_rad:W_lag_3h', \n",
    "    # 'clear_sky_rad:W_lag_forward_3h', \n",
    "    # 'diffuse_rad:W_lag_3h', \n",
    "    # 'diffuse_rad:W_lag_forward_3h', \n",
    "    # 'direct_rad_1h:J_lag_3h', \n",
    "    # 'direct_rad_1h:J_lag_forward_3h', \n",
    "    # 'is_in_shadow:idx_lag_3h', \n",
    "    # 'is_in_shadow:idx_lag_forward_3h', \n",
    "    # 'clear_sky_energy_1h:J_lag_3h', \n",
    "    # 'clear_sky_energy_1h:J_lag_forward_3h', \n",
    "    # 'effective_cloud_cover:p_lag_3h', \n",
    "    # 'effective_cloud_cover:p_lag_forward_3h', \n",
    "    # 'visibility:m_lag_3h', \n",
    "    # 'visibility:m_lag_forward_3h', \n",
    "    # 'total_cloud_cover:p_lag_3h', \n",
    "    # 'total_cloud_cover:p_lag_forward_3h'\n",
    "]\n",
    "\n",
    "CUSTOM_COLUMNS_TO_KEEP = [\n",
    "    \"hour_cos\",\n",
    "    \"hour_sin\",\n",
    "    \"month_sin\",\n",
    "    \"month_cos\",\n",
    "    \"day-of-year\",\n",
    "    \"hours_since_forecast\"\n",
    "]\n",
    "\n",
    "WEATHER_FEATURES = [\n",
    "    \"direct_rad:W\",\n",
    "    \"clear_sky_rad:W\",\n",
    "    \"diffuse_rad:W\",\n",
    "    \"direct_rad_1h:J\",\n",
    "    \"is_in_shadow:idx\",\n",
    "    \"clear_sky_energy_1h:J\",\n",
    "    \"effective_cloud_cover:p\",\n",
    "    \"visibility:m\",\n",
    "    \"total_cloud_cover:p\",\n",
    "]\n",
    "\n",
    "\n",
    "TEST_COLUMNS_TO_KEEP = [\n",
    "    \"direct_rad:W\",\n",
    "    \"clear_sky_rad:W\",\n",
    "    \"diffuse_rad:W\",\n",
    "    \"direct_rad_1h:J\",\n",
    "    \"is_in_shadow:idx\",\n",
    "    \"clear_sky_energy_1h:J\",\n",
    "    \"diffuse_rad_1h:J\",\n",
    "    \"is_day:idx\",\n",
    "    \"sun_elevation:d\",\n",
    "    \"ceiling_height_agl:m\",\n",
    "    \"effective_cloud_cover:p\",\n",
    "    \"visibility:m\",\n",
    "    \"total_cloud_cover:p\",\n",
    "    \"air_density_2m:kgm3\",\n",
    "    \"wind_speed_v_10m:ms\",\n",
    "    \"dew_point_2m:K\",\n",
    "    \"wind_speed_u_10m:ms\",\n",
    "    \"t_1000hPa:K\",\n",
    "    \"absolute_humidity_2m:gm3\",\n",
    "    \"snow_water:kgm2\",\n",
    "    \"relative_humidity_1000hPa:p\",\n",
    "    \"fresh_snow_24h:cm\",\n",
    "    \"cloud_base_agl:m\",\n",
    "    \"fresh_snow_12h:cm\",\n",
    "    \"snow_depth:cm\",\n",
    "    \"dew_or_rime:idx\",\n",
    "    \"fresh_snow_6h:cm\",\n",
    "    \"super_cooled_liquid_water:kgm2\",\n",
    "    \"fresh_snow_3h:cm\",\n",
    "    \"rain_water:kgm2\",\n",
    "    \"precip_type_5min:idx\",\n",
    "    \"precip_5min:mm\",\n",
    "    \"fresh_snow_1h:cm\",\n",
    "    \"sun_azimuth:d\",\n",
    "    \"msl_pressure:hPa\",\n",
    "    \"pressure_100m:hPa\",\n",
    "    \"pressure_50m:hPa\",\n",
    "    \"sfc_pressure:hPa\",\n",
    "    \"prob_rime:p\",\n",
    "    \"wind_speed_10m:ms\",\n",
    "    # \"elevation:m\",\n",
    "    # \"snow_density:kgm3\",\n",
    "    # \"snow_drift:idx\",\n",
    "    \"snow_melt_10min:mm\",\n",
    "    \"wind_speed_w_1000hPa:ms\",\n",
    "    \"observed_or_estimated\"\n",
    "    # \"location_A\",\n",
    "    # \"location_B\",\n",
    "    # \"location_C\",\n",
    "    # \"date_calc\",\n",
    "] + CUSTOM_COLUMNS_TO_KEEP  +  LAGGED_COLUMNS_TO_KEEP\n",
    "\n",
    "COLUMNS_TO_KEEP = TEST_COLUMNS_TO_KEEP + [\"pv_measurement\"]\n",
    "\n",
    "\n",
    "\n",
    "def create_weather_lagged_features(df, weather_features):\n",
    "    # Choose the weather features for which you want to create lagged versions\n",
    "    for feature in weather_features:\n",
    "        # Assuming hourly data, adjust the lags for your specific dataset\n",
    "        # Creating lagged features for 1 hour, 1 day, and 1 week\n",
    "        # df[f'{feature}_lag_1h'] = df[feature].shift(1)\n",
    "        # df[f'{feature}_lag_2h'] = df[feature].shift(2)\n",
    "        # df[f'{feature}_lag_3h'] = df[feature].shift(3)\n",
    "\n",
    "        df[f'{feature}_lag_forward_1h'] = df[feature].shift(-1)\n",
    "        # df[f'{feature}_lag_forward_2h'] = df[feature].shift(-2)\n",
    "        # df[f'{feature}_lag_forward_3h'] = df[feature].shift(-3)\n",
    "        # df[f'{feature}_lag_24h'] = df[feature].shift(24*4)\n",
    "        # df[f'{feature}_lag_168h'] = df[feature].shift(24 * 7 * 4 * 365)\n",
    "        # df[f'{feature}_front_lag_1h'] = df[feature].shift(-4)\n",
    "        # df[f'{feature}_front_lag_24h'] = df[feature].shift(-24*4)\n",
    "\n",
    "\n",
    "    # Handling edges by filling NaNs with appropriate values or dropping them\n",
    "    # You may choose to fill with zeroes or interpolate, based on what makes more sense for your data\n",
    "    # df.fillna(method='ffill', inplace=True)  # Forward fill  # Autogluon should handle this for us.\n",
    "    # df.fillna(method='bfill', inplace=True)  # Backward fill  # Autogluon should handle this for us.\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "B_SCALE_VALUE = 6.3\n",
    "C_SCALE_VALUE = 8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_custom_fields(df):\n",
    "     df['hour_sin'] = np.sin(2 * np.pi * df['date_forecast'].dt.hour / 24)\n",
    "     df['hour_cos'] = np.cos(2 * np.pi * df['date_forecast'].dt.hour / 24)\n",
    "\n",
    "     df['month_sin'] = np.sin(2 * np.pi * df['date_forecast'].dt.month / 12)\n",
    "     df['month_cos'] = np.cos(2 * np.pi * df['date_forecast'].dt.month / 12)\n",
    "     df['day-of-year'] = df['date_forecast'].dt.dayofyear\n",
    "     return df\n",
    "\n",
    "def add_calc_date(df_observed, df_estimated, df_test):\n",
    "    # Function to calculate the difference in hours\n",
    "    def calculate_hour_difference(row):\n",
    "        diff = row['date_calc'] - row['date_forecast']\n",
    "        return diff.total_seconds() / 3600  # Convert difference to hours\n",
    "\n",
    "    # Apply the function to calculate the hour difference for df_estimated and df_test\n",
    "    df_estimated['hours_since_forecast'] = df_estimated.apply(calculate_hour_difference, axis=1)\n",
    "    df_test['hours_since_forecast'] = df_test.apply(calculate_hour_difference, axis=1)\n",
    "\n",
    "    # Fill in zero for df_observed\n",
    "    df_observed['hours_since_forecast'] = 0\n",
    "\n",
    "    return df_observed, df_estimated, df_test\n",
    "\n",
    "\n",
    "def remove_outliers(df):\n",
    "    # Use a mask to filter out the rows where rolling std is zero but keep the rows where the value itself is zero\n",
    "    # Because some places in the data, the pv-measurements are messed up and are repeating.\n",
    "    mask = (df['pv_measurement'].rolling(2).std() == 0) & (df['pv_measurement'] != 0)\n",
    "    df[mask] = np.NaN  # Put this to NaN and hope autoGluon Handles.\n",
    "    return df\n",
    "\n",
    "def resample_add_data(df, is_test_data):\n",
    "    df = add_custom_fields(df)\n",
    "    df.set_index('date_forecast', inplace=True)\n",
    "    df = df.resample('1H').mean()\n",
    "    \n",
    "    # Remove empty dates if test data\n",
    "    if is_test_data:\n",
    "        non_nan_threshold = len(df.columns) // 2\n",
    "        df.dropna(thresh=non_nan_threshold, inplace=True)\n",
    "\n",
    "    # df.interpolate(method=\"linear\", inplace=True)  # Autogluon should handle this for us.\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_location_feature(X, location):\n",
    "      # Treat location as a categorical feature by converting it to a category type\n",
    "    X['location'] = location\n",
    "    X['location'] = X['location'].astype(str)  # Convert to string if 'location' is not an int\n",
    "    # X['dew_or_rime:idx'] = X['dew_or_rime:idx'].astype(str)\n",
    "    # X['is_day:idx'] = X['is_day:idx'].astype(str)\n",
    "    # X['is_in_shadow:idx'] = X['is_in_shadow:idx'].astype(str)\n",
    "    # categorical_columns = ['location', 'dew_or_rime:idx', 'is_day:idx', 'is_in_shadow:idx']\n",
    "\n",
    "    # # Before filling NaN values, add 'missing' as a category for each categorical column.\n",
    "    # for column in categorical_columns:\n",
    "    #     X[column] = X[column].astype('category')  # Ensure the column is of type 'category'.\n",
    "    #     if 'missing' not in X[column].cat.categories:\n",
    "    #         X[column] = X[column].cat.add_categories(['missing'])  # Add 'missing' as a new category.\n",
    "    #     X[column] = X[column].fillna('missing')\n",
    "    # X['location'] = X['location'].astype('category')\n",
    "    # X['dew_or_rime:idx'] = X['dew_or_rime:idx'].astype('category')\n",
    "    # X['is_day:idx'] = X['is_day:idx'].astype('category')\n",
    "    # X['is_in_shadow:idx'] = X['is_in_shadow:idx'].astype('category')\n",
    "    \n",
    "    return X\n",
    "\n",
    "def add_scaling(X_test, X_training, location):\n",
    "    global scalers\n",
    "    continuous_columns = X_training.select_dtypes(include=['float32', 'int32']).columns\n",
    "    if location not in scalers:\n",
    "        scalers[location] = MinMaxScaler()\n",
    "    X_training[continuous_columns] = scalers[location].fit_transform(X_training[continuous_columns])\n",
    "    \n",
    "    X_test[continuous_columns] = scalers[location].transform(X_test[continuous_columns])\n",
    "\n",
    "    return X_test, X_training\n",
    "\n",
    "# Skip this as we have hours since forecast as a feature.\n",
    "# Deprecated as the concat is moved to main function.\n",
    "def make_observed_and_estimated_category(df_observed, df_estimated, df_test):\n",
    "     # Hot encode in wether observed or estimated\n",
    "    df_observed['observed_or_estimated'] = 'observed'\n",
    "    df_estimated['observed_or_estimated'] = 'estimated'\n",
    "    df_test['observed_or_estimated'] = 'estimated'\n",
    "    # Concatenate observed and estimated\n",
    "    df_training = pd.concat([df_observed, df_estimated], axis=0).sort_values(by=\"date_forecast\")\n",
    "    df_training['observed_or_estimated'] = df_training['observed_or_estimated'].astype('category')\n",
    "    df_test['observed_or_estimated'] = df_test['observed_or_estimated'].astype('category')\n",
    "\n",
    "    return df_training, df_test\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize a dictionary to hold the scalers for each location\n",
    "\n",
    "scalers = {}\n",
    "\n",
    "\n",
    "def prepare_data(location):\n",
    "    # Load data\n",
    "    scaling = False  # Set scaling to True to enable individual scaling for each location\n",
    "    global scalers\n",
    "    global scale_target \n",
    "    scale_target = False\n",
    "\n",
    "    # Load training data\n",
    "    df_observed = pd.read_parquet(f\"data/{location}/X_train_observed.parquet\")\n",
    "    df_estimated = pd.read_parquet(f\"data/{location}/X_train_estimated.parquet\")\n",
    "    df_target = pd.read_parquet(f\"data/{location}/train_targets.parquet\")\n",
    "    # drop nan values in target data, pv measurement\n",
    "    df_target.dropna(inplace=True)\n",
    "\n",
    "   \n",
    "    # Load test data\n",
    "    df_test = pd.read_parquet(f\"data/{location}/X_test_estimated.parquet\")\n",
    "\n",
    "   \n",
    "    # Add calculated date\n",
    "    df_observed, df_estimated, df_test = add_calc_date(df_observed, df_estimated, df_test)\n",
    "    \n",
    "\n",
    "    # Resample and add custom fields\n",
    "\n",
    "    df_observed = resample_add_data(df_observed, False)\n",
    "    df_estimated = resample_add_data(df_estimated, False)\n",
    "    df_test = resample_add_data(df_test, True)\n",
    "\n",
    "    df_training, df_test = make_observed_and_estimated_category(df_observed, df_estimated, df_test)\n",
    "    \n",
    "\n",
    "    # Autogluon should scale for us.\n",
    "    if scale_target:\n",
    "        if location == \"B\":\n",
    "            df_target[\"pv_measurement\"] = df_target[\"pv_measurement\"] * B_SCALE_VALUE\n",
    "        elif location == \"C\":\n",
    "            df_target[\"pv_measurement\"] = df_target[\"pv_measurement\"] * C_SCALE_VALUE\n",
    "    \n",
    "    # Merge training with target data\n",
    "    df_training = pd.merge(df_training, df_target, left_on=\"date_forecast\", right_on=\"time\", how=\"inner\")\n",
    "    \n",
    "    # Create lagged features and remove outliers training\n",
    "    df_training = create_weather_lagged_features(df_training, WEATHER_FEATURES)\n",
    "    df_training = df_training[COLUMNS_TO_KEEP]\n",
    "    df_training = remove_outliers(df_training)\n",
    "\n",
    "    df_test = create_weather_lagged_features(df_test, WEATHER_FEATURES)\n",
    "    df_test = df_test[TEST_COLUMNS_TO_KEEP]\n",
    "\n",
    "\n",
    "    # Add categories\n",
    "    df_training = add_location_feature(df_training, location)\n",
    "    X_test = add_location_feature(df_test, location)\n",
    "    \n",
    "    # Add scaling\n",
    "    if scaling:\n",
    "        X_test, X_training = add_scaling(X_test, X_training, location)\n",
    "    df_test.reset_index(inplace=True)\n",
    "    df_test.drop(columns=[\"date_forecast\"], inplace=True)\n",
    "    # y_training = np.log1p(y_training)\n",
    "    return df_training, X_test\n",
    "\n",
    "\n",
    "\n",
    "# Use prepare_data function\n",
    "\n",
    "combined_df_train = []\n",
    "combined_df_test = []\n",
    "combined_df_validation = []\n",
    "\n",
    "for location in locations:\n",
    "    # Prepare the training data\n",
    "    X_training, X_test = prepare_data(location)\n",
    "\n",
    "    df_training, df_test = prepare_data(location)\n",
    "    # split df training into training and validation, with validation being only estimated data\n",
    "    X_training, X_validation = train_test_split(df_training[df_training[\"observed_or_estimated\"] == \"estimated\"], test_size=1440, shuffle=True)\n",
    "    X_training = pd.concat([X_training, df_training[df_training[\"observed_or_estimated\"] == \"observed\"]])\n",
    "    X_training = shuffle(X_training, random_state=420)\n",
    "    \n",
    "    combined_df_train.append(X_training)\n",
    "    combined_df_validation.append(X_validation)\n",
    "\n",
    "    combined_df_test.append(X_test)\n",
    "\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbmXT = {'learning_rate': 0.05, 'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}\n",
    "r51 = {'layers': [200, 100, 50],\n",
    "     'emb_drop': 0.6046989241462619,\n",
    "     'ps': 0.09244767444160731,\n",
    "     'bs': 1024,\n",
    "     'lr': 0.00775309042164966,\n",
    "     'epochs': 48,\n",
    "     'early.stopping.min_delta': 0.0001,\n",
    "     'early.stopping.patience': 20,\n",
    "     'smoothing': 0.0, 'ag_args': {'name_suffix': '_r51'}}\n",
    "\n",
    "r118 = {'learning_rate': 0.021720607471727896,\n",
    "     'extra_trees': True,\n",
    "     'feature_fraction': 0.7832570544199176,\n",
    "     'min_data_in_leaf': 3,\n",
    "     'num_leaves': 21}\n",
    "\n",
    "rf_r5 = {'n_estimators': 300,\n",
    "     'max_leaf_nodes': 50000,\n",
    "     'n_jobs': -1,\n",
    "     'random_state': 0,\n",
    "     'bootstrap': True,\n",
    "     'min_samples_leaf': 5,\n",
    "     'max_features': 0.5}\n",
    "\n",
    "hyperparameters_a = {\n",
    "    'NN_TORCH': {},\n",
    "    'GBM': [lgbmXT, 'GBMLarge', r118],\n",
    "    'FASTAI': [r51]\n",
    "}\n",
    "\n",
    "hyperparameters_b = {\n",
    "    'NN_TORCH': {},\n",
    "    'GBM': [lgbmXT, r118],\n",
    "    'KNN': [{'weights': 'uniform'}],\n",
    "    'FASTAI': [r51],\n",
    "    'CAT': {}\n",
    "}\n",
    "\n",
    "hyperparameters_c = {\n",
    "    'NN_TORCH': {},\n",
    "    'GBM': [lgbmXT, r118],\n",
    "    'KNN': [{'weights': 'uniform'}],\n",
    "    'FASTAI': [r51],\n",
    "    'CAT': {},\n",
    "    'XGB': {},\n",
    "\n",
    "}\n",
    "\n",
    "level_2_hyperparameters = {\n",
    "    'XT': [{}],\n",
    "    'RF': [{}, rf_r5],\n",
    "    'GBM': ['GBMLarge'],\n",
    "    'NN_TORCH': {},\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"autogluon_models/test_modelA\"\n",
      "Presets specified: ['experimental_zeroshot_hpo_hybrid']\n",
      "/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=3\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"autogluon_models/test_modelA\"\n",
      "AutoGluon Version:  0.8.3b20231109\n",
      "Python Version:     3.11.6\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 21.6.0: Sat Jun 18 17:07:28 PDT 2022; root:xnu-8020.140.41~1/RELEASE_ARM64_T8110\n",
      "Disk Space Avail:   22.76 GB / 245.11 GB (9.3%)\n",
      "Train Data Rows:    32643\n",
      "Train Data Columns: 59\n",
      "Tuning Data Rows:    1440\n",
      "Tuning Data Columns: 59\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (5733.42, 0.0, 642.2353, 1173.613)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/tabular/learner/default_learner.py:215: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\n",
      "/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/tabular/learner/default_learner.py:223: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1675.46 MB\n",
      "\tTrain Data (Original)  Memory Usage: 10.6 MB (0.6% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['location']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  1 | ['observed_or_estimated']\n",
      "\t\t('float', [])    : 57 | ['direct_rad:W', 'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad_1h:J', 'is_in_shadow:idx', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 57 | ['direct_rad:W', 'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad_1h:J', 'is_in_shadow:idx', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['observed_or_estimated']\n",
      "\t0.1s = Fit runtime\n",
      "\t58 features in original data used to generate 58 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 8.62 MB (0.5% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.1s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "use_bag_holdout=True, will use tuning_data as holdout (will not be used for early stopping).\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'0': {'NN_TORCH': {}, 'GBM': [{'learning_rate': 0.05, 'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, 'GBMLarge', {'learning_rate': 0.021720607471727896, 'extra_trees': True, 'feature_fraction': 0.7832570544199176, 'min_data_in_leaf': 3, 'num_leaves': 21}], 'FASTAI': [{'layers': [200, 100, 50], 'emb_drop': 0.6046989241462619, 'ps': 0.09244767444160731, 'bs': 1024, 'lr': 0.00775309042164966, 'epochs': 48, 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0, 'ag_args': {'name_suffix': '_r51'}}]},\n",
      "\t'1': {'XT': [{}], 'RF': [{}, {'n_estimators': 300, 'max_leaf_nodes': 50000, 'n_jobs': -1, 'random_state': 0, 'bootstrap': True, 'min_samples_leaf': 5, 'max_features': 0.5}], 'GBM': ['GBMLarge'], 'NN_TORCH': {}},\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: RandomForest_BAG_L1 ...\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 154 due to low memory. Expected memory usage reduced from 29.17% -> 15.0% of available memory...\n",
      "\t-98.6922\t = Validation score   (-mean_absolute_error)\n",
      "\t18.41s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Fitting model: RandomForest_2_BAG_L1 ...\n",
      "\t-101.2292\t = Validation score   (-mean_absolute_error)\n",
      "\t14.32s\t = Training   runtime\n",
      "\t0.5s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_BAG_L1 ...\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 160 due to low memory. Expected memory usage reduced from 28.01% -> 15.0% of available memory...\n",
      "\t-98.0382\t = Validation score   (-mean_absolute_error)\n",
      "\t3.29s\t = Training   runtime\n",
      "\t0.37s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
      "2023-11-12 19:53:58,144\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tFitting 24 child models (S1F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.03%)\n",
      "\t-85.1439\t = Validation score   (-mean_absolute_error)\n",
      "\t139.26s\t = Training   runtime\n",
      "\t0.96s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ...\n",
      "\tMemory not enough to fit 24 folds in parallel. Will train 16 folds in parallel instead (Estimated 3.33% memory usage per fold, 53.35%/80.00% total).\n",
      "\tFitting 24 child models (S1F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.03%)\n",
      "\t-85.0048\t = Validation score   (-mean_absolute_error)\n",
      "\t598.51s\t = Training   runtime\n",
      "\t655.18s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-81.561\t = Validation score   (-mean_absolute_error)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 5 L2 models ...\n",
      "Fitting model: RandomForest_BAG_L2 ...\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 271 due to low memory. Expected memory usage reduced from 16.59% -> 15.0% of available memory...\n",
      "\t-82.9751\t = Validation score   (-mean_absolute_error)\n",
      "\t41.29s\t = Training   runtime\n",
      "\t0.6s\t = Validation runtime\n",
      "Fitting model: RandomForest_2_BAG_L2 ...\n",
      "\t-81.8083\t = Validation score   (-mean_absolute_error)\n",
      "\t17.85s\t = Training   runtime\n",
      "\t0.52s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_BAG_L2 ...\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 261 due to low memory. Expected memory usage reduced from 17.22% -> 15.0% of available memory...\n",
      "\t-82.8233\t = Validation score   (-mean_absolute_error)\n",
      "\t6.77s\t = Training   runtime\n",
      "\t0.61s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
      "\tFitting 24 child models (S1F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.02%)\n",
      "\t-85.9584\t = Validation score   (-mean_absolute_error)\n",
      "\t116.01s\t = Training   runtime\n",
      "\t1.07s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ...\n",
      "\tFitting 24 child models (S1F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.03%)\n",
      "\t-81.3989\t = Validation score   (-mean_absolute_error)\n",
      "\t35.72s\t = Training   runtime\n",
      "\t2.98s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ...\n",
      "\t-79.4474\t = Validation score   (-mean_absolute_error)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1053.86s ... Best model: \"WeightedEnsemble_L3\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"autogluon_models/test_modelA\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"autogluon_models/test_modelB\"\n",
      "Presets specified: ['experimental_zeroshot_hpo_hybrid']\n",
      "/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=3\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"autogluon_models/test_modelB\"\n",
      "AutoGluon Version:  0.8.3b20231109\n",
      "Python Version:     3.11.6\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 21.6.0: Sat Jun 18 17:07:28 PDT 2022; root:xnu-8020.140.41~1/RELEASE_ARM64_T8110\n",
      "Disk Space Avail:   21.68 GB / 245.11 GB (8.8%)\n",
      "Train Data Rows:    28151\n",
      "Train Data Columns: 59\n",
      "Tuning Data Rows:    1440\n",
      "Tuning Data Columns: 59\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (1152.3, -0.0, 95.8456, 204.21204)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/tabular/learner/default_learner.py:215: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\n",
      "/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/tabular/learner/default_learner.py:223: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3083.14 MB\n",
      "\tTrain Data (Original)  Memory Usage: 9.2 MB (0.3% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['location']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  1 | ['observed_or_estimated']\n",
      "\t\t('float', [])    : 57 | ['direct_rad:W', 'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad_1h:J', 'is_in_shadow:idx', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 57 | ['direct_rad:W', 'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad_1h:J', 'is_in_shadow:idx', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['observed_or_estimated']\n",
      "\t0.1s = Fit runtime\n",
      "\t58 features in original data used to generate 58 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 7.49 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.1s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "use_bag_holdout=True, will use tuning_data as holdout (will not be used for early stopping).\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'0': {'NN_TORCH': {}, 'GBM': [{'learning_rate': 0.05, 'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {'learning_rate': 0.021720607471727896, 'extra_trees': True, 'feature_fraction': 0.7832570544199176, 'min_data_in_leaf': 3, 'num_leaves': 21}], 'KNN': [{'weights': 'uniform'}], 'FASTAI': [{'layers': [200, 100, 50], 'emb_drop': 0.6046989241462619, 'ps': 0.09244767444160731, 'bs': 1024, 'lr': 0.00775309042164966, 'epochs': 48, 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0, 'ag_args': {'name_suffix': '_r51'}}], 'CAT': {}},\n",
      "\t'1': {'XT': [{}], 'RF': [{}, {'n_estimators': 300, 'max_leaf_nodes': 50000, 'n_jobs': -1, 'random_state': 0, 'bootstrap': True, 'min_samples_leaf': 5, 'max_features': 0.5}], 'GBM': ['GBMLarge'], 'NN_TORCH': {}},\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: RandomForest_BAG_L1 ...\n",
      "\t-15.5941\t = Validation score   (-mean_absolute_error)\n",
      "\t37.49s\t = Training   runtime\n",
      "\t0.53s\t = Validation runtime\n",
      "Fitting model: RandomForest_2_BAG_L1 ...\n",
      "\t-15.9791\t = Validation score   (-mean_absolute_error)\n",
      "\t15.44s\t = Training   runtime\n",
      "\t0.45s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_BAG_L1 ...\n",
      "\t-14.9157\t = Validation score   (-mean_absolute_error)\n",
      "\t5.39s\t = Training   runtime\n",
      "\t0.56s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
      "\tFitting 24 child models (S1F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.01%)\n",
      "\t-11.9197\t = Validation score   (-mean_absolute_error)\n",
      "\t173.23s\t = Training   runtime\n",
      "\t0.93s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ...\n",
      "\tFitting 24 child models (S1F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.03%)\n",
      "\t-13.9935\t = Validation score   (-mean_absolute_error)\n",
      "\t565.16s\t = Training   runtime\n",
      "\t447.73s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-11.8383\t = Validation score   (-mean_absolute_error)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 5 L2 models ...\n",
      "Fitting model: RandomForest_BAG_L2 ...\n",
      "\t-12.6107\t = Validation score   (-mean_absolute_error)\n",
      "\t40.6s\t = Training   runtime\n",
      "\t0.55s\t = Validation runtime\n",
      "Fitting model: RandomForest_2_BAG_L2 ...\n",
      "\t-12.3842\t = Validation score   (-mean_absolute_error)\n",
      "\t17.41s\t = Training   runtime\n",
      "\t0.43s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_BAG_L2 ...\n",
      "\t-12.6844\t = Validation score   (-mean_absolute_error)\n",
      "\t6.07s\t = Training   runtime\n",
      "\t0.58s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
      "\tFitting 24 child models (S1F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.01%)\n",
      "\t-11.7441\t = Validation score   (-mean_absolute_error)\n",
      "\t107.5s\t = Training   runtime\n",
      "\t0.92s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ...\n",
      "\tFitting 24 child models (S1F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.03%)\n",
      "\t-12.419\t = Validation score   (-mean_absolute_error)\n",
      "\t35.28s\t = Training   runtime\n",
      "\t3.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ...\n",
      "\t-11.5899\t = Validation score   (-mean_absolute_error)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1053.42s ... Best model: \"WeightedEnsemble_L3\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"autogluon_models/test_modelB\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"autogluon_models/test_modelC\"\n",
      "Presets specified: ['experimental_zeroshot_hpo_hybrid']\n",
      "/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=3\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"autogluon_models/test_modelC\"\n",
      "AutoGluon Version:  0.8.3b20231109\n",
      "Python Version:     3.11.6\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 21.6.0: Sat Jun 18 17:07:28 PDT 2022; root:xnu-8020.140.41~1/RELEASE_ARM64_T8110\n",
      "Disk Space Avail:   21.68 GB / 245.11 GB (8.8%)\n",
      "Train Data Rows:    24195\n",
      "Train Data Columns: 59\n",
      "Tuning Data Rows:    1440\n",
      "Tuning Data Columns: 59\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and label-values can't be converted to int).\n",
      "\tLabel info (max, min, mean, stddev): (999.6, -0.0, 79.1158, 168.11261)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/tabular/learner/default_learner.py:215: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\n",
      "/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/tabular/learner/default_learner.py:223: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3712.33 MB\n",
      "\tTrain Data (Original)  Memory Usage: 7.97 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['location']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  1 | ['observed_or_estimated']\n",
      "\t\t('float', [])    : 57 | ['direct_rad:W', 'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad_1h:J', 'is_in_shadow:idx', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 57 | ['direct_rad:W', 'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad_1h:J', 'is_in_shadow:idx', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['observed_or_estimated']\n",
      "\t0.1s = Fit runtime\n",
      "\t58 features in original data used to generate 58 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 6.49 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.09s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "use_bag_holdout=True, will use tuning_data as holdout (will not be used for early stopping).\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'0': {'NN_TORCH': {}, 'GBM': [{'learning_rate': 0.05, 'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {'learning_rate': 0.021720607471727896, 'extra_trees': True, 'feature_fraction': 0.7832570544199176, 'min_data_in_leaf': 3, 'num_leaves': 21}], 'KNN': [{'weights': 'uniform'}], 'FASTAI': [{'layers': [200, 100, 50], 'emb_drop': 0.6046989241462619, 'ps': 0.09244767444160731, 'bs': 1024, 'lr': 0.00775309042164966, 'epochs': 48, 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0, 'ag_args': {'name_suffix': '_r51'}}], 'CAT': {}, 'XGB': {}},\n",
      "\t'1': {'XT': [{}], 'RF': [{}, {'n_estimators': 300, 'max_leaf_nodes': 50000, 'n_jobs': -1, 'random_state': 0, 'bootstrap': True, 'min_samples_leaf': 5, 'max_features': 0.5}], 'GBM': ['GBMLarge'], 'NN_TORCH': {}},\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: RandomForest_BAG_L1 ...\n",
      "\t-16.2801\t = Validation score   (-mean_absolute_error)\n",
      "\t23.17s\t = Training   runtime\n",
      "\t0.4s\t = Validation runtime\n",
      "Fitting model: RandomForest_2_BAG_L1 ...\n",
      "\t-17.3777\t = Validation score   (-mean_absolute_error)\n",
      "\t9.85s\t = Training   runtime\n",
      "\t0.38s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_BAG_L1 ...\n",
      "\t-15.7963\t = Validation score   (-mean_absolute_error)\n",
      "\t3.6s\t = Training   runtime\n",
      "\t0.45s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
      "\tFitting 24 child models (S1F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.01%)\n",
      "\t-11.4926\t = Validation score   (-mean_absolute_error)\n",
      "\t111.01s\t = Training   runtime\n",
      "\t0.78s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ...\n",
      "\tFitting 24 child models (S1F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.02%)\n"
     ]
    }
   ],
   "source": [
    "def train_model(dataset):\n",
    "    # Define the path where the AutoGluon models will be saved\n",
    "    # enumerate all the locations\n",
    "    for index, location in enumerate(locations):\n",
    "        save_path = f\"autogluon_models/test_model{location}\"\n",
    "\n",
    "        if location == \"A\":\n",
    "            hyperparameters = hyperparameters_a\n",
    "        elif location == \"B\":\n",
    "            hyperparameters = hyperparameters_b\n",
    "        else: \n",
    "            hyperparameters = hyperparameters_c\n",
    "\n",
    "        model = TabularPredictor(\n",
    "            label=\"pv_measurement\", path=save_path, eval_metric=\"mae\"\n",
    "        )\n",
    "        model.fit(\n",
    "            train_data=dataset[index].dropna(subset=[\"pv_measurement\"]),\n",
    "            tuning_data=combined_df_validation[index].dropna(subset=[\"pv_measurement\"]),\n",
    "            presets=\"experimental_zeroshot_hpo_hybrid\",\n",
    "            use_bag_holdout=True,\n",
    "            hyperparameters={0: hyperparameters, 1: level_2_hyperparameters},\n",
    "            num_bag_sets=3,\n",
    "            num_stack_levels=1,\n",
    "        )\n",
    "\n",
    "\n",
    "train_model(combined_df_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Presets specified: ['experimental_zeroshot_hpo_hybrid']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=3\n",
      "Beginning AutoGluon training ... Time limit = 120s\n",
      "AutoGluon will save models to \"autogluon_models/test_modelA-2\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.2\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "Disk Space Avail:   254.00 GB / 510.77 GB (49.7%)\n",
      "Train Data Rows:    32643\n",
      "Train Data Columns: 59\n",
      "Tuning Data Rows:    1440\n",
      "Tuning Data Columns: 59\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (5733.42, 0.0, 644.80035, 1176.10731)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3188.35 MB\n",
      "\tTrain Data (Original)  Memory Usage: 10.6 MB (0.3% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['location']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  1 | ['observed_or_estimated']\n",
      "\t\t('float', [])    : 57 | ['direct_rad:W', 'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad_1h:J', 'is_in_shadow:idx', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 57 | ['direct_rad:W', 'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad_1h:J', 'is_in_shadow:idx', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['observed_or_estimated']\n",
      "\t0.2s = Fit runtime\n",
      "\t58 features in original data used to generate 58 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 8.62 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.22s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "use_bag_holdout=True, will use tuning_data as holdout (will not be used for early stopping).\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'0': {'NN_TORCH': {}, 'GBM': [{'learning_rate': 0.05, 'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, 'GBMLarge', {'learning_rate': 0.021720607471727896, 'extra_trees': True, 'feature_fraction': 0.7832570544199176, 'min_data_in_leaf': 3, 'num_leaves': 21}], 'FASTAI': [{'layers': [200, 100, 50], 'emb_drop': 0.6046989241462619, 'ps': 0.09244767444160731, 'bs': 1024, 'lr': 0.00775309042164966, 'epochs': 48, 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0, 'ag_args': {'name_suffix': '_r51'}}]},\n",
      "\t'1': {'XT': [{}], 'RF': [{}, {'n_estimators': 300, 'max_leaf_nodes': 50000, 'n_jobs': -1, 'random_state': 0, 'bootstrap': True, 'min_samples_leaf': 5, 'max_features': 0.5}], 'GBM': ['GBMLarge'], 'NN_TORCH': {}},\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: RandomForest_BAG_L1 ... Training model for up to 79.83s of the 119.78s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 247 due to low time. Expected time usage reduced from 96.9s -> 79.8s...\n",
      "\t-88.8245\t = Validation score   (-mean_absolute_error)\n",
      "\t34.27s\t = Training   runtime\n",
      "\t1.39s\t = Validation runtime\n",
      "Fitting model: RandomForest_2_BAG_L1 ... Training model for up to 43.57s of the 83.52s of remaining time.\n",
      "\t-90.5196\t = Validation score   (-mean_absolute_error)\n",
      "\t15.81s\t = Training   runtime\n",
      "\t1.16s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_BAG_L1 ... Training model for up to 26.34s of the 66.28s of remaining time.\n",
      "\t-87.2436\t = Validation score   (-mean_absolute_error)\n",
      "\t7.16s\t = Training   runtime\n",
      "\t1.62s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 16.76s of the 56.7s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-89.0093\t = Validation score   (-mean_absolute_error)\n",
      "\t15.79s\t = Training   runtime\n",
      "\t0.63s\t = Validation runtime\n",
      "Completed 1/3 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 119.78s of the 37.75s of remaining time.\n",
      "\t-83.6775\t = Validation score   (-mean_absolute_error)\n",
      "\t0.13s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 5 L2 models ...\n",
      "Fitting model: RandomForest_BAG_L2 ... Training model for up to 37.6s of the 37.55s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 102 due to low time. Expected time usage reduced from 109.8s -> 37.6s...\n",
      "\t-82.8198\t = Validation score   (-mean_absolute_error)\n",
      "\t15.8s\t = Training   runtime\n",
      "\t0.57s\t = Validation runtime\n",
      "Fitting model: RandomForest_2_BAG_L2 ... Training model for up to 20.93s of the 20.88s of remaining time.\n",
      "\t-81.8308\t = Validation score   (-mean_absolute_error)\n",
      "\t16.86s\t = Training   runtime\n",
      "\t1.2s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_BAG_L2 ... Training model for up to 2.61s of the 2.56s of remaining time.\n",
      "\t-83.5431\t = Validation score   (-mean_absolute_error)\n",
      "\t7.38s\t = Training   runtime\n",
      "\t1.65s\t = Validation runtime\n",
      "Completed 1/3 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 119.78s of the -7.32s of remaining time.\n",
      "\t-81.8218\t = Validation score   (-mean_absolute_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 127.43s ... Best model: \"WeightedEnsemble_L3\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"autogluon_models/test_modelA-2\\\")\n",
      "Presets specified: ['experimental_zeroshot_hpo_hybrid']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=3\n",
      "Beginning AutoGluon training ... Time limit = 120s\n",
      "AutoGluon will save models to \"autogluon_models/test_modelB-2\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.2\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "Disk Space Avail:   252.45 GB / 510.77 GB (49.4%)\n",
      "Train Data Rows:    28151\n",
      "Train Data Columns: 59\n",
      "Tuning Data Rows:    1440\n",
      "Tuning Data Columns: 59\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (1152.3, -0.0, 95.63734, 204.09705)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3235.45 MB\n",
      "\tTrain Data (Original)  Memory Usage: 9.2 MB (0.3% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['location']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  1 | ['observed_or_estimated']\n",
      "\t\t('float', [])    : 57 | ['direct_rad:W', 'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad_1h:J', 'is_in_shadow:idx', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 57 | ['direct_rad:W', 'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad_1h:J', 'is_in_shadow:idx', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['observed_or_estimated']\n",
      "\t0.1s = Fit runtime\n",
      "\t58 features in original data used to generate 58 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 7.49 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.18s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "use_bag_holdout=True, will use tuning_data as holdout (will not be used for early stopping).\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'0': {'NN_TORCH': {}, 'GBM': [{'learning_rate': 0.05, 'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {'learning_rate': 0.021720607471727896, 'extra_trees': True, 'feature_fraction': 0.7832570544199176, 'min_data_in_leaf': 3, 'num_leaves': 21}], 'KNN': [{'weights': 'uniform'}], 'FASTAI': [{'layers': [200, 100, 50], 'emb_drop': 0.6046989241462619, 'ps': 0.09244767444160731, 'bs': 1024, 'lr': 0.00775309042164966, 'epochs': 48, 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0, 'ag_args': {'name_suffix': '_r51'}}], 'CAT': {}},\n",
      "\t'1': {'XT': [{}], 'RF': [{}, {'n_estimators': 300, 'max_leaf_nodes': 50000, 'n_jobs': -1, 'random_state': 0, 'bootstrap': True, 'min_samples_leaf': 5, 'max_features': 0.5}], 'GBM': ['GBMLarge'], 'NN_TORCH': {}},\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: RandomForest_BAG_L1 ... Training model for up to 79.86s of the 119.82s of remaining time.\n",
      "\t-16.1856\t = Validation score   (-mean_absolute_error)\n",
      "\t32.99s\t = Training   runtime\n",
      "\t1.27s\t = Validation runtime\n",
      "Fitting model: RandomForest_2_BAG_L1 ... Training model for up to 44.96s of the 84.92s of remaining time.\n",
      "\t-17.2314\t = Validation score   (-mean_absolute_error)\n",
      "\t13.49s\t = Training   runtime\n",
      "\t0.97s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_BAG_L1 ... Training model for up to 30.16s of the 70.12s of remaining time.\n",
      "\t-16.0277\t = Validation score   (-mean_absolute_error)\n",
      "\t5.21s\t = Training   runtime\n",
      "\t1.31s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 23.0s of the 62.96s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-15.2799\t = Validation score   (-mean_absolute_error)\n",
      "\t21.12s\t = Training   runtime\n",
      "\t0.58s\t = Validation runtime\n",
      "Completed 1/3 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 119.82s of the 38.75s of remaining time.\n",
      "\t-14.5562\t = Validation score   (-mean_absolute_error)\n",
      "\t0.13s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 5 L2 models ...\n",
      "Fitting model: RandomForest_BAG_L2 ... Training model for up to 38.6s of the 38.57s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 107 due to low time. Expected time usage reduced from 108.0s -> 38.6s...\n",
      "\t-14.7509\t = Validation score   (-mean_absolute_error)\n",
      "\t15.0s\t = Training   runtime\n",
      "\t0.46s\t = Validation runtime\n",
      "Fitting model: RandomForest_2_BAG_L2 ... Training model for up to 22.84s of the 22.81s of remaining time.\n",
      "\t-14.6168\t = Validation score   (-mean_absolute_error)\n",
      "\t15.42s\t = Training   runtime\n",
      "\t1.01s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_BAG_L2 ... Training model for up to 6.19s of the 6.15s of remaining time.\n",
      "\t-14.7916\t = Validation score   (-mean_absolute_error)\n",
      "\t5.91s\t = Training   runtime\n",
      "\t1.47s\t = Validation runtime\n",
      "Completed 1/3 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 119.82s of the -1.86s of remaining time.\n",
      "\t-14.6166\t = Validation score   (-mean_absolute_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 121.96s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"autogluon_models/test_modelB-2\\\")\n",
      "Presets specified: ['experimental_zeroshot_hpo_hybrid']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=3\n",
      "Beginning AutoGluon training ... Time limit = 120s\n",
      "AutoGluon will save models to \"autogluon_models/test_modelC-2\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.2\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "Disk Space Avail:   251.34 GB / 510.77 GB (49.2%)\n",
      "Train Data Rows:    24195\n",
      "Train Data Columns: 59\n",
      "Tuning Data Rows:    1440\n",
      "Tuning Data Columns: 59\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and label-values can't be converted to int).\n",
      "\tLabel info (max, min, mean, stddev): (999.6, 0.0, 79.06727, 167.96381)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3157.79 MB\n",
      "\tTrain Data (Original)  Memory Usage: 7.97 MB (0.3% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['location']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  1 | ['observed_or_estimated']\n",
      "\t\t('float', [])    : 57 | ['direct_rad:W', 'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad_1h:J', 'is_in_shadow:idx', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 57 | ['direct_rad:W', 'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad_1h:J', 'is_in_shadow:idx', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['observed_or_estimated']\n",
      "\t0.1s = Fit runtime\n",
      "\t58 features in original data used to generate 58 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 6.49 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.17s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "use_bag_holdout=True, will use tuning_data as holdout (will not be used for early stopping).\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'0': {'NN_TORCH': {}, 'GBM': [{'learning_rate': 0.05, 'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {'learning_rate': 0.021720607471727896, 'extra_trees': True, 'feature_fraction': 0.7832570544199176, 'min_data_in_leaf': 3, 'num_leaves': 21}], 'KNN': [{'weights': 'uniform'}], 'FASTAI': [{'layers': [200, 100, 50], 'emb_drop': 0.6046989241462619, 'ps': 0.09244767444160731, 'bs': 1024, 'lr': 0.00775309042164966, 'epochs': 48, 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0, 'ag_args': {'name_suffix': '_r51'}}], 'CAT': {}, 'XGB': {}},\n",
      "\t'1': {'XT': [{}], 'RF': [{}, {'n_estimators': 300, 'max_leaf_nodes': 50000, 'n_jobs': -1, 'random_state': 0, 'bootstrap': True, 'min_samples_leaf': 5, 'max_features': 0.5}], 'GBM': ['GBMLarge'], 'NN_TORCH': {}},\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: RandomForest_BAG_L1 ... Training model for up to 79.87s of the 119.83s of remaining time.\n",
      "\t-15.111\t = Validation score   (-mean_absolute_error)\n",
      "\t22.31s\t = Training   runtime\n",
      "\t0.9s\t = Validation runtime\n",
      "Fitting model: RandomForest_2_BAG_L1 ... Training model for up to 56.21s of the 96.17s of remaining time.\n",
      "\t-15.9874\t = Validation score   (-mean_absolute_error)\n",
      "\t9.03s\t = Training   runtime\n",
      "\t0.73s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_BAG_L1 ... Training model for up to 46.23s of the 86.19s of remaining time.\n",
      "\t-13.6262\t = Validation score   (-mean_absolute_error)\n",
      "\t3.7s\t = Training   runtime\n",
      "\t1.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 41.0s of the 80.97s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-11.9958\t = Validation score   (-mean_absolute_error)\n",
      "\t35.31s\t = Training   runtime\n",
      "\t0.48s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2.62s of the 42.59s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-28.677\t = Validation score   (-mean_absolute_error)\n",
      "\t4.38s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Completed 1/3 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 119.83s of the 35.02s of remaining time.\n",
      "\t-11.5153\t = Validation score   (-mean_absolute_error)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 5 L2 models ...\n",
      "Fitting model: RandomForest_BAG_L2 ... Training model for up to 34.87s of the 34.84s of remaining time.\n",
      "\t-12.1495\t = Validation score   (-mean_absolute_error)\n",
      "\t25.25s\t = Training   runtime\n",
      "\t0.99s\t = Validation runtime\n",
      "Fitting model: RandomForest_2_BAG_L2 ... Training model for up to 8.19s of the 8.16s of remaining time.\n",
      "\t-12.3313\t = Validation score   (-mean_absolute_error)\n",
      "\t10.05s\t = Training   runtime\n",
      "\t0.78s\t = Validation runtime\n",
      "Completed 1/3 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 119.83s of the -2.9s of remaining time.\n",
      "\t-12.1495\t = Validation score   (-mean_absolute_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 122.98s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"autogluon_models/test_modelC-2\\\")\n"
     ]
    }
   ],
   "source": [
    "# shuffle dataset so that a slightly different model is trained\n",
    "combined_df_train = shuffle(combined_df_train, random_state=69)\n",
    "\n",
    "# Model 1\n",
    "def train_model(dataset):\n",
    "    # Define the path where the AutoGluon models will be saved\n",
    "    for index, location in enumerate(locations):\n",
    "        save_path = f\"autogluon_models/test_model{location}-2\"\n",
    "\n",
    "        if location == \"A\":\n",
    "            hyperparameters = hyperparameters_a\n",
    "        elif location == \"B\":\n",
    "            hyperparameters = hyperparameters_b\n",
    "        else: \n",
    "            hyperparameters = hyperparameters_c\n",
    "\n",
    "        # Initialize the TabularPredictor object\n",
    "        model = TabularPredictor(\n",
    "            label=\"pv_measurement\", path=save_path, eval_metric=\"mae\"\n",
    "        )\n",
    "        model.fit(\n",
    "            train_data=dataset[index].dropna(subset=[\"pv_measurement\"]),\n",
    "            tuning_data=combined_df_validation[index].dropna(subset=[\"pv_measurement\"]),\n",
    "            presets=\"experimental_zeroshot_hpo_hybrid\",\n",
    "            use_bag_holdout=True,\n",
    "            hyperparameters={0: hyperparameters, 1: level_2_hyperparameters},\n",
    "            num_bag_sets=3,\n",
    "            num_stack_levels=1,\n",
    "        )\n",
    "\n",
    "\n",
    "# Train the model using all available training data and the initial validation set for early stopping\n",
    "train_model(combined_df_train)\n",
    "\n",
    "# Evaluate the model using the same validation set\n",
    "# evaluate_model(combined_X_val, combined_Y_val, location, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming you have defined WEATHER_FEATURES, TEST_COLUMNS_TO_KEEP, and other functions previously\n",
    "\n",
    "def make_predictions(df_test_pred, location):\n",
    "    eval_model1 = TabularPredictor.load(f\"autogluon_models/test_model{location}\", require_version_match=False)\n",
    "    preds1 = eval_model1.predict(df_test_pred)\n",
    "    eval_model2 = TabularPredictor.load(f\"autogluon_models/test_model{location}-2\", require_version_match=False)\n",
    "    preds2 = eval_model2.predict(df_test_pred)\n",
    "    return (preds1 + preds2) / 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    True\n",
      "1    True\n",
      "2    True\n",
      "3    True\n",
      "4    True\n",
      "Name: location, dtype: bool\n",
      "0    False\n",
      "1    False\n",
      "2    False\n",
      "3    False\n",
      "4    False\n",
      "Name: location, dtype: bool\n",
      "0    False\n",
      "1    False\n",
      "2    False\n",
      "3    False\n",
      "4    False\n",
      "Name: location, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "df_submission = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "for index, location in enumerate(locations): \n",
    "    preds = make_predictions(combined_df_test[index], location)\n",
    "\n",
    "    # Assign the predictions to df_submission for the current location\n",
    "    mask = df_submission[\"location\"] == location\n",
    "    print(mask.head())\n",
    "    # Add a check to make sure the lengths match\n",
    "    if len(preds) != mask.sum():\n",
    "        print(f\"Length of predictions: {len(preds)}\")\n",
    "        print(f\"Length of submission entries: {mask.sum()}\")\n",
    "        raise ValueError(f\"Mismatch in length of predictions and submission entries for location {location}.\")\n",
    "\n",
    "    df_submission.loc[mask, \"prediction\"] = preds.to_numpy()\n",
    "\n",
    "df_submission['prediction'] = df_submission['prediction'].apply(lambda x: max(x, 0))\n",
    "\n",
    "df_submission['prediction'] = df_submission['prediction'].apply(lambda x: 0 if x < 0.1 else x)\n",
    "\n",
    "\n",
    "# Set predictions to zero where up to three non-zero predictions are surrounded by zeros\n",
    "for i in range(1, len(df_submission) - 1):\n",
    "    # Check single non-zero prediction surrounded by zeros\n",
    "    if df_submission.loc[i - 1, 'prediction'] == 0 and df_submission.loc[i + 1, 'prediction'] == 0:\n",
    "        df_submission.loc[i, 'prediction'] = 0\n",
    "    # Check two consecutive non-zero predictions surrounded by zeros\n",
    "    if i < len(df_submission) - 2 and df_submission.loc[i - 1, 'prediction'] == 0 and df_submission.loc[i + 2, 'prediction'] == 0:\n",
    "        df_submission.loc[i, 'prediction'] = 0\n",
    "        df_submission.loc[i + 1, 'prediction'] = 0\n",
    "    # Check three consecutive non-zero predictions surrounded by zeros\n",
    "\n",
    "\n",
    "# Save the results to a new submission file\n",
    "df_submission[[\"id\", \"prediction\"]].to_csv(\"predictions/short-storybook.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
