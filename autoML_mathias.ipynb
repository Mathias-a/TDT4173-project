{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "from autogluon.tabular import TabularPredictor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constans and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = [\"A\", \"B\", \"C\"]\n",
    "features_order = []\n",
    "\n",
    "LAGGED_COLUMNS_TO_KEEP = [\n",
    "    'direct_rad:W_lag_1h', \n",
    "    'direct_rad:W_lag_forward_1h', \n",
    "    'clear_sky_rad:W_lag_1h', \n",
    "    'clear_sky_rad:W_lag_forward_1h', \n",
    "    'diffuse_rad:W_lag_1h', \n",
    "    'diffuse_rad:W_lag_forward_1h', \n",
    "    'direct_rad_1h:J_lag_1h', \n",
    "    'direct_rad_1h:J_lag_forward_1h', \n",
    "    'is_in_shadow:idx_lag_1h', \n",
    "    'is_in_shadow:idx_lag_forward_1h', \n",
    "    'clear_sky_energy_1h:J_lag_1h', \n",
    "    'clear_sky_energy_1h:J_lag_forward_1h', \n",
    "    'effective_cloud_cover:p_lag_1h', \n",
    "    'effective_cloud_cover:p_lag_forward_1h', \n",
    "    'visibility:m_lag_1h', \n",
    "    'visibility:m_lag_forward_1h', \n",
    "    'total_cloud_cover:p_lag_1h', \n",
    "    'total_cloud_cover:p_lag_forward_1h', \n",
    "\n",
    "\n",
    "    # 'direct_rad:W_lag_2h', \n",
    "    # 'direct_rad:W_lag_forward_2h', \n",
    "    # 'clear_sky_rad:W_lag_2h', \n",
    "    # 'clear_sky_rad:W_lag_forward_2h', \n",
    "    # 'diffuse_rad:W_lag_2h', \n",
    "    # 'diffuse_rad:W_lag_forward_2h', \n",
    "    # 'direct_rad_1h:J_lag_2h', \n",
    "    # 'direct_rad_1h:J_lag_forward_2h', \n",
    "    # 'is_in_shadow:idx_lag_2h', \n",
    "    # 'is_in_shadow:idx_lag_forward_2h', \n",
    "    # 'clear_sky_energy_1h:J_lag_2h', \n",
    "    # 'clear_sky_energy_1h:J_lag_forward_2h', \n",
    "    # 'effective_cloud_cover:p_lag_2h', \n",
    "    # 'effective_cloud_cover:p_lag_forward_2h', \n",
    "    # 'visibility:m_lag_2h', \n",
    "    # 'visibility:m_lag_forward_2h', \n",
    "    # 'total_cloud_cover:p_lag_2h', \n",
    "    # 'total_cloud_cover:p_lag_forward_2h', \n",
    "\n",
    "    # 'direct_rad:W_lag_3h', \n",
    "    # 'direct_rad:W_lag_forward_3h', \n",
    "    # 'clear_sky_rad:W_lag_3h', \n",
    "    # 'clear_sky_rad:W_lag_forward_3h', \n",
    "    # 'diffuse_rad:W_lag_3h', \n",
    "    # 'diffuse_rad:W_lag_forward_3h', \n",
    "    # 'direct_rad_1h:J_lag_3h', \n",
    "    # 'direct_rad_1h:J_lag_forward_3h', \n",
    "    # 'is_in_shadow:idx_lag_3h', \n",
    "    # 'is_in_shadow:idx_lag_forward_3h', \n",
    "    # 'clear_sky_energy_1h:J_lag_3h', \n",
    "    # 'clear_sky_energy_1h:J_lag_forward_3h', \n",
    "    # 'effective_cloud_cover:p_lag_3h', \n",
    "    # 'effective_cloud_cover:p_lag_forward_3h', \n",
    "    # 'visibility:m_lag_3h', \n",
    "    # 'visibility:m_lag_forward_3h', \n",
    "    # 'total_cloud_cover:p_lag_3h', \n",
    "    # 'total_cloud_cover:p_lag_forward_3h'\n",
    "]\n",
    "\n",
    "CUSTOM_COLUMNS_TO_KEEP = [\n",
    "    \"hour_cos\",\n",
    "    \"hour_sin\",\n",
    "    \"month_sin\",\n",
    "    \"month_cos\",\n",
    "    \"day-of-year\",\n",
    "]\n",
    "\n",
    "WEATHER_FEATURES = [\n",
    "    \"direct_rad:W\",\n",
    "    \"clear_sky_rad:W\",\n",
    "    \"diffuse_rad:W\",\n",
    "    \"direct_rad_1h:J\",\n",
    "    \"is_in_shadow:idx\",\n",
    "    \"clear_sky_energy_1h:J\",\n",
    "    \"effective_cloud_cover:p\",\n",
    "    \"visibility:m\",\n",
    "    \"total_cloud_cover:p\",\n",
    "]\n",
    "\n",
    "\n",
    "TEST_COLUMNS_TO_KEEP = [\n",
    "    \"direct_rad:W\",\n",
    "    \"clear_sky_rad:W\",\n",
    "    \"diffuse_rad:W\",\n",
    "    \"direct_rad_1h:J\",\n",
    "    \"is_in_shadow:idx\",\n",
    "    \"clear_sky_energy_1h:J\",\n",
    "    \"diffuse_rad_1h:J\",\n",
    "    \"is_day:idx\",\n",
    "    \"sun_elevation:d\",\n",
    "    \"ceiling_height_agl:m\",\n",
    "    \"effective_cloud_cover:p\",\n",
    "    \"visibility:m\",\n",
    "    \"total_cloud_cover:p\",\n",
    "    \"air_density_2m:kgm3\",\n",
    "    \"wind_speed_v_10m:ms\",\n",
    "    \"dew_point_2m:K\",\n",
    "    \"wind_speed_u_10m:ms\",\n",
    "    \"t_1000hPa:K\",\n",
    "    \"absolute_humidity_2m:gm3\",\n",
    "    \"snow_water:kgm2\",\n",
    "    \"relative_humidity_1000hPa:p\",\n",
    "    \"fresh_snow_24h:cm\",\n",
    "    \"cloud_base_agl:m\",\n",
    "    \"fresh_snow_12h:cm\",\n",
    "    \"snow_depth:cm\",\n",
    "    \"dew_or_rime:idx\",\n",
    "    \"fresh_snow_6h:cm\",\n",
    "    \"super_cooled_liquid_water:kgm2\",\n",
    "    \"fresh_snow_3h:cm\",\n",
    "    \"rain_water:kgm2\",\n",
    "    \"precip_type_5min:idx\",\n",
    "    \"precip_5min:mm\",\n",
    "    \"fresh_snow_1h:cm\",\n",
    "    \"sun_azimuth:d\",\n",
    "    \"msl_pressure:hPa\",\n",
    "    \"pressure_100m:hPa\",\n",
    "    \"pressure_50m:hPa\",\n",
    "    \"sfc_pressure:hPa\",\n",
    "    \"prob_rime:p\",\n",
    "    \"wind_speed_10m:ms\",\n",
    "    \"elevation:m\",\n",
    "    # \"snow_density:kgm3\",\n",
    "    \"snow_drift:idx\",\n",
    "    \"snow_melt_10min:mm\",\n",
    "    \"wind_speed_w_1000hPa:ms\",\n",
    "    \"observed_or_estimated\"\n",
    "    # \"location_A\",\n",
    "    # \"location_B\",\n",
    "    # \"location_C\",\n",
    "    # \"date_calc\",\n",
    "] + CUSTOM_COLUMNS_TO_KEEP  +  LAGGED_COLUMNS_TO_KEEP\n",
    "\n",
    "COLUMNS_TO_KEEP = TEST_COLUMNS_TO_KEEP + [\"pv_measurement\"]\n",
    "\n",
    "\n",
    "\n",
    "def create_weather_lagged_features(df, weather_features):\n",
    "    # Choose the weather features for which you want to create lagged versions\n",
    "    for feature in weather_features:\n",
    "        # Assuming hourly data, adjust the lags for your specific dataset\n",
    "        # Creating lagged features for 1 hour, 1 day, and 1 week\n",
    "        df[f'{feature}_lag_1h'] = df[feature].shift(1)\n",
    "        # df[f'{feature}_lag_2h'] = df[feature].shift(2)\n",
    "        # df[f'{feature}_lag_3h'] = df[feature].shift(3)\n",
    "\n",
    "        df[f'{feature}_lag_forward_1h'] = df[feature].shift(-1)\n",
    "        # df[f'{feature}_lag_forward_2h'] = df[feature].shift(-2)\n",
    "        # df[f'{feature}_lag_forward_3h'] = df[feature].shift(-3)\n",
    "        # df[f'{feature}_lag_24h'] = df[feature].shift(24*4)\n",
    "        # df[f'{feature}_lag_168h'] = df[feature].shift(24 * 7 * 4 * 365)\n",
    "        # df[f'{feature}_front_lag_1h'] = df[feature].shift(-4)\n",
    "        # df[f'{feature}_front_lag_24h'] = df[feature].shift(-24*4)\n",
    "\n",
    "\n",
    "    # Handling edges by filling NaNs with appropriate values or dropping them\n",
    "    # You may choose to fill with zeroes or interpolate, based on what makes more sense for your data\n",
    "    df.fillna(method='ffill', inplace=True)  # Forward fill\n",
    "    df.fillna(method='bfill', inplace=True)  # Backward fill\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_lagged_features(df, column_name='pv_measurement'):\n",
    "    # Assuming 'date_forecast' is the datetime column used for sorting\n",
    "\n",
    "    df[f'{column_name}_prev_month'] = df[column_name].shift(24*7) # previous week\n",
    "\n",
    "    # For yearly lag, you would need to calculate the number of observations per year\n",
    "    # If the data is not consistent (leap years, etc.), you may need a more complex method\n",
    "    # Here's a simple version assuming 365 days a year:\n",
    "    df[f'{column_name}_prev_year'] = df[column_name].shift(24*365) # previous year\n",
    "    df[f'{column_name}_2years_ago'] = df[column_name].shift(24*365*2) # next year\n",
    "\n",
    "    # Handling edges by filling NaNs with appropriate values or dropping them\n",
    "    df.fillna(method='ffill', inplace=True)  # Forward fill\n",
    "    df.fillna(method='bfill', inplace=True)  # Backward fill\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_custom_fields(df):\n",
    "     df['hour_sin'] = np.sin(2 * np.pi * df['date_forecast'].dt.hour / 24)\n",
    "     df['hour_cos'] = np.cos(2 * np.pi * df['date_forecast'].dt.hour / 24)\n",
    "\n",
    "     df['month_sin'] = np.sin(2 * np.pi * df['date_forecast'].dt.month / 12)\n",
    "     df['month_cos'] = np.cos(2 * np.pi * df['date_forecast'].dt.month / 12)\n",
    "     df['day-of-year'] = df['date_forecast'].dt.dayofyear\n",
    "     return df\n",
    "\n",
    "def remove_outliers(df):\n",
    "    # Use a mask to filter out the rows where rolling std is zero but keep the rows where the value itself is zero\n",
    "    mask = (df['pv_measurement'].rolling(5).std() == 0) & (df['pv_measurement'] != 0)\n",
    "    df = df[~mask]\n",
    "    return df\n",
    "\n",
    "def resample_add_data(df, is_test_data):\n",
    "    df = add_custom_fields(df)\n",
    "    df.set_index('date_forecast', inplace=True)\n",
    "    df = df.resample('1H').mean()\n",
    "    # Remove empty dates if test data\n",
    "    if is_test_data:\n",
    "        non_nan_threshold = len(df.columns) // 2  \n",
    "        df.dropna(thresh=non_nan_threshold, inplace=True)\n",
    "    df.interpolate(method=\"linear\", inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_location_feature(X, location):\n",
    "      # Treat location as a categorical feature by converting it to a category type\n",
    "    X['location'] = location\n",
    "    X['location'] = X['location'].astype(str)  # Convert to string if 'location' is not an int\n",
    "    X['dew_or_rime:idx'] = X['dew_or_rime:idx'].astype(str)\n",
    "    X['is_day:idx'] = X['is_day:idx'].astype(str)\n",
    "    X['is_in_shadow:idx'] = X['is_in_shadow:idx'].astype(str)\n",
    "    categorical_columns = ['location', 'dew_or_rime:idx', 'is_day:idx', 'is_in_shadow:idx']\n",
    "\n",
    "    # Before filling NaN values, add 'missing' as a category for each categorical column.\n",
    "    for column in categorical_columns:\n",
    "        X[column] = X[column].astype('category')  # Ensure the column is of type 'category'.\n",
    "        if 'missing' not in X[column].cat.categories:\n",
    "            X[column] = X[column].cat.add_categories(['missing'])  # Add 'missing' as a new category.\n",
    "        X[column] = X[column].fillna('missing')\n",
    "    X['location'] = X['location'].astype('category')\n",
    "    X['dew_or_rime:idx'] = X['dew_or_rime:idx'].astype('category')\n",
    "    X['is_day:idx'] = X['is_day:idx'].astype('category')\n",
    "    X['is_in_shadow:idx'] = X['is_in_shadow:idx'].astype('category')\n",
    "    \n",
    "    return X\n",
    "\n",
    "def add_scaling(X_test, X_training, location):\n",
    "    global scalers\n",
    "    continuous_columns = X_training.select_dtypes(include=['float32', 'int32']).columns\n",
    "    if location not in scalers:\n",
    "        scalers[location] = MinMaxScaler()\n",
    "    X_training[continuous_columns] = scalers[location].fit_transform(X_training[continuous_columns])\n",
    "    \n",
    "    X_test[continuous_columns] = scalers[location].transform(X_test[continuous_columns])\n",
    "\n",
    "    return X_test, X_training\n",
    "\n",
    "def make_observed_and_estimated_category(df_observed, df_estimated, df_test):\n",
    "     # Hot encode in wether observed or estimated\n",
    "    df_observed['observed_or_estimated'] = 'observed'\n",
    "    df_estimated['observed_or_estimated'] = 'estimated'\n",
    "    df_test['observed_or_estimated'] = 'estimated'\n",
    "    # Concatenate observed and estimated\n",
    "    df_training = pd.concat([df_observed, df_estimated], axis=0).sort_values(by=\"date_forecast\")\n",
    "    df_training['observed_or_estimated'] = df_training['observed_or_estimated'].astype('category')\n",
    "    df_test['observed_or_estimated'] = df_test['observed_or_estimated'].astype('category')\n",
    "\n",
    "    return df_training, df_test\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_89662/3755544180.py:162: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)  # Forward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_89662/3755544180.py:163: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)  # Backward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_89662/3755544180.py:162: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)  # Forward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_89662/3755544180.py:163: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)  # Backward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_89662/3755544180.py:162: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)  # Forward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_89662/3755544180.py:163: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)  # Backward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_89662/3755544180.py:162: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)  # Forward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_89662/3755544180.py:163: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)  # Backward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_89662/3755544180.py:162: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)  # Forward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_89662/3755544180.py:163: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)  # Backward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_89662/3755544180.py:162: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)  # Forward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_89662/3755544180.py:163: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)  # Backward fill\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize a dictionary to hold the scalers for each location\n",
    "\n",
    "scalers = {}\n",
    "\n",
    "\n",
    "def prepare_data(location):\n",
    "    # Load data\n",
    "    scaling = False  # Set scaling to True to enable individual scaling for each location\n",
    "    global scalers\n",
    "\n",
    "    # Load training data\n",
    "    df_observed = pd.read_parquet(f\"data/{location}/X_train_observed.parquet\")\n",
    "    df_estimated = pd.read_parquet(f\"data/{location}/X_train_estimated.parquet\")\n",
    "    df_target = pd.read_parquet(f\"data/{location}/train_targets.parquet\")\n",
    "\n",
    "   \n",
    "    # Load test data\n",
    "    df_test = pd.read_parquet(f\"data/{location}/X_test_estimated.parquet\")\n",
    "\n",
    "   \n",
    "    # Hot encode in wether observed or estimated\n",
    "\n",
    "    # Resample and add custom fields\n",
    "\n",
    "    df_observed = resample_add_data(df_observed, False)\n",
    "    df_estimated = resample_add_data(df_estimated, False)\n",
    "    df_test = resample_add_data(df_test, True)\n",
    "\n",
    "    df_training, df_test = make_observed_and_estimated_category(df_observed, df_estimated, df_test)\n",
    "\n",
    "    # Merge training with target data\n",
    "    df_training = pd.merge(df_training, df_target, left_on=\"date_forecast\", right_on=\"time\", how=\"inner\")\n",
    "    \n",
    "    # Create lagged features and remove outliers training\n",
    "    df_training = create_weather_lagged_features(df_training, WEATHER_FEATURES)\n",
    "    df_training = df_training[COLUMNS_TO_KEEP]\n",
    "    df_training = remove_outliers(df_training)\n",
    "\n",
    "    # Create lagged features test\n",
    "    df_test = create_weather_lagged_features(df_test, WEATHER_FEATURES)\n",
    "    df_test = df_test[TEST_COLUMNS_TO_KEEP]\n",
    "\n",
    "    # Make training x and y\n",
    "    y_training = df_training[\"pv_measurement\"]\n",
    "    X_training = df_training.drop(\"pv_measurement\", axis=1)\n",
    "\n",
    "    # Add categories\n",
    "    X_training = add_location_feature(X_training, location)\n",
    "    X_test = add_location_feature(df_test, location)\n",
    "    \n",
    "    # Add scaling\n",
    "    if scaling:\n",
    "        X_test, X_training = add_scaling(X_test, X_training, location)\n",
    "    X_test.reset_index(inplace=True)\n",
    "    X_test.drop(\"date_forecast\", axis=1, inplace=True)\n",
    "    # y_training = np.log1p(y_training)\n",
    "    return X_training, X_test, y_training\n",
    "\n",
    "\n",
    "\n",
    "# Use prepare_data function\n",
    "\n",
    "combined_X_train = pd.DataFrame()\n",
    "combined_X_val = pd.DataFrame()\n",
    "combined_Y_train = pd.DataFrame()\n",
    "combined_Y_val = pd.DataFrame()\n",
    "combined_X_test = pd.DataFrame()\n",
    "\n",
    "for location in locations:\n",
    "    # Prepare the training data\n",
    "    X_training, X_test, y_training = prepare_data(location)\n",
    "    \n",
    "    # Split and concatenate the training data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_training, y_training, test_size=0.0001, random_state=42)\n",
    "    combined_X_train = pd.concat([combined_X_train, X_train])\n",
    "    combined_X_val = pd.concat([combined_X_val, X_val])\n",
    "    combined_Y_train = pd.concat([combined_Y_train, y_train])\n",
    "    combined_Y_val = pd.concat([combined_Y_val, y_val])\n",
    "\n",
    "    combined_X_test = pd.concat([combined_X_test, X_test])\n",
    "\n",
    "    # Spl\n",
    "\n",
    "combined_X_test\n",
    "combined_X_train, combined_Y_train = shuffle(combined_X_train, combined_Y_train, random_state=42)\n",
    "combined_X_val, combined_Y_val = shuffle(combined_X_val, combined_Y_val, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"autogluon_models/test_model\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Presets specified: ['best_quality']\n",
      "/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (248467 samples, 85.97 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"autogluon_models/test_model\"\n",
      "AutoGluon Version:  0.8.3b20231108\n",
      "Python Version:     3.11.6\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 21.6.0: Sat Jun 18 17:07:28 PDT 2022; root:xnu-8020.140.41~1/RELEASE_ARM64_T8110\n",
      "Disk Space Avail:   82.57 GB / 245.11 GB (33.7%)\n",
      "Train Data Rows:    248467\n",
      "Train Data Columns: 69\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (5733.42, -0.0, 283.36896, 766.78915)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/tabular/learner/default_learner.py:215: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2620.21 MB\n",
      "\tTrain Data (Original)  Memory Usage: 83.98 MB (3.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([                          'direct_rad:W',\n",
      "                              'clear_sky_rad:W',\n",
      "                                'diffuse_rad:W',\n",
      "                              'direct_rad_1h:J',\n",
      "                             'is_in_shadow:idx',\n",
      "                        'clear_sky_energy_1h:J',\n",
      "                             'diffuse_rad_1h:J',\n",
      "                                   'is_day:idx',\n",
      "                              'sun_elevation:d',\n",
      "                         'ceiling_height_agl:m',\n",
      "                      'effective_cloud_cover:p',\n",
      "                                 'visibility:m',\n",
      "                          'total_cloud_cover:p',\n",
      "                          'air_density_2m:kgm3',\n",
      "                          'wind_speed_v_10m:ms',\n",
      "                               'dew_point_2m:K',\n",
      "                          'wind_speed_u_10m:ms',\n",
      "                                  't_1000hPa:K',\n",
      "                     'absolute_humidity_2m:gm3',\n",
      "                              'snow_water:kgm2',\n",
      "                  'relative_humidity_1000hPa:p',\n",
      "                            'fresh_snow_24h:cm',\n",
      "                             'cloud_base_agl:m',\n",
      "                            'fresh_snow_12h:cm',\n",
      "                                'snow_depth:cm',\n",
      "                              'dew_or_rime:idx',\n",
      "                             'fresh_snow_6h:cm',\n",
      "               'super_cooled_liquid_water:kgm2',\n",
      "                             'fresh_snow_3h:cm',\n",
      "                              'rain_water:kgm2',\n",
      "                         'precip_type_5min:idx',\n",
      "                               'precip_5min:mm',\n",
      "                             'fresh_snow_1h:cm',\n",
      "                                'sun_azimuth:d',\n",
      "                             'msl_pressure:hPa',\n",
      "                            'pressure_100m:hPa',\n",
      "                             'pressure_50m:hPa',\n",
      "                             'sfc_pressure:hPa',\n",
      "                                  'prob_rime:p',\n",
      "                            'wind_speed_10m:ms',\n",
      "                                  'elevation:m',\n",
      "                               'snow_drift:idx',\n",
      "                           'snow_melt_10min:mm',\n",
      "                      'wind_speed_w_1000hPa:ms',\n",
      "                        'observed_or_estimated',\n",
      "                                     'hour_cos',\n",
      "                                     'hour_sin',\n",
      "                                    'month_sin',\n",
      "                                    'month_cos',\n",
      "                                  'day-of-year',\n",
      "                          'direct_rad:W_lag_1h',\n",
      "                  'direct_rad:W_lag_forward_1h',\n",
      "                       'clear_sky_rad:W_lag_1h',\n",
      "               'clear_sky_rad:W_lag_forward_1h',\n",
      "                         'diffuse_rad:W_lag_1h',\n",
      "                 'diffuse_rad:W_lag_forward_1h',\n",
      "                       'direct_rad_1h:J_lag_1h',\n",
      "               'direct_rad_1h:J_lag_forward_1h',\n",
      "                      'is_in_shadow:idx_lag_1h',\n",
      "              'is_in_shadow:idx_lag_forward_1h',\n",
      "                 'clear_sky_energy_1h:J_lag_1h',\n",
      "         'clear_sky_energy_1h:J_lag_forward_1h',\n",
      "               'effective_cloud_cover:p_lag_1h',\n",
      "       'effective_cloud_cover:p_lag_forward_1h',\n",
      "                          'visibility:m_lag_1h',\n",
      "                  'visibility:m_lag_forward_1h',\n",
      "                   'total_cloud_cover:p_lag_1h',\n",
      "           'total_cloud_cover:p_lag_forward_1h',\n",
      "                                     'location',\n",
      "                                    'old_index',\n",
      "                                              0],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/features/generators/fillna.py:58: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n",
      "  X.fillna(self._fillna_feature_map, inplace=True, downcast=False)\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  4 | ['is_in_shadow:idx', 'is_day:idx', 'dew_or_rime:idx', 'observed_or_estimated']\n",
      "\t\t('float', [])    : 64 | ['direct_rad:W', 'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad_1h:J', 'clear_sky_energy_1h:J', ...]\n",
      "\t\t('object', [])   :  1 | ['location']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :  4 | ['is_in_shadow:idx', 'is_day:idx', 'dew_or_rime:idx', 'location']\n",
      "\t\t('float', [])     : 64 | ['direct_rad:W', 'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad_1h:J', 'clear_sky_energy_1h:J', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['observed_or_estimated']\n",
      "\t1.1s = Fit runtime\n",
      "\t69 features in original data used to generate 69 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 69.82 MB (2.7% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.16s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
      "\t-840.9638\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.13s\t = Training   runtime\n",
      "\t55.57s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ...\n",
      "\t-999.2449\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.13s\t = Training   runtime\n",
      "\t59.94s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ...\n",
      "2023-11-08 23:24:48,771\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 17.22% memory usage per fold, 68.89%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=0.17%)\n",
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "\tWarning: Exception caused LightGBMXT_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=89806, ip=127.0.0.1)\n",
      "ModuleNotFoundError: No module named 'lightgbm'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=89806, ip=127.0.0.1)\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 854, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 98, in _fit\n",
      "    try_import_lightgbm()  # raise helpful error message if LightGBM isn't installed\n",
      "    ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 84, in try_import_lightgbm\n",
      "    raise ImportError(\"`import lightgbm` failed. \" f\"A quick tip is to install via `pip install autogluon.tabular[lightgbm]=={__version__}`.\")\n",
      "ImportError: `import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==0.8.3b20231108`.\n",
      "Fitting model: LightGBM_BAG_L1 ...\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 17.76% memory usage per fold, 71.02%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=0.18%)\n",
      "\tWarning: Exception caused LightGBM_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=89813, ip=127.0.0.1)\n",
      "ModuleNotFoundError: No module named 'lightgbm'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=89813, ip=127.0.0.1)\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 854, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 98, in _fit\n",
      "    try_import_lightgbm()  # raise helpful error message if LightGBM isn't installed\n",
      "    ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 84, in try_import_lightgbm\n",
      "    raise ImportError(\"`import lightgbm` failed. \" f\"A quick tip is to install via `pip install autogluon.tabular[lightgbm]=={__version__}`.\")\n",
      "ImportError: `import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==0.8.3b20231108`.\n",
      "2023-11-08 23:24:59,514\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=89803, ip=127.0.0.1)\n",
      "ModuleNotFoundError: No module named 'lightgbm'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=89803, ip=127.0.0.1)\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 854, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 98, in _fit\n",
      "    try_import_lightgbm()  # raise helpful error message if LightGBM isn't installed\n",
      "    ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 84, in try_import_lightgbm\n",
      "    raise ImportError(\"`import lightgbm` failed. \" f\"A quick tip is to install via `pip install autogluon.tabular[lightgbm]=={__version__}`.\")\n",
      "ImportError: `import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==0.8.3b20231108`.\n",
      "2023-11-08 23:24:59,515\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: RandomForestMSE_BAG_L1 ...\n",
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "2023-11-08 23:25:05,118\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-11-08 23:25:05,119\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-11-08 23:25:05,120\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 164 due to low memory. Expected memory usage reduced from 27.34% -> 15.0% of available memory...\n",
      "\t-920.8997\t = Validation score   (-root_mean_squared_error)\n",
      "\t178.03s\t = Training   runtime\n",
      "\t3.16s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ...\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 22.02% memory usage per fold, 44.04%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=0.22%)\n",
      "\t-685.2626\t = Validation score   (-root_mean_squared_error)\n",
      "\t295.57s\t = Training   runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ...\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 198 due to low memory. Expected memory usage reduced from 22.68% -> 15.0% of available memory...\n",
      "\t-914.8672\t = Validation score   (-root_mean_squared_error)\n",
      "\t50.24s\t = Training   runtime\n",
      "\t3.59s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 32.19% memory usage per fold, 64.37%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=0.32%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=89967, ip=127.0.0.1)\n",
      "ModuleNotFoundError: No module named 'fastai'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=89967, ip=127.0.0.1)\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 854, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 208, in _fit\n",
      "    try_import_fastai()\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 123, in try_import_fastai\n",
      "    raise ImportError(f\"Import fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]=={__version__}`. \")\n",
      "ImportError: Import fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==0.8.3b20231108`.\n",
      "Fitting model: XGBoost_BAG_L1 ...\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 24.09% memory usage per fold, 48.19%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=0.24%)\n",
      "2023-11-08 23:33:58,877\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-11-08 23:33:58,880\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=89965, ip=127.0.0.1)\n",
      "ModuleNotFoundError: No module named 'fastai'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=89965, ip=127.0.0.1)\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 854, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 208, in _fit\n",
      "    try_import_fastai()\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 123, in try_import_fastai\n",
      "    raise ImportError(f\"Import fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]=={__version__}`. \")\n",
      "ImportError: Import fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==0.8.3b20231108`.\n",
      "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=89977, ip=127.0.0.1)\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 854, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 145, in _fit\n",
      "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1801, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1752, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 854, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 273, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 688, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 661, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 603, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 566, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 532, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
      "                                                                                         ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/ray/_private/worker.py\", line 2524, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=89977, ip=127.0.0.1)\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 854, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 145, in _fit\n",
      "    self.params_trained[\"n_estimators\"] = bst.best_ntree_limit\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'Booster' object has no attribute 'best_ntree_limit'\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.69% memory usage per fold, 46.75%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=0.12%)\n",
      "2023-11-08 23:34:08,889\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\t-792.1023\t = Validation score   (-root_mean_squared_error)\n",
      "\t150.49s\t = Training   runtime\n",
      "\t1.57s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ...\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 18.87% memory usage per fold, 75.46%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=0.19%)\n",
      "\tWarning: Exception caused LightGBMLarge_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=90100, ip=127.0.0.1)\n",
      "ModuleNotFoundError: No module named 'lightgbm'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=90100, ip=127.0.0.1)\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 854, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 98, in _fit\n",
      "    try_import_lightgbm()  # raise helpful error message if LightGBM isn't installed\n",
      "    ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 84, in try_import_lightgbm\n",
      "    raise ImportError(\"`import lightgbm` failed. \" f\"A quick tip is to install via `pip install autogluon.tabular[lightgbm]=={__version__}`.\")\n",
      "ImportError: `import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==0.8.3b20231108`.\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-685.2626\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.88s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 9 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ...\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 17.76% memory usage per fold, 71.06%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=0.18%)\n",
      "\tWarning: Exception caused LightGBMXT_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=90096, ip=127.0.0.1)\n",
      "ModuleNotFoundError: No module named 'lightgbm'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=90096, ip=127.0.0.1)\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 854, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 98, in _fit\n",
      "    try_import_lightgbm()  # raise helpful error message if LightGBM isn't installed\n",
      "    ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 84, in try_import_lightgbm\n",
      "    raise ImportError(\"`import lightgbm` failed. \" f\"A quick tip is to install via `pip install autogluon.tabular[lightgbm]=={__version__}`.\")\n",
      "ImportError: `import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==0.8.3b20231108`.\n",
      "2023-11-08 23:36:39,110\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-11-08 23:36:39,112\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-11-08 23:36:39,112\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: LightGBM_BAG_L2 ...\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 17.47% memory usage per fold, 69.87%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=0.17%)\n",
      "\tWarning: Exception caused LightGBM_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=90110, ip=127.0.0.1)\n",
      "ModuleNotFoundError: No module named 'lightgbm'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=90110, ip=127.0.0.1)\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 854, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 98, in _fit\n",
      "    try_import_lightgbm()  # raise helpful error message if LightGBM isn't installed\n",
      "    ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 84, in try_import_lightgbm\n",
      "    raise ImportError(\"`import lightgbm` failed. \" f\"A quick tip is to install via `pip install autogluon.tabular[lightgbm]=={__version__}`.\")\n",
      "ImportError: `import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==0.8.3b20231108`.\n",
      "2023-11-08 23:36:40,243\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-11-08 23:36:40,245\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-11-08 23:36:40,245\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: RandomForestMSE_BAG_L2 ...\n",
      "2023-11-08 23:36:46,024\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-11-08 23:36:46,025\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2023-11-08 23:36:46,026\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 187 due to low memory. Expected memory usage reduced from 23.97% -> 15.0% of available memory...\n",
      "\t-353.1365\t = Validation score   (-root_mean_squared_error)\n",
      "\t241.83s\t = Training   runtime\n",
      "\t3.39s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ...\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 20.73% memory usage per fold, 41.47%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=0.21%)\n"
     ]
    }
   ],
   "source": [
    "def train_model(X_train, y_train, X_val, y_val):\n",
    "    # Define the path where the AutoGluon models will be saved\n",
    "    save_path = f'autogluon_models/test_model'\n",
    "    \n",
    "    # Initialize the TabularPredictor object\n",
    "    model = TabularPredictor(label='pv_measurement', path=save_path)\n",
    "    X_train['old_index'] = X_train.index\n",
    "    X_val['old_index'] = X_val.index\n",
    "    y_train['old_index'] = y_train.index\n",
    "    y_val['old_index'] = y_val.index\n",
    "    train_data = pd.merge(X_train, y_train, left_on=\"old_index\", right_on=\"old_index\", how='inner')\n",
    "    val_data = pd.merge(X_val, y_val, left_on=\"old_index\", right_on=\"old_index\", how='inner')\n",
    "\n",
    "    print(train_data.columns)\n",
    "\n",
    "    train_data.drop(\"old_index\", axis=1, inplace=True)\n",
    "    val_data.drop(\"old_index\", axis=1, inplace=True)\n",
    "\n",
    "    # rename column \"0\" to \"pv_measurement\"\n",
    "    train_data.rename(columns={0: \"pv_measurement\"}, inplace=True)\n",
    "    val_data.rename(columns={0: \"pv_measurement\"}, inplace=True)\n",
    "\n",
    "    # Fit the model using the training data\n",
    "    # AutoGluon automatically handles validation if you pass the val data as a tuple\n",
    "    # model.fit(train_data=train_data, presets='best_quality')\n",
    "    model.fit(train_data=train_data, presets='best_quality')\n",
    "    \n",
    "    # Evaluate the trained model on validation data\n",
    "    performance = model.evaluate(X_val)\n",
    "    print(f\"Location {location}, Mean Absolute Error on Test Data: {performance['mean_absolute_error']}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(X_val, y_val, location, model=None):\n",
    "    if model is None:\n",
    "        # If no model is passed, we assume the model has been previously saved and needs to be loaded\n",
    "        save_path = f'autogluon_models_location_{location}'\n",
    "        model = TabularPredictor.load(save_path)\n",
    "\n",
    "    # Predictions are made on the non-transformed validation data\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # If y_val was transformed (e.g., log1p), then apply the inverse transformation\n",
    "    y_val = np.expm1(y_val)\n",
    "    y_pred = np.expm1(y_pred)\n",
    "    \n",
    "    # Calculate the MAE\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    print(f'Location {location}, Mean Absolute Error: {mae}')\n",
    "\n",
    "# Train the model using all available training data and the initial validation set for early stopping\n",
    "model = train_model(combined_X_train, combined_Y_train, combined_X_val, combined_Y_val)\n",
    "\n",
    "# Evaluate the model using the same validation set\n",
    "# evaluate_model(combined_X_val, combined_Y_val, location, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming you have defined WEATHER_FEATURES, TEST_COLUMNS_TO_KEEP, and other functions previously\n",
    "\n",
    "def make_predictions(df_test_pred):\n",
    "    \n",
    "    # Load model \n",
    "    model = CatBoostRegressor()\n",
    "    model.load_model(f\"catboost_model_merged.cbm\")\n",
    "\n",
    "    preds = model.predict(df_test_pred)\n",
    "    \n",
    "    # Inverse transform the predictions\n",
    "    # preds = np.expm1(preds)\n",
    "    return preds\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate locally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "make_predictions() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/simenseeberg/Documents/ntnu/h23/tdt4173/project/TDT4173-project/catboost_model_merged_vol3.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simenseeberg/Documents/ntnu/h23/tdt4173/project/TDT4173-project/catboost_model_merged_vol3.ipynb#Y125sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simenseeberg/Documents/ntnu/h23/tdt4173/project/TDT4173-project/catboost_model_merged_vol3.ipynb#Y125sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mfor\u001b[39;00m loc \u001b[39min\u001b[39;00m locations:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/simenseeberg/Documents/ntnu/h23/tdt4173/project/TDT4173-project/catboost_model_merged_vol3.ipynb#Y125sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     evaluate_model_locally(loc, scalers)\n",
      "\u001b[1;32m/Users/simenseeberg/Documents/ntnu/h23/tdt4173/project/TDT4173-project/catboost_model_merged_vol3.ipynb Cell 12\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/simenseeberg/Documents/ntnu/h23/tdt4173/project/TDT4173-project/catboost_model_merged_vol3.ipynb#Y125sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m target_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_parquet(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdata/\u001b[39m\u001b[39m{\u001b[39;00mlocation\u001b[39m}\u001b[39;00m\u001b[39m/train_targets.parquet\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/simenseeberg/Documents/ntnu/h23/tdt4173/project/TDT4173-project/catboost_model_merged_vol3.ipynb#Y125sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Make predictions\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/simenseeberg/Documents/ntnu/h23/tdt4173/project/TDT4173-project/catboost_model_merged_vol3.ipynb#Y125sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m preds \u001b[39m=\u001b[39m make_predictions(X_training, location)[\u001b[39m-\u001b[39m\u001b[39m720\u001b[39m:]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simenseeberg/Documents/ntnu/h23/tdt4173/project/TDT4173-project/catboost_model_merged_vol3.ipynb#Y125sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m target \u001b[39m=\u001b[39m target_df\u001b[39m.\u001b[39mtail(\u001b[39m720\u001b[39m)[\u001b[39m\"\u001b[39m\u001b[39mpv_measurement\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto_numpy()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simenseeberg/Documents/ntnu/h23/tdt4173/project/TDT4173-project/catboost_model_merged_vol3.ipynb#Y125sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m differences \u001b[39m=\u001b[39m preds \u001b[39m-\u001b[39m target\n",
      "\u001b[0;31mTypeError\u001b[0m: make_predictions() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def evaluate_model_locally(location, scalers):\n",
    "    # Load the test data\n",
    "    target_df = pd.read_parquet(f\"data/{location}/train_targets.parquet\")\n",
    "    \n",
    "    # Make predictions\n",
    "    preds = make_predictions(X_training)[-720:]\n",
    "    target = target_df.tail(720)[\"pv_measurement\"].to_numpy()\n",
    "    \n",
    "    differences = preds - target\n",
    "    # Count predictions lower than the actual\n",
    "    lower_predictions = (differences < 0) & (target != 0)\n",
    "    # Count predictions higher than the actual\n",
    "    higher_predictions = (differences > 0) & (target != 0)\n",
    "\n",
    "    # Biggest misreads\n",
    "    absolute_differences = abs(differences)\n",
    "    max_diff_index = absolute_differences.argmax()  # Index of the biggest difference\n",
    "    max_diff_value = absolute_differences[max_diff_index]  # Value of the biggest difference\n",
    "    print(f\"Number of predictions that are a lower value than the actual, given that the actual is not 0: {lower_predictions.sum()}\")\n",
    "    print(f\"Number of predictions that are larger than the target, given that the target is not 0: {higher_predictions.sum()}\")\n",
    "    print(f\"The biggest misread is at index {max_diff_index} with a difference of {max_diff_value}\")\n",
    "    \n",
    "    index = target_df.index[-720:]\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(60,6))\n",
    "    plt.plot(index, target, label=\"Target\")\n",
    "    plt.plot(index, preds, label=\"Predictions\")\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Target vs Predictions')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "    plt.show()\n",
    "\n",
    "for loc in locations:\n",
    "    evaluate_model_locally(loc, scalers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit to csv 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160\n",
      "2160\n"
     ]
    }
   ],
   "source": [
    "df_submission = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "preds = make_predictions(combined_X_test)\n",
    "print(len(preds))\n",
    "print(len(df_submission))\n",
    "df_submission[\"prediction\"] = preds\n",
    "\n",
    "# Save the results to a new submission file\n",
    "df_submission[[\"id\", \"prediction\"]].to_csv(\"kaggle_submission_catboost_10.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
