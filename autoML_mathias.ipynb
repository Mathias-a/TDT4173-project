{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import autogluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constans and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = [\"A\", \"B\", \"C\"]\n",
    "features_order = []\n",
    "\n",
    "LAGGED_COLUMNS_TO_KEEP = [\n",
    "    'direct_rad:W_lag_1h', \n",
    "    'direct_rad:W_lag_forward_1h', \n",
    "    'clear_sky_rad:W_lag_1h', \n",
    "    'clear_sky_rad:W_lag_forward_1h', \n",
    "    'diffuse_rad:W_lag_1h', \n",
    "    'diffuse_rad:W_lag_forward_1h', \n",
    "    'direct_rad_1h:J_lag_1h', \n",
    "    'direct_rad_1h:J_lag_forward_1h', \n",
    "    'is_in_shadow:idx_lag_1h', \n",
    "    'is_in_shadow:idx_lag_forward_1h', \n",
    "    'clear_sky_energy_1h:J_lag_1h', \n",
    "    'clear_sky_energy_1h:J_lag_forward_1h', \n",
    "    'effective_cloud_cover:p_lag_1h', \n",
    "    'effective_cloud_cover:p_lag_forward_1h', \n",
    "    'visibility:m_lag_1h', \n",
    "    'visibility:m_lag_forward_1h', \n",
    "    'total_cloud_cover:p_lag_1h', \n",
    "    'total_cloud_cover:p_lag_forward_1h', \n",
    "\n",
    "\n",
    "    # 'direct_rad:W_lag_2h', \n",
    "    # 'direct_rad:W_lag_forward_2h', \n",
    "    # 'clear_sky_rad:W_lag_2h', \n",
    "    # 'clear_sky_rad:W_lag_forward_2h', \n",
    "    # 'diffuse_rad:W_lag_2h', \n",
    "    # 'diffuse_rad:W_lag_forward_2h', \n",
    "    # 'direct_rad_1h:J_lag_2h', \n",
    "    # 'direct_rad_1h:J_lag_forward_2h', \n",
    "    # 'is_in_shadow:idx_lag_2h', \n",
    "    # 'is_in_shadow:idx_lag_forward_2h', \n",
    "    # 'clear_sky_energy_1h:J_lag_2h', \n",
    "    # 'clear_sky_energy_1h:J_lag_forward_2h', \n",
    "    # 'effective_cloud_cover:p_lag_2h', \n",
    "    # 'effective_cloud_cover:p_lag_forward_2h', \n",
    "    # 'visibility:m_lag_2h', \n",
    "    # 'visibility:m_lag_forward_2h', \n",
    "    # 'total_cloud_cover:p_lag_2h', \n",
    "    # 'total_cloud_cover:p_lag_forward_2h', \n",
    "\n",
    "    # 'direct_rad:W_lag_3h', \n",
    "    # 'direct_rad:W_lag_forward_3h', \n",
    "    # 'clear_sky_rad:W_lag_3h', \n",
    "    # 'clear_sky_rad:W_lag_forward_3h', \n",
    "    # 'diffuse_rad:W_lag_3h', \n",
    "    # 'diffuse_rad:W_lag_forward_3h', \n",
    "    # 'direct_rad_1h:J_lag_3h', \n",
    "    # 'direct_rad_1h:J_lag_forward_3h', \n",
    "    # 'is_in_shadow:idx_lag_3h', \n",
    "    # 'is_in_shadow:idx_lag_forward_3h', \n",
    "    # 'clear_sky_energy_1h:J_lag_3h', \n",
    "    # 'clear_sky_energy_1h:J_lag_forward_3h', \n",
    "    # 'effective_cloud_cover:p_lag_3h', \n",
    "    # 'effective_cloud_cover:p_lag_forward_3h', \n",
    "    # 'visibility:m_lag_3h', \n",
    "    # 'visibility:m_lag_forward_3h', \n",
    "    # 'total_cloud_cover:p_lag_3h', \n",
    "    # 'total_cloud_cover:p_lag_forward_3h'\n",
    "]\n",
    "\n",
    "CUSTOM_COLUMNS_TO_KEEP = [\n",
    "    \"hour_cos\",\n",
    "    \"hour_sin\",\n",
    "    \"month_sin\",\n",
    "    \"month_cos\",\n",
    "    \"day-of-year\",\n",
    "]\n",
    "\n",
    "WEATHER_FEATURES = [\n",
    "    \"direct_rad:W\",\n",
    "    \"clear_sky_rad:W\",\n",
    "    \"diffuse_rad:W\",\n",
    "    \"direct_rad_1h:J\",\n",
    "    \"is_in_shadow:idx\",\n",
    "    \"clear_sky_energy_1h:J\",\n",
    "    \"effective_cloud_cover:p\",\n",
    "    \"visibility:m\",\n",
    "    \"total_cloud_cover:p\",\n",
    "]\n",
    "\n",
    "\n",
    "TEST_COLUMNS_TO_KEEP = [\n",
    "    \"direct_rad:W\",\n",
    "    \"clear_sky_rad:W\",\n",
    "    \"diffuse_rad:W\",\n",
    "    \"direct_rad_1h:J\",\n",
    "    \"is_in_shadow:idx\",\n",
    "    \"clear_sky_energy_1h:J\",\n",
    "    \"diffuse_rad_1h:J\",\n",
    "    \"is_day:idx\",\n",
    "    \"sun_elevation:d\",\n",
    "    \"ceiling_height_agl:m\",\n",
    "    \"effective_cloud_cover:p\",\n",
    "    \"visibility:m\",\n",
    "    \"total_cloud_cover:p\",\n",
    "    \"air_density_2m:kgm3\",\n",
    "    \"wind_speed_v_10m:ms\",\n",
    "    \"dew_point_2m:K\",\n",
    "    \"wind_speed_u_10m:ms\",\n",
    "    \"t_1000hPa:K\",\n",
    "    \"absolute_humidity_2m:gm3\",\n",
    "    \"snow_water:kgm2\",\n",
    "    \"relative_humidity_1000hPa:p\",\n",
    "    \"fresh_snow_24h:cm\",\n",
    "    \"cloud_base_agl:m\",\n",
    "    \"fresh_snow_12h:cm\",\n",
    "    \"snow_depth:cm\",\n",
    "    \"dew_or_rime:idx\",\n",
    "    \"fresh_snow_6h:cm\",\n",
    "    \"super_cooled_liquid_water:kgm2\",\n",
    "    \"fresh_snow_3h:cm\",\n",
    "    \"rain_water:kgm2\",\n",
    "    \"precip_type_5min:idx\",\n",
    "    \"precip_5min:mm\",\n",
    "    \"fresh_snow_1h:cm\",\n",
    "    \"sun_azimuth:d\",\n",
    "    \"msl_pressure:hPa\",\n",
    "    \"pressure_100m:hPa\",\n",
    "    \"pressure_50m:hPa\",\n",
    "    \"sfc_pressure:hPa\",\n",
    "    \"prob_rime:p\",\n",
    "    \"wind_speed_10m:ms\",\n",
    "    # \"elevation:m\",\n",
    "    # \"snow_density:kgm3\",\n",
    "    # \"snow_drift:idx\",\n",
    "    \"snow_melt_10min:mm\",\n",
    "    \"wind_speed_w_1000hPa:ms\",\n",
    "    \"observed_or_estimated\"\n",
    "    # \"location_A\",\n",
    "    # \"location_B\",\n",
    "    # \"location_C\",\n",
    "    # \"date_calc\",\n",
    "] + CUSTOM_COLUMNS_TO_KEEP  +  LAGGED_COLUMNS_TO_KEEP\n",
    "\n",
    "COLUMNS_TO_KEEP = TEST_COLUMNS_TO_KEEP + [\"pv_measurement\"]\n",
    "\n",
    "\n",
    "\n",
    "def create_weather_lagged_features(df, weather_features):\n",
    "    # Choose the weather features for which you want to create lagged versions\n",
    "    for feature in weather_features:\n",
    "        # Assuming hourly data, adjust the lags for your specific dataset\n",
    "        # Creating lagged features for 1 hour, 1 day, and 1 week\n",
    "        df[f'{feature}_lag_1h'] = df[feature].shift(1)\n",
    "        # df[f'{feature}_lag_2h'] = df[feature].shift(2)\n",
    "        # df[f'{feature}_lag_3h'] = df[feature].shift(3)\n",
    "\n",
    "        df[f'{feature}_lag_forward_1h'] = df[feature].shift(-1)\n",
    "        # df[f'{feature}_lag_forward_2h'] = df[feature].shift(-2)\n",
    "        # df[f'{feature}_lag_forward_3h'] = df[feature].shift(-3)\n",
    "        # df[f'{feature}_lag_24h'] = df[feature].shift(24*4)\n",
    "        # df[f'{feature}_lag_168h'] = df[feature].shift(24 * 7 * 4 * 365)\n",
    "        # df[f'{feature}_front_lag_1h'] = df[feature].shift(-4)\n",
    "        # df[f'{feature}_front_lag_24h'] = df[feature].shift(-24*4)\n",
    "\n",
    "\n",
    "    # Handling edges by filling NaNs with appropriate values or dropping them\n",
    "    # You may choose to fill with zeroes or interpolate, based on what makes more sense for your data\n",
    "    df.fillna(method='ffill', inplace=True)  # Forward fill\n",
    "    df.fillna(method='bfill', inplace=True)  # Backward fill\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_lagged_features(df, column_name='pv_measurement'):\n",
    "    # Assuming 'date_forecast' is the datetime column used for sorting\n",
    "\n",
    "    df[f'{column_name}_prev_month'] = df[column_name].shift(24*7) # previous week\n",
    "\n",
    "    # For yearly lag, you would need to calculate the number of observations per year\n",
    "    # If the data is not consistent (leap years, etc.), you may need a more complex method\n",
    "    # Here's a simple version assuming 365 days a year:\n",
    "    df[f'{column_name}_prev_year'] = df[column_name].shift(24*365) # previous year\n",
    "    df[f'{column_name}_2years_ago'] = df[column_name].shift(24*365*2) # next year\n",
    "\n",
    "    # Handling edges by filling NaNs with appropriate values or dropping them\n",
    "    df.fillna(method='ffill', inplace=True)  # Forward fill\n",
    "    df.fillna(method='bfill', inplace=True)  # Backward fill\n",
    "\n",
    "    return df\n",
    "\n",
    "B_SCALE_VALUE = 6.3\n",
    "C_SCALE_VALUE = 8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_custom_fields(df):\n",
    "     df['hour_sin'] = np.sin(2 * np.pi * df['date_forecast'].dt.hour / 24)\n",
    "     df['hour_cos'] = np.cos(2 * np.pi * df['date_forecast'].dt.hour / 24)\n",
    "\n",
    "     df['month_sin'] = np.sin(2 * np.pi * df['date_forecast'].dt.month / 12)\n",
    "     df['month_cos'] = np.cos(2 * np.pi * df['date_forecast'].dt.month / 12)\n",
    "     df['day-of-year'] = df['date_forecast'].dt.dayofyear\n",
    "     return df\n",
    "\n",
    "def remove_outliers(df):\n",
    "    # Use a mask to filter out the rows where rolling std is zero but keep the rows where the value itself is zero\n",
    "    mask = (df['pv_measurement'].rolling(5).std() == 0) & (df['pv_measurement'] != 0)\n",
    "    df = df[~mask]\n",
    "    return df\n",
    "\n",
    "def resample_add_data(df, is_test_data):\n",
    "    df = add_custom_fields(df)\n",
    "    df.set_index('date_forecast', inplace=True)\n",
    "    df = df.resample('1H').mean()\n",
    "    # Remove empty dates if test data\n",
    "    if is_test_data:\n",
    "        non_nan_threshold = len(df.columns) // 2  \n",
    "        df.dropna(thresh=non_nan_threshold, inplace=True)\n",
    "    df.interpolate(method=\"linear\", inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_location_feature(X, location):\n",
    "      # Treat location as a categorical feature by converting it to a category type\n",
    "    X['location'] = location\n",
    "    X['location'] = X['location'].astype(str)  # Convert to string if 'location' is not an int\n",
    "    X['dew_or_rime:idx'] = X['dew_or_rime:idx'].astype(str)\n",
    "    X['is_day:idx'] = X['is_day:idx'].astype(str)\n",
    "    X['is_in_shadow:idx'] = X['is_in_shadow:idx'].astype(str)\n",
    "    categorical_columns = ['location', 'dew_or_rime:idx', 'is_day:idx', 'is_in_shadow:idx']\n",
    "\n",
    "    # Before filling NaN values, add 'missing' as a category for each categorical column.\n",
    "    for column in categorical_columns:\n",
    "        X[column] = X[column].astype('category')  # Ensure the column is of type 'category'.\n",
    "        if 'missing' not in X[column].cat.categories:\n",
    "            X[column] = X[column].cat.add_categories(['missing'])  # Add 'missing' as a new category.\n",
    "        X[column] = X[column].fillna('missing')\n",
    "    X['location'] = X['location'].astype('category')\n",
    "    X['dew_or_rime:idx'] = X['dew_or_rime:idx'].astype('category')\n",
    "    X['is_day:idx'] = X['is_day:idx'].astype('category')\n",
    "    X['is_in_shadow:idx'] = X['is_in_shadow:idx'].astype('category')\n",
    "    \n",
    "    return X\n",
    "\n",
    "def add_scaling(X_test, X_training, location):\n",
    "    global scalers\n",
    "    continuous_columns = X_training.select_dtypes(include=['float32', 'int32']).columns\n",
    "    if location not in scalers:\n",
    "        scalers[location] = MinMaxScaler()\n",
    "    X_training[continuous_columns] = scalers[location].fit_transform(X_training[continuous_columns])\n",
    "    \n",
    "    X_test[continuous_columns] = scalers[location].transform(X_test[continuous_columns])\n",
    "\n",
    "    return X_test, X_training\n",
    "\n",
    "def make_observed_and_estimated_category(df_observed, df_estimated, df_test):\n",
    "     # Hot encode in wether observed or estimated\n",
    "    df_observed['observed_or_estimated'] = 'observed'\n",
    "    df_estimated['observed_or_estimated'] = 'estimated'\n",
    "    df_test['observed_or_estimated'] = 'estimated'\n",
    "    # Concatenate observed and estimated\n",
    "    df_training = pd.concat([df_observed, df_estimated], axis=0).sort_values(by=\"date_forecast\")\n",
    "    df_training['observed_or_estimated'] = df_training['observed_or_estimated'].astype('category')\n",
    "    df_test['observed_or_estimated'] = df_test['observed_or_estimated'].astype('category')\n",
    "\n",
    "    return df_training, df_test\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_70938/620350683.py:162: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)  # Forward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_70938/620350683.py:163: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)  # Backward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_70938/620350683.py:162: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)  # Forward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_70938/620350683.py:163: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)  # Backward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_70938/620350683.py:162: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)  # Forward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_70938/620350683.py:163: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)  # Backward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_70938/620350683.py:162: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)  # Forward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_70938/620350683.py:163: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)  # Backward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_70938/620350683.py:162: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)  # Forward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_70938/620350683.py:163: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)  # Backward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_70938/620350683.py:162: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)  # Forward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_70938/620350683.py:163: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)  # Backward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_70938/620350683.py:162: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)  # Forward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_70938/620350683.py:163: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)  # Backward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_70938/620350683.py:162: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)  # Forward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_70938/620350683.py:163: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)  # Backward fill\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_70938/620350683.py:162: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)  # Forward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_70938/620350683.py:163: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)  # Backward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_70938/620350683.py:162: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)  # Forward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_70938/620350683.py:163: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)  # Backward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_70938/620350683.py:162: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)  # Forward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_70938/620350683.py:163: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)  # Backward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_70938/620350683.py:162: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)  # Forward fill\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_70938/620350683.py:163: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)  # Backward fill\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize a dictionary to hold the scalers for each location\n",
    "\n",
    "scalers = {}\n",
    "\n",
    "\n",
    "def prepare_data(location):\n",
    "    # Load data\n",
    "    scaling = False  # Set scaling to True to enable individual scaling for each location\n",
    "    global scalers\n",
    "    global scale_target \n",
    "    scale_target = False\n",
    "\n",
    "    # Load training data\n",
    "    df_observed = pd.read_parquet(f\"data/{location}/X_train_observed.parquet\")\n",
    "    df_estimated = pd.read_parquet(f\"data/{location}/X_train_estimated.parquet\")\n",
    "    df_target = pd.read_parquet(f\"data/{location}/train_targets.parquet\")\n",
    "\n",
    "   \n",
    "    # Load test data\n",
    "    df_test = pd.read_parquet(f\"data/{location}/X_test_estimated.parquet\")\n",
    "\n",
    "   \n",
    "    # Hot encode in wether observed or estimated\n",
    "\n",
    "    # Resample and add custom fields\n",
    "\n",
    "    df_observed = resample_add_data(df_observed, False)\n",
    "    df_estimated = resample_add_data(df_estimated, False)\n",
    "    df_test = resample_add_data(df_test, True)\n",
    "\n",
    "    df_training, df_test = make_observed_and_estimated_category(df_observed, df_estimated, df_test)\n",
    "\n",
    "    if scale_target:\n",
    "        if location == \"B\":\n",
    "            df_target[\"pv_measurement\"] = df_target[\"pv_measurement\"] * B_SCALE_VALUE\n",
    "        elif location == \"C\":\n",
    "            df_target[\"pv_measurement\"] = df_target[\"pv_measurement\"] * C_SCALE_VALUE\n",
    "\n",
    "    # Merge training with target data\n",
    "    df_training = pd.merge(df_training, df_target, left_on=\"date_forecast\", right_on=\"time\", how=\"inner\")\n",
    "    \n",
    "    # Create lagged features and remove outliers training\n",
    "    df_training = create_weather_lagged_features(df_training, WEATHER_FEATURES)\n",
    "    df_training = df_training[COLUMNS_TO_KEEP]\n",
    "    df_training = remove_outliers(df_training)\n",
    "\n",
    "    df_test = create_weather_lagged_features(df_test, WEATHER_FEATURES)\n",
    "    df_test = df_test[TEST_COLUMNS_TO_KEEP]\n",
    "\n",
    "\n",
    "    # Add categories\n",
    "    df_training = add_location_feature(df_training, location)\n",
    "    X_test = add_location_feature(df_test, location)\n",
    "    \n",
    "    # Add scaling\n",
    "    if scaling:\n",
    "        X_test, X_training = add_scaling(X_test, X_training, location)\n",
    "    df_test.reset_index(inplace=True)\n",
    "    df_test.drop(columns=[\"date_forecast\"], inplace=True)\n",
    "    # y_training = np.log1p(y_training)\n",
    "    return df_training, X_test\n",
    "\n",
    "\n",
    "\n",
    "# Use prepare_data function\n",
    "\n",
    "combined_df_train = []\n",
    "combined_df_test = []\n",
    "combined_df_validation = []\n",
    "\n",
    "for location in locations:\n",
    "    # Prepare the training data\n",
    "    X_training, X_test = prepare_data(location)\n",
    "\n",
    "    df_training, df_test = prepare_data(location)\n",
    "    X_training, X_validation = train_test_split(df_training, test_size=1440, shuffle=False)\n",
    "    \n",
    "    combined_df_train.append(X_training)\n",
    "    combined_df_validation.append(X_validation)\n",
    "\n",
    "    combined_df_test.append(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of these hyperparameters have been found by experimenting with some standard parameters in AutoGluon, and then only using the best ones for each location to make the train time shorter\n",
    "lgbmXT = {'extra_trees': True, 'learning_rate': 0.05, 'num_leaves': 40, 'max_depth': 15, 'ag_args': {'name_suffix': 'XT'}}\n",
    "custom_fastai = {'bs': 520, 'lr': 0.01, 'epochs': 48, 'layers': [200, 100, 50], 'ag_args': {'name_suffix': 'custom'}}\n",
    "hyperparameters = {\n",
    "    'NN_TORCH': {},\n",
    "    'GBM': [lgbmXT, 'GBMLarge'],\n",
    "    'KNN': [{'weights': 'uniform'}],\n",
    "    'FASTAI': [custom_fastai],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"autogluon_models/test_modelA\"\n",
      "Presets specified: ['experimental_zeroshot_hpo_hybrid']\n",
      "/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=3\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"autogluon_models/test_modelA\"\n",
      "AutoGluon Version:  0.8.3b20231109\n",
      "Python Version:     3.11.6\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 21.6.0: Sat Jun 18 17:07:28 PDT 2022; root:xnu-8020.140.41~1/RELEASE_ARM64_T8110\n",
      "Disk Space Avail:   48.76 GB / 245.11 GB (19.9%)\n",
      "Train Data Rows:    32645\n",
      "Train Data Columns: 67\n",
      "Tuning Data Rows:    1440\n",
      "Tuning Data Columns: 67\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (5733.42, 0.0, 621.47827, 1155.76418)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/tabular/learner/default_learner.py:215: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\n",
      "/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/tabular/learner/default_learner.py:223: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1758.05 MB\n",
      "\tTrain Data (Original)  Memory Usage: 9.31 MB (0.5% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['location']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  4 | ['is_in_shadow:idx', 'is_day:idx', 'dew_or_rime:idx', 'observed_or_estimated']\n",
      "\t\t('float', [])    : 62 | ['direct_rad:W', 'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad_1h:J', 'clear_sky_energy_1h:J', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :  3 | ['is_in_shadow:idx', 'is_day:idx', 'dew_or_rime:idx']\n",
      "\t\t('float', [])     : 62 | ['direct_rad:W', 'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad_1h:J', 'clear_sky_energy_1h:J', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['observed_or_estimated']\n",
      "\t0.1s = Fit runtime\n",
      "\t66 features in original data used to generate 66 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 9.27 MB (0.5% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.15s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "use_bag_holdout=True, will use tuning_data as holdout (will not be used for early stopping).\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'learning_rate': 0.05, 'num_leaves': 40, 'max_depth': 15, 'ag_args': {'name_suffix': 'XT'}}, 'GBMLarge'],\n",
      "\t'KNN': [{'weights': 'uniform'}],\n",
      "\t'FASTAI': [{'bs': 520, 'lr': 0.01, 'epochs': 48, 'layers': [200, 100, 50], 'ag_args': {'name_suffix': 'custom'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: KNeighbors_BAG_L1 ...\n",
      "\t-275.3844\t = Validation score   (-mean_absolute_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t1.25s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ...\n",
      "\tMemory not enough to fit 24 folds in parallel. Will train 16 folds in parallel instead (Estimated 4.66% memory usage per fold, 74.53%/80.00% total).\n",
      "\tFitting 24 child models (S1F1 - S3F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.05%)\n"
     ]
    }
   ],
   "source": [
    "def train_model(dataset):\n",
    "    # Define the path where the AutoGluon models will be saved\n",
    "    # enumerate all the locations\n",
    "    for index, location in enumerate(locations):\n",
    "        save_path = f\"autogluon_models/test_model{location}\"\n",
    "\n",
    "        # Initialize the TabularPredictor object\n",
    "        model = TabularPredictor(\n",
    "            label=\"pv_measurement\", path=save_path, eval_metric=\"mae\"\n",
    "        )\n",
    "        model.fit(\n",
    "            train_data=combined_df_train[index],\n",
    "            tuning_data=combined_df_validation[index],\n",
    "            presets=\"experimental_zeroshot_hpo_hybrid\",\n",
    "            use_bag_holdout=True,\n",
    "            hyperparameters=hyperparameters,\n",
    "            num_bag_sets=3,\n",
    "        )\n",
    "\n",
    "\n",
    "def evaluate_model(X_val, y_val, location, model=None):\n",
    "    if model is None:\n",
    "        # If no model is passed, we assume the model has been previously saved and needs to be loaded\n",
    "        save_path = f\"autogluon_models_location_{location}\"\n",
    "        # model = TabularPredictor.load(save_path)\n",
    "        model = autogluon.multimodal(save_path)\n",
    "\n",
    "    # Predictions are made on the non-transformed validation data\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    # If y_val was transformed (e.g., log1p), then apply the inverse transformation\n",
    "    # y_val = np.expm1(y_val)\n",
    "    # y_pred = np.expm1(y_pred)\n",
    "\n",
    "    # Calculate the MAE\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    print(f\"Location {location}, Mean Absolute Error: {mae}\")\n",
    "\n",
    "\n",
    "# Train the model using all available training data and the initial validation set for early stopping\n",
    "train_model(combined_df_train)\n",
    "\n",
    "# Evaluate the model using the same validation set\n",
    "# evaluate_model(combined_X_val, combined_Y_val, location, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                   model   score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0    WeightedEnsemble_L3 -222.201533     408.102809  241.963995                0.000203           0.082755            3       True         10\n",
      "1    WeightedEnsemble_L2 -223.313536     308.377791  204.431077                0.001020           0.049250            2       True          5\n",
      "2  NeuralNetTorch_BAG_L1 -229.367630       0.320073   52.797235                0.320073          52.797235            1       True          2\n",
      "3  NeuralNetTorch_BAG_L2 -232.463768     407.877028  272.089651                0.315156          33.084625            2       True          7\n",
      "4      LightGBMXT_BAG_L2 -233.006165     408.102606  241.881240                0.540733           2.876214            2       True          6\n",
      "5   LightGBM_r118_BAG_L2 -235.034971     409.257999  246.269047                1.696126           7.264022            2       True          8\n",
      "6   LightGBM_r118_BAG_L1 -241.195192      68.759812   28.126547               68.759812          28.126547            1       True          3\n",
      "7   LightGBMLarge_BAG_L1 -244.219273     239.296886  123.458045              239.296886         123.458045            1       True          4\n",
      "8   LightGBMLarge_BAG_L2 -245.912980     408.265378  244.524183                0.703505           5.519157            2       True          9\n",
      "9      LightGBMXT_BAG_L1 -249.377221      99.185102   34.623199               99.185102          34.623199            1       True          1\n",
      "Number of models trained: 10\n",
      "Types of models trained:\n",
      "{'StackerEnsembleModel_LGB', 'StackerEnsembleModel_TabularNeuralNetTorch', 'WeightedEnsembleModel'}\n",
      "Bagging used: True  (with 8 folds)\n",
      "Multi-layer stack-ensembling used: True  (with 3 levels)\n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', [])  :  3 | ['is_in_shadow:idx', 'is_day:idx', 'dew_or_rime:idx']\n",
      "('float', [])     : 62 | ['direct_rad:W', 'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad_1h:J', 'clear_sky_energy_1h:J', ...]\n",
      "('int', ['bool']) :  1 | ['observed_or_estimated']\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/.venv/lib/python3.11/site-packages/autogluon/core/utils/plots.py:169: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"\n",
      "  warnings.warn('AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_types': {'LightGBMXT_BAG_L1': 'StackerEnsembleModel_LGB',\n",
       "  'NeuralNetTorch_BAG_L1': 'StackerEnsembleModel_TabularNeuralNetTorch',\n",
       "  'LightGBM_r118_BAG_L1': 'StackerEnsembleModel_LGB',\n",
       "  'LightGBMLarge_BAG_L1': 'StackerEnsembleModel_LGB',\n",
       "  'WeightedEnsemble_L2': 'WeightedEnsembleModel',\n",
       "  'LightGBMXT_BAG_L2': 'StackerEnsembleModel_LGB',\n",
       "  'NeuralNetTorch_BAG_L2': 'StackerEnsembleModel_TabularNeuralNetTorch',\n",
       "  'LightGBM_r118_BAG_L2': 'StackerEnsembleModel_LGB',\n",
       "  'LightGBMLarge_BAG_L2': 'StackerEnsembleModel_LGB',\n",
       "  'WeightedEnsemble_L3': 'WeightedEnsembleModel'},\n",
       " 'model_performance': {'LightGBMXT_BAG_L1': -249.37722128563706,\n",
       "  'NeuralNetTorch_BAG_L1': -229.3676299371763,\n",
       "  'LightGBM_r118_BAG_L1': -241.19519219164295,\n",
       "  'LightGBMLarge_BAG_L1': -244.21927276731188,\n",
       "  'WeightedEnsemble_L2': -223.31353625891828,\n",
       "  'LightGBMXT_BAG_L2': -233.0061651661061,\n",
       "  'NeuralNetTorch_BAG_L2': -232.4637676937949,\n",
       "  'LightGBM_r118_BAG_L2': -235.03497127990784,\n",
       "  'LightGBMLarge_BAG_L2': -245.9129803617067,\n",
       "  'WeightedEnsemble_L3': -222.20153286658686},\n",
       " 'model_best': 'WeightedEnsemble_L3',\n",
       " 'model_paths': {'LightGBMXT_BAG_L1': ['LightGBMXT_BAG_L1'],\n",
       "  'NeuralNetTorch_BAG_L1': ['NeuralNetTorch_BAG_L1'],\n",
       "  'LightGBM_r118_BAG_L1': ['LightGBM_r118_BAG_L1'],\n",
       "  'LightGBMLarge_BAG_L1': ['LightGBMLarge_BAG_L1'],\n",
       "  'WeightedEnsemble_L2': ['WeightedEnsemble_L2'],\n",
       "  'LightGBMXT_BAG_L2': ['LightGBMXT_BAG_L2'],\n",
       "  'NeuralNetTorch_BAG_L2': ['NeuralNetTorch_BAG_L2'],\n",
       "  'LightGBM_r118_BAG_L2': ['LightGBM_r118_BAG_L2'],\n",
       "  'LightGBMLarge_BAG_L2': ['LightGBMLarge_BAG_L2'],\n",
       "  'WeightedEnsemble_L3': ['WeightedEnsemble_L3']},\n",
       " 'model_fit_times': {'LightGBMXT_BAG_L1': 34.62319898605347,\n",
       "  'NeuralNetTorch_BAG_L1': 52.79723501205444,\n",
       "  'LightGBM_r118_BAG_L1': 28.12654685974121,\n",
       "  'LightGBMLarge_BAG_L1': 123.45804476737976,\n",
       "  'WeightedEnsemble_L2': 0.049250125885009766,\n",
       "  'LightGBMXT_BAG_L2': 2.876214027404785,\n",
       "  'NeuralNetTorch_BAG_L2': 33.084625005722046,\n",
       "  'LightGBM_r118_BAG_L2': 7.264021873474121,\n",
       "  'LightGBMLarge_BAG_L2': 5.51915717124939,\n",
       "  'WeightedEnsemble_L3': 0.08275485038757324},\n",
       " 'model_pred_times': {'LightGBMXT_BAG_L1': 99.18510222434998,\n",
       "  'NeuralNetTorch_BAG_L1': 0.32007336616516113,\n",
       "  'LightGBM_r118_BAG_L1': 68.75981163978577,\n",
       "  'LightGBMLarge_BAG_L1': 239.29688572883606,\n",
       "  'WeightedEnsemble_L2': 0.0010199546813964844,\n",
       "  'LightGBMXT_BAG_L2': 0.5407330989837646,\n",
       "  'NeuralNetTorch_BAG_L2': 0.3151555061340332,\n",
       "  'LightGBM_r118_BAG_L2': 1.6961259841918945,\n",
       "  'LightGBMLarge_BAG_L2': 0.7035050392150879,\n",
       "  'WeightedEnsemble_L3': 0.0002028942108154297},\n",
       " 'num_bag_folds': 8,\n",
       " 'max_stack_level': 3,\n",
       " 'model_hyperparams': {'LightGBMXT_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'NeuralNetTorch_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'LightGBM_r118_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'LightGBMLarge_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'WeightedEnsemble_L2': {'use_orig_features': False,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'LightGBMXT_BAG_L2': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'NeuralNetTorch_BAG_L2': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'LightGBM_r118_BAG_L2': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'LightGBMLarge_BAG_L2': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'WeightedEnsemble_L3': {'use_orig_features': False,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True}},\n",
       " 'leaderboard':                    model   score_val  pred_time_val    fit_time  \\\n",
       " 0    WeightedEnsemble_L3 -222.201533     408.102809  241.963995   \n",
       " 1    WeightedEnsemble_L2 -223.313536     308.377791  204.431077   \n",
       " 2  NeuralNetTorch_BAG_L1 -229.367630       0.320073   52.797235   \n",
       " 3  NeuralNetTorch_BAG_L2 -232.463768     407.877028  272.089651   \n",
       " 4      LightGBMXT_BAG_L2 -233.006165     408.102606  241.881240   \n",
       " 5   LightGBM_r118_BAG_L2 -235.034971     409.257999  246.269047   \n",
       " 6   LightGBM_r118_BAG_L1 -241.195192      68.759812   28.126547   \n",
       " 7   LightGBMLarge_BAG_L1 -244.219273     239.296886  123.458045   \n",
       " 8   LightGBMLarge_BAG_L2 -245.912980     408.265378  244.524183   \n",
       " 9      LightGBMXT_BAG_L1 -249.377221      99.185102   34.623199   \n",
       " \n",
       "    pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       " 0                0.000203           0.082755            3       True   \n",
       " 1                0.001020           0.049250            2       True   \n",
       " 2                0.320073          52.797235            1       True   \n",
       " 3                0.315156          33.084625            2       True   \n",
       " 4                0.540733           2.876214            2       True   \n",
       " 5                1.696126           7.264022            2       True   \n",
       " 6               68.759812          28.126547            1       True   \n",
       " 7              239.296886         123.458045            1       True   \n",
       " 8                0.703505           5.519157            2       True   \n",
       " 9               99.185102          34.623199            1       True   \n",
       " \n",
       "    fit_order  \n",
       " 0         10  \n",
       " 1          5  \n",
       " 2          2  \n",
       " 3          7  \n",
       " 4          6  \n",
       " 5          8  \n",
       " 6          3  \n",
       " 7          4  \n",
       " 8          9  \n",
       " 9          1  }"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.leaderboard(combined_df_validation, silent=True)\n",
    "# model.feature_importance(combined_df_validation, time_limit=120)\n",
    "\n",
    "predictior = TabularPredictor.load(f\"autogluon_models/test_modelA\", require_version_match=False)\n",
    "\n",
    "\n",
    "# Get the leaderboard\n",
    "leaderboard = predictior.leaderboard(silent=True)\n",
    "\n",
    "# Retrieve the best model's name\n",
    "best_model = leaderboard.iloc[0]['model']\n",
    "predictior.fit_summary(show_plot=True, verbosity=3)\n",
    "\n",
    "# # Retrieve hyperparameters of the best model\n",
    "# best_model_info = predictior.get_model_info(model=best_model)\n",
    "# best_hyperparameters = best_model_info['hyperparameters']\n",
    "\n",
    "# # Optionally, save or print these hyperparameters\n",
    "# print(\"Best Model Hyperparameters:\", best_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming you have defined WEATHER_FEATURES, TEST_COLUMNS_TO_KEEP, and other functions previously\n",
    "\n",
    "def make_predictions(df_test_pred, location):\n",
    "    eval_model = TabularPredictor.load(f\"autogluon_models/test_model{location}\", require_version_match=False)\n",
    "    preds = eval_model.predict(df_test_pred)\n",
    "    return preds\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate locally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/autoML_Mathias.ipynb Cell 14\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/autoML_Mathias.ipynb#X16sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/autoML_Mathias.ipynb#X16sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mfor\u001b[39;00m loc \u001b[39min\u001b[39;00m locations:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/autoML_Mathias.ipynb#X16sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     evaluate_model_locally(loc, scalers)\n",
      "\u001b[1;32m/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/autoML_Mathias.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/autoML_Mathias.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m target_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_parquet(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdata/\u001b[39m\u001b[39m{\u001b[39;00mlocation\u001b[39m}\u001b[39;00m\u001b[39m/train_targets.parquet\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/autoML_Mathias.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Make predictions\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/autoML_Mathias.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# filter x_validate to only include values from location\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/autoML_Mathias.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m pred_dataset \u001b[39m=\u001b[39m combined_df_validation[combined_df_validation[\u001b[39m\"\u001b[39;49m\u001b[39mlocation\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m==\u001b[39m location]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/autoML_Mathias.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m pred_dataset\u001b[39m.\u001b[39mreset_index(inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mathiasaas/NTNU/maskinlaring/project/TDT4173-project/autoML_Mathias.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m preds \u001b[39m=\u001b[39m make_predictions(pred_dataset\u001b[39m.\u001b[39mdrop(\u001b[39m\"\u001b[39m\u001b[39mpv_measurement\u001b[39m\u001b[39m\"\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), location)[\u001b[39m-\u001b[39m\u001b[39m720\u001b[39m:]\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def evaluate_model_locally(location, scalers):\n",
    "    # Load the test data\n",
    "    target_df = pd.read_parquet(f\"data/{location}/train_targets.parquet\")\n",
    "    \n",
    "    # Make predictions\n",
    "    # filter x_validate to only include values from location\n",
    "    pred_dataset = combined_df_validation[combined_df_validation[\"location\"] == location]\n",
    "    pred_dataset.reset_index(inplace=True)\n",
    "\n",
    "    preds = make_predictions(pred_dataset.drop(\"pv_measurement\", axis=1), location)[-720:]\n",
    "    target = target_df.tail(720)[\"pv_measurement\"].to_numpy()\n",
    "    \n",
    "    differences = preds - target\n",
    "    # Count predictions lower than the actual\n",
    "    lower_predictions = (differences < 0) & (target != 0)\n",
    "    # Count predictions higher than the actual\n",
    "    higher_predictions = (differences > 0) & (target != 0)\n",
    "\n",
    "    # Biggest misreads\n",
    "    absolute_differences = abs(differences)\n",
    "    max_diff_index = absolute_differences.argmax()  # Index of the biggest difference\n",
    "    # max_diff_value = absolute_differences[max_diff_index]  # Value of the biggest difference\n",
    "    print(f\"Number of predictions that are a lower value than the actual, given that the actual is not 0: {lower_predictions.sum()}\")\n",
    "    print(f\"Number of predictions that are larger than the target, given that the target is not 0: {higher_predictions.sum()}\")\n",
    "    # print(f\"The biggest misread is at index {max_diff_index} with a difference of {max_diff_value}\")\n",
    "    \n",
    "    index = target_df.index[-720:]\n",
    "    print(f'location: {location}')\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(60,6))\n",
    "    plt.plot(index, target, label=\"Target\")\n",
    "    plt.plot(index, preds, label=\"Predictions\")\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Target vs Predictions')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "    plt.show()\n",
    "\n",
    "for loc in locations:\n",
    "    evaluate_model_locally(loc, scalers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit to csv 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    True\n",
      "1    True\n",
      "2    True\n",
      "3    True\n",
      "4    True\n",
      "Name: location, dtype: bool\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_70938/1193821682.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_submission[\"prediction\"][720*index:720*(index+1)] = preds\n",
      "/var/folders/z2/g8xyn7s12bg5v9jx9vpx997h0000gn/T/ipykernel_70938/1193821682.py:15: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 1.55013785e-01  1.41574830e-01  6.80673301e-01  5.73722000e+01\n",
      "  3.54871216e+02  8.91423462e+02  1.68224780e+03  2.26416846e+03\n",
      "  2.28078687e+03  2.03573828e+03  2.28673096e+03  2.21240625e+03\n",
      "  2.25042822e+03  2.09130859e+03  2.15882446e+03  1.73242468e+03\n",
      "  1.31635352e+03  6.39129089e+02  2.39281906e+02  1.99814491e+01\n",
      "  3.63039732e-01  7.58141056e-02  1.89071095e+00  1.13339388e+00\n",
      "  1.43170446e-01  1.89927369e-01  2.86056948e+00  1.61057175e+02\n",
      "  5.53167725e+02  1.28695825e+03  1.89625244e+03  2.39321948e+03\n",
      "  2.97047363e+03  3.38759570e+03  3.38899146e+03  3.32854883e+03\n",
      "  3.29875635e+03  3.23682056e+03  3.07713623e+03  2.64746777e+03\n",
      "  1.69729810e+03  8.50470459e+02  4.00372803e+02  7.63585815e+01\n",
      "  2.99612737e+00  9.29383457e-01  5.62243640e-01  8.01759541e-01\n",
      " -6.82591796e-01  4.43640292e-01  3.17460670e+01  2.68136597e+02\n",
      "  6.81245972e+02  1.33948816e+03  2.34067041e+03  3.20811914e+03\n",
      "  4.16389160e+03  4.67651562e+03  4.69383643e+03  4.37408594e+03\n",
      "  4.33613428e+03  3.75241406e+03  2.72717212e+03  2.06032739e+03\n",
      "  1.38078711e+03  7.68634216e+02  3.87564789e+02  1.26760361e+02\n",
      "  2.08580041e+00 -2.45418862e-01 -5.23096025e-02 -1.86602145e-01\n",
      "  7.08672523e-01  3.48280132e-01  1.17285910e+01  1.80606049e+02\n",
      "  4.90448181e+02  1.13232788e+03  2.31904175e+03  3.38287036e+03\n",
      "  3.64077637e+03  3.50807715e+03  3.49182935e+03  3.12467554e+03\n",
      "  2.95488770e+03  3.17562622e+03  3.31539502e+03  2.63080713e+03\n",
      "  1.59051172e+03  6.07083801e+02  3.14782501e+02  1.12949631e+02\n",
      "  3.20497227e+00  1.58884093e-01  5.46793491e-02  2.21552029e-01\n",
      " -5.30691624e-01  4.64697391e-01  6.30808907e+01  2.39220810e+02\n",
      "  6.00488953e+02  1.49736401e+03  2.61699170e+03  3.42185156e+03\n",
      "  4.25148682e+03  4.69216113e+03  4.90741406e+03  4.88926074e+03\n",
      "  4.60261133e+03  4.04763379e+03  3.48474146e+03  2.88515332e+03\n",
      "  1.94995251e+03  9.82584595e+02  4.89166412e+02  2.17163223e+02\n",
      "  2.15945702e+01 -1.49638951e-02  1.38641864e-01  1.60128564e-01\n",
      "  2.20853269e-01  7.55906284e-01  7.41357422e+01  2.25704208e+02\n",
      "  6.48828308e+02  1.50382263e+03  2.40596143e+03  3.37628198e+03\n",
      "  3.76039990e+03  3.55886401e+03  3.33758203e+03  2.79333813e+03\n",
      "  2.87177808e+03  3.17786426e+03  2.99501318e+03  2.36524072e+03\n",
      "  1.66967444e+03  8.98304260e+02  4.40826416e+02  2.00356049e+02\n",
      "  1.94150429e+01 -1.17596984e-03  4.16729152e-02 -7.99534917e-02\n",
      " -1.92546546e-02  5.77183366e-01  8.24742126e+01  2.36745499e+02\n",
      "  6.17832764e+02  1.47803137e+03  2.47447095e+03  3.46228613e+03\n",
      "  4.21661035e+03  4.49381250e+03  4.44103906e+03  4.26558252e+03\n",
      "  3.85907373e+03  3.54667993e+03  3.07078076e+03  2.50333032e+03\n",
      "  1.58923584e+03  8.49959656e+02  4.33515991e+02  1.69952026e+02\n",
      "  1.07008781e+01  3.51615041e-01  1.96736008e-01 -2.06907466e-03\n",
      " -2.67497450e-03 -2.73052976e-02  9.46806049e+00  5.98693619e+01\n",
      "  1.13708321e+02  1.81070801e+02  2.56490967e+02  3.96035553e+02\n",
      "  5.88223450e+02  7.45992798e+02  1.00012012e+03  1.45239673e+03\n",
      "  1.76155518e+03  1.57053613e+03  1.58074670e+03  1.32246191e+03\n",
      "  7.66158875e+02  4.85644897e+02  2.95894196e+02  1.64175735e+02\n",
      "  1.89438171e+01 -1.96764797e-01 -1.46865219e-01 -2.68480003e-01\n",
      "  4.78511095e-01  6.13153577e-01  3.94015846e+01  1.76714050e+02\n",
      "  3.63785248e+02  7.68460449e+02  1.18600293e+03  1.73596655e+03\n",
      "  1.97276648e+03  2.10544385e+03  2.20305396e+03  1.96153723e+03\n",
      "  2.48966797e+03  2.35043018e+03  2.16494531e+03  1.18190295e+03\n",
      "  8.81467224e+02  4.29680237e+02  2.01338776e+02  6.04784889e+01\n",
      "  5.20184422e+00 -8.50154102e-01 -7.50964284e-01 -4.38458443e-01\n",
      "  8.24151039e-02  4.50994074e-01  3.08237457e+01  1.08695312e+02\n",
      "  1.82703705e+02  4.05564331e+02  7.67985596e+02  1.41852563e+03\n",
      "  1.89269580e+03  1.57729419e+03  1.55872961e+03  1.35842664e+03\n",
      "  1.04855029e+03  8.70162659e+02  7.82852844e+02  5.49018555e+02\n",
      "  3.90407288e+02  3.49560486e+02  2.30295227e+02  6.73607712e+01\n",
      "  1.17245893e+01  1.70816660e-01  2.95239359e-01  3.04305762e-01\n",
      "  1.48885816e-01  1.64039820e-01  2.19856720e+01  1.01783836e+02\n",
      "  3.10060547e+02  7.21042664e+02  1.18784314e+03  1.42374023e+03\n",
      "  1.72401624e+03  1.52912769e+03  1.84891211e+03  1.86046045e+03\n",
      "  2.17120703e+03  2.12813428e+03  2.19463306e+03  2.09246753e+03\n",
      "  1.50507288e+03  9.01116150e+02  4.71762024e+02  2.29914032e+02\n",
      "  4.31297226e+01  6.97116017e-01  3.08077157e-01  5.40142059e-02\n",
      "  1.31017387e-01  1.24855888e+00  4.80663223e+01  1.68013824e+02\n",
      "  3.45274475e+02  5.74607727e+02  8.94947266e+02  1.00687238e+03\n",
      "  1.32735864e+03  1.26119214e+03  1.58621362e+03  1.08700159e+03\n",
      "  8.56826477e+02  8.13929504e+02  7.85174072e+02  6.78480591e+02\n",
      "  5.27656006e+02  3.23553833e+02  2.18961731e+02  8.48234940e+01\n",
      "  1.21332045e+01 -2.65680142e-02  3.49765420e-02  2.89214641e-01\n",
      "  3.20028722e-01  4.05249691e+00  6.28228645e+01  1.64620483e+02\n",
      "  3.55311798e+02  7.26132690e+02  9.58489197e+02  1.22994141e+03\n",
      "  1.53642456e+03  1.74876135e+03  1.82288110e+03  1.64013586e+03\n",
      "  1.65694348e+03  1.83722949e+03  1.77164526e+03  1.48135083e+03\n",
      "  1.20286548e+03  6.66432495e+02  3.37966431e+02  1.91336029e+02\n",
      "  3.67124252e+01  3.74469399e-01 -2.10392803e-01 -5.76111078e-02\n",
      "  6.68514967e-02  1.95895493e+00  6.43509445e+01  2.05134933e+02\n",
      "  3.75425873e+02  7.12428101e+02  1.06425757e+03  1.50091150e+03\n",
      "  1.78140698e+03  2.12723706e+03  1.95867603e+03  1.98367957e+03\n",
      "  1.48635425e+03  1.38062207e+03  1.08600195e+03  8.66845703e+02\n",
      "  6.68209534e+02  3.80983215e+02  1.38333862e+02  6.04424973e+01\n",
      "  1.23974981e+01  1.06853895e-01 -9.65101197e-02 -4.57390696e-01\n",
      "  5.97704798e-02  7.74702013e-01  2.77819996e+01  7.94424286e+01\n",
      "  2.28891953e+02  4.87852539e+02  6.96740234e+02  9.02132080e+02\n",
      "  1.18724731e+03  1.30881641e+03  1.25531177e+03  1.24381274e+03\n",
      "  1.14298181e+03  1.15017297e+03  1.11781726e+03  9.92141296e+02\n",
      "  7.11393311e+02  4.17871033e+02  2.42111267e+02  1.26153427e+02\n",
      "  2.72985001e+01 -7.16372132e-02 -1.08347662e-01 -2.55359858e-01\n",
      " -4.00527358e-01  2.32281947e+00  5.21160545e+01  1.68381134e+02\n",
      "  3.00558563e+02  5.20775452e+02  8.04867065e+02  9.77806824e+02\n",
      "  1.54530396e+03  1.70636304e+03  1.69633032e+03  1.94680164e+03\n",
      "  1.70025586e+03  1.23448193e+03  1.10781714e+03  9.40603699e+02\n",
      "  8.92920959e+02  5.08008575e+02  2.28994293e+02  8.57740173e+01\n",
      "  1.31130314e+01 -2.46921331e-02 -2.67292075e-02 -8.35705921e-02\n",
      "  2.15141103e-02  3.66229701e+00  6.11822243e+01  1.43753830e+02\n",
      "  2.90179749e+02  4.41092590e+02  5.29725586e+02  6.96135376e+02\n",
      "  8.88917603e+02  1.14010498e+03  1.29993469e+03  1.32325317e+03\n",
      "  1.35188062e+03  1.21795825e+03  1.10297168e+03  1.02117560e+03\n",
      "  9.60080811e+02  7.80986816e+02  4.01034027e+02  2.20907501e+02\n",
      "  7.89883728e+01  2.02545929e+00 -7.61870146e-02 -8.12697709e-02\n",
      "  8.65005672e-01  1.86109600e+01  1.64735626e+02  3.68056824e+02\n",
      "  7.70356567e+02  1.77057593e+03  3.03499121e+03  3.93129370e+03\n",
      "  4.71849707e+03  5.02336621e+03  5.11133105e+03  5.08682910e+03\n",
      "  4.84219189e+03  4.27519727e+03  3.61322607e+03  3.02123438e+03\n",
      "  2.16160156e+03  1.20645374e+03  6.07853455e+02  3.33457916e+02\n",
      "  1.26677887e+02  3.29388094e+00  1.94480762e-01 -4.32605207e-01\n",
      "  8.19358826e-01  1.67813854e+01  1.26068939e+02  3.39167786e+02\n",
      "  7.19147461e+02  1.30934839e+03  1.99481494e+03  2.79089258e+03\n",
      "  3.91460254e+03  4.07060156e+03  3.57295044e+03  1.71439722e+03\n",
      "  1.22879797e+03  9.00964783e+02  1.13303735e+03  1.11290210e+03\n",
      "  8.74481812e+02  5.16311218e+02  2.72921173e+02  1.73331497e+02\n",
      "  7.36171875e+01  2.02005386e+00  4.21458900e-01  2.85033941e-01\n",
      "  2.92633832e-01  6.85269547e+00  7.55617599e+01  1.90358459e+02\n",
      "  4.02808258e+02  8.99273865e+02  1.88409656e+03  2.75119019e+03\n",
      "  3.63481104e+03  4.36482227e+03  4.59292383e+03  4.56012207e+03\n",
      "  4.33540234e+03  3.83763330e+03  3.32178784e+03  2.91188184e+03\n",
      "  2.17088501e+03  1.25046680e+03  6.29702393e+02  3.26605835e+02\n",
      "  1.11744812e+02  3.65086651e+00  4.69789684e-01  7.01095104e-01\n",
      "  4.75005239e-01  2.20724144e+01  1.27156265e+02  2.99230896e+02\n",
      "  6.98688721e+02  1.53244324e+03  2.51045801e+03  3.35276685e+03\n",
      "  4.17209570e+03  4.76461035e+03  4.92800098e+03  4.87704004e+03\n",
      "  4.66152979e+03  4.11867725e+03  3.59238428e+03  3.02026709e+03\n",
      "  2.16757031e+03  1.24762598e+03  6.38362793e+02  3.46744629e+02\n",
      "  1.36589188e+02  3.96433377e+00 -2.59560972e-01 -4.35922623e-01\n",
      "  6.62768364e-01  1.22496090e+01  1.16878067e+02  3.15882568e+02\n",
      "  6.98287659e+02  1.43116443e+03  2.37752002e+03  3.40941797e+03\n",
      "  4.33019043e+03  4.74207959e+03  4.70643262e+03  3.89781104e+03\n",
      "  3.46651929e+03  3.18386914e+03  2.55795776e+03  1.84748755e+03\n",
      "  1.38491455e+03  7.95288757e+02  4.30733337e+02  2.07904175e+02\n",
      "  5.23591537e+01  6.20544314e-01 -9.92053092e-01  2.12129951e-02\n",
      " -9.40860152e-01  5.42888641e+00  5.40588379e+01  1.22802063e+02\n",
      "  2.67665192e+02  4.56843201e+02  6.82448730e+02  7.79556396e+02\n",
      "  8.23038208e+02  9.08259338e+02  1.78830151e+03  2.68964355e+03\n",
      "  3.87401465e+03  3.70663477e+03  3.27441040e+03  2.64554980e+03\n",
      "  1.74913379e+03  1.02493335e+03  4.97016266e+02  2.75247284e+02\n",
      "  9.71468353e+01  2.89300799e+00 -6.02630794e-01 -1.04364634e+00\n",
      " -1.09534192e+00  2.18371487e+00  4.15916176e+01  8.76288071e+01\n",
      "  8.59136505e+01  1.62646744e+02  3.58877014e+02  6.84709229e+02\n",
      "  1.00011292e+03  1.18244507e+03  1.13293628e+03  1.33789233e+03\n",
      "  1.63145874e+03  1.95694531e+03  2.12695239e+03  2.14255420e+03\n",
      "  1.51461975e+03  8.10423828e+02  3.67859161e+02  1.60015442e+02\n",
      "  5.68432007e+01  2.34115505e+00  2.44718313e-01 -8.36996794e-01\n",
      "  2.17261732e-01  1.75746899e+01  1.29867920e+02  2.89177368e+02\n",
      "  6.95064941e+02  1.56385620e+03  2.69696777e+03  3.62296875e+03\n",
      "  4.21990967e+03  4.31751807e+03  4.45480078e+03  4.20580518e+03\n",
      "  3.57067188e+03  2.56752344e+03  2.26510156e+03  1.98403101e+03\n",
      "  1.28758057e+03  7.51956482e+02  4.03049072e+02  2.34915970e+02\n",
      "  9.10970535e+01  2.83299589e+00 -1.32836914e+00 -1.53238189e+00\n",
      " -5.16101837e-01  1.25117474e+01  1.27213470e+02  2.86051117e+02\n",
      "  6.60763733e+02  1.38093457e+03  2.25524609e+03  3.13048486e+03\n",
      "  4.12785889e+03  4.55416602e+03  4.85382275e+03  4.85236035e+03\n",
      "  4.50100391e+03  3.94247705e+03  3.05646484e+03  2.22735913e+03\n",
      "  1.42149146e+03  8.63058838e+02  5.09205109e+02  2.69959503e+02\n",
      "  8.71340103e+01  3.49940634e+00  2.30286822e-01  3.48314703e-01\n",
      "  2.98352569e-01  7.06949329e+00  1.04371902e+02  2.29846924e+02\n",
      "  3.56733551e+02  4.90188232e+02  5.93218201e+02  6.49198059e+02\n",
      "  8.92591248e+02  1.28428210e+03  1.11010168e+03  8.46302734e+02\n",
      "  1.15100122e+03  8.13338257e+02  7.65426636e+02  6.41967285e+02\n",
      "  4.17976440e+02  2.23530304e+02  1.14263687e+02  5.49309769e+01\n",
      "  2.27936268e+01  3.51645648e-01 -4.53074038e-01 -6.41485512e-01\n",
      " -7.34878659e-01  1.41633689e+00  3.02728958e+01  6.87286224e+01\n",
      "  1.36420898e+02  2.02162598e+02  2.90346893e+02  4.97085663e+02\n",
      "  4.64054779e+02  4.73886536e+02  4.80429993e+02  4.76117126e+02\n",
      "  6.15660767e+02  7.13643494e+02  7.66198242e+02  7.21928345e+02\n",
      "  5.48004272e+02  3.68085632e+02  2.25510727e+02  1.08463211e+02\n",
      "  4.23489571e+01  1.72001719e+00  3.87228392e-02  2.53708303e-01\n",
      " -1.60284758e-01  1.22413826e+01  1.06784622e+02  2.20595474e+02\n",
      "  4.75205994e+02  1.03404785e+03  2.08024731e+03  2.81008789e+03\n",
      "  3.47141211e+03  3.76754150e+03  3.81047559e+03  3.72525537e+03\n",
      "  3.43926660e+03  2.67461230e+03  2.35766162e+03  1.88953418e+03\n",
      "  1.15698840e+03  7.13758423e+02  4.13168823e+02  2.58044769e+02\n",
      "  1.01426407e+02  2.03487945e+00  2.16678947e-01 -4.76749539e-02\n",
      "  4.73231763e-01  3.90216422e+00  5.73949585e+01  1.65700653e+02\n",
      "  2.30706650e+02  2.90636200e+02  3.55843567e+02  5.18777039e+02\n",
      "  1.84291833e+03  1.75672681e+03  2.69410156e+03  3.25986133e+03\n",
      "  2.84028735e+03  2.25074829e+03  1.29081335e+03  1.06099902e+03\n",
      "  8.83995178e+02  6.28365784e+02  4.07924133e+02  2.32612457e+02\n",
      "  7.47804413e+01  1.84040999e+00  3.56960267e-01  7.32407391e-01]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df_submission[\"prediction\"][720*index:720*(index+1)] = preds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    False\n",
      "1    False\n",
      "2    False\n",
      "3    False\n",
      "4    False\n",
      "Name: location, dtype: bool\n",
      "0    False\n",
      "1    False\n",
      "2    False\n",
      "3    False\n",
      "4    False\n",
      "Name: location, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "df_submission = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "for index, location in enumerate(locations): \n",
    "    preds = make_predictions(combined_df_test[index], location)\n",
    "\n",
    "    # Assign the predictions to df_submission for the current location\n",
    "    mask = df_submission[\"location\"] == location\n",
    "    print(mask.head())\n",
    "    # Add a check to make sure the lengths match\n",
    "    if len(preds) != mask.sum():\n",
    "        print(f\"Length of predictions: {len(preds)}\")\n",
    "        print(f\"Length of submission entries: {mask.sum()}\")\n",
    "        raise ValueError(f\"Mismatch in length of predictions and submission entries for location {location}.\")\n",
    "\n",
    "    df_submission[\"prediction\"][720*index:720*(index+1)] = preds\n",
    "\n",
    "\n",
    "# Save the results to a new submission file\n",
    "df_submission[[\"id\", \"prediction\"]].to_csv(\"predictions/autogluon-split.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
