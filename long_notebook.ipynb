{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General introduction\n",
    "\n",
    "Kaggle name: K Klustering Klan\n",
    "\n",
    "- Simen Seeberg-Rommetveit \n",
    "- Erik Hæstad Bjørnstad\n",
    "- Mathias Haakon Aas\n",
    "\n",
    "This is the long notebook containing these parts: \n",
    "- Data exploration \n",
    "- Feature engineering \n",
    "- Different models \n",
    "- Model interpratation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "direct_rad:W                      0.754767\n",
      "clear_sky_rad:W                   0.685952\n",
      "diffuse_rad:W                     0.665438\n",
      "direct_rad_1h:J                   0.661076\n",
      "is_in_shadow:idx                  0.617804\n",
      "clear_sky_energy_1h:J             0.612760\n",
      "diffuse_rad_1h:J                  0.605134\n",
      "is_day:idx                        0.569564\n",
      "sun_elevation:d                   0.440104\n",
      "ceiling_height_agl:m              0.228913\n",
      "effective_cloud_cover:p           0.227951\n",
      "visibility:m                      0.227326\n",
      "date_calc                         0.203918\n",
      "total_cloud_cover:p               0.190475\n",
      "air_density_2m:kgm3               0.147271\n",
      "wind_speed_v_10m:ms               0.124835\n",
      "dew_point_2m:K                    0.123936\n",
      "wind_speed_u_10m:ms               0.122025\n",
      "t_1000hPa:K                       0.120597\n",
      "absolute_humidity_2m:gm3          0.112695\n",
      "snow_water:kgm2                   0.097222\n",
      "relative_humidity_1000hPa:p       0.096870\n",
      "fresh_snow_24h:cm                 0.095611\n",
      "cloud_base_agl:m                  0.084773\n",
      "fresh_snow_12h:cm                 0.080159\n",
      "snow_depth:cm                     0.079995\n",
      "dew_or_rime:idx                   0.073287\n",
      "fresh_snow_6h:cm                  0.070095\n",
      "super_cooled_liquid_water:kgm2    0.069481\n",
      "fresh_snow_3h:cm                  0.060710\n",
      "rain_water:kgm2                   0.057966\n",
      "precip_type_5min:idx              0.056906\n",
      "precip_5min:mm                    0.055967\n",
      "fresh_snow_1h:cm                  0.051029\n",
      "sun_azimuth:d                     0.049702\n",
      "msl_pressure:hPa                  0.030976\n",
      "pressure_100m:hPa                 0.030839\n",
      "pressure_50m:hPa                  0.028767\n",
      "sfc_pressure:hPa                  0.026407\n",
      "prob_rime:p                       0.017702\n",
      "wind_speed_10m:ms                 0.008200\n",
      "elevation:m                            NaN\n",
      "snow_density:kgm3                      NaN\n",
      "snow_drift:idx                         NaN\n",
      "snow_melt_10min:mm                     NaN\n",
      "wind_speed_w_1000hPa:ms                NaN\n",
      "Name: pv_measurement, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def rank_features_by_correlation(data_parquet_file, target_parquet_file):\n",
    "    # Load main dataset and target dataset\n",
    "    df_data = pd.read_parquet(data_parquet_file).head(5000)\n",
    "    df_target = pd.read_parquet(target_parquet_file)\n",
    "\n",
    "    # Check if 'date_forecast' and 'time' columns exist in their respective DataFrames\n",
    "    if 'date_forecast' not in df_data.columns or 'time' not in df_target.columns:\n",
    "        raise ValueError(\"'date_forecast' or 'time' column not found in one or both datasets\")\n",
    "\n",
    "    # Merge the datasets based on 'date_forecast' and 'time'\n",
    "    df_merged = pd.merge(df_data, df_target, left_on='date_forecast', right_on='time', how='inner')\n",
    "\n",
    "    # Get the name of the target column (assuming it's the last column in df_target)\n",
    "    target_column = df_target.columns[-1]\n",
    "\n",
    "    # Check if target column exists in merged DataFrame\n",
    "    if target_column not in df_merged.columns:\n",
    "        raise ValueError(f\"Target column '{target_column}' not found in merged dataset\")\n",
    "\n",
    "    # Calculate correlation of each feature with the target column\n",
    "    correlation_scores = df_merged.drop(['date_forecast', 'time'], axis=1).corr()[target_column].drop(target_column)\n",
    "\n",
    "    # Rank features based on the absolute value of their correlation scores\n",
    "    ranked_features = correlation_scores.abs().sort_values(ascending=False)\n",
    "\n",
    "    return ranked_features\n",
    "\n",
    "# Usage example\n",
    "data_parquet_file = \"data/A/X_train_estimated.parquet\"\n",
    "target_parquet_file = \"data/A/train_targets.parquet\"\n",
    "ranked_features = rank_features_by_correlation(data_parquet_file, target_parquet_file)\n",
    "print(ranked_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended feature importance with data engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "direct_rad:W                   0.754767\n",
      "clear_sky_rad:W                0.685952\n",
      "diffuse_rad:W                  0.665438\n",
      "direct_rad_1h:J                0.661076\n",
      "is_in_shadow:idx               0.617804\n",
      "clear_sky_energy_1h:J          0.612760\n",
      "diffuse_rad_1h:J               0.605134\n",
      "is_day:idx                     0.569564\n",
      "sun_elevation_binned_Medium    0.569564\n",
      "sun_elevation_binned_Low       0.569564\n",
      "clear_sky_rad:W_no_outliers    0.560152\n",
      "sun_elevation:d                0.440104\n",
      "rad_change                     0.428878\n",
      "hour_cos                       0.395176\n",
      "effective_cloud_cover:p        0.227951\n",
      "visibility:m                   0.227326\n",
      "date_calc                      0.203918\n",
      "total_cloud_cover:p            0.190475\n",
      "month_sin                      0.180140\n",
      "month                          0.178678\n",
      "hour_sin                       0.170814\n",
      "month_cos                      0.155833\n",
      "air_density_2m:kgm3            0.147271\n",
      "ceiling_height_agl:m           0.129813\n",
      "wind_speed_v_10m:ms            0.124835\n",
      "dew_point_2m:K                 0.123936\n",
      "wind_speed_u_10m:ms            0.122025\n",
      "t_1000hPa:K                    0.120597\n",
      "absolute_humidity_2m:gm3       0.112695\n",
      "Name: pv_measurement, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def manipulate_single_feature(df, feature_name):\n",
    "    \"\"\"\n",
    "    Manipulate a single feature: \n",
    "    - Removing outliers\n",
    "    - Applying log transformation (if all values are positive)\n",
    "    - Returns a DataFrame with the manipulated feature and potential new features.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Outlier Removal\n",
    "    low, high = np.percentile(df[feature_name], [3, 97])\n",
    "    median = df[feature_name].median()\n",
    "    df[feature_name + '_no_outliers'] = np.where(df[feature_name] < low, median, df[feature_name])\n",
    "    df[feature_name + '_no_outliers'] = np.where(df[feature_name] > high, median, df[feature_name])\n",
    "    \n",
    "    # 2. Log Transformation\n",
    "    if df[feature_name].min() > 0:\n",
    "        df[feature_name + '_log'] = np.log(df[feature_name])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def rank_features_by_correlation(data_parquet_file, observed_parquet_file = None, target_parquet_file = None, threshold=None, num_features_to_keep=None):\n",
    "    # Load main dataset, observed dataset and target dataset\n",
    "    df_data = pd.read_parquet(data_parquet_file).head(5000)\n",
    "    if observed_parquet_file is not None:\n",
    "        df_observed = pd.read_parquet(observed_parquet_file).head(5000)\n",
    "    \n",
    "        # Concatenate the estimated and observed data vertically\n",
    "        df_data = pd.concat([df_data, df_observed], axis=0, ignore_index=True)\n",
    "\n",
    "    df_target = pd.read_parquet(target_parquet_file)\n",
    "\n",
    "    # Check if 'date_forecast' and 'time' columns exist in their respective DataFrames\n",
    "    if 'date_forecast' not in df_data.columns or 'time' not in df_target.columns:\n",
    "        raise ValueError(\"'date_forecast' or 'time' column not found in one or both datasets\")\n",
    "\n",
    "    # Merge the datasets based on 'date_forecast' and 'time'\n",
    "    df_merged = pd.merge(df_data, df_target, left_on='date_forecast', right_on='time', how='inner')\n",
    "\n",
    "    # Feature Engineering\n",
    "\n",
    "    # 1. Time-based Features\n",
    "    df_merged['hour'] = df_merged['time'].dt.hour\n",
    "    df_merged['month'] = df_merged['time'].dt.month\n",
    "    df_merged['weekday'] = df_merged['time'].dt.weekday\n",
    "\n",
    "    # Sinusoidal transformations for cyclical time features\n",
    "    df_merged['hour_sin'] = np.sin(2 * np.pi * df_merged['hour'] / 24)\n",
    "    df_merged['hour_cos'] = np.cos(2 * np.pi * df_merged['hour'] / 24)\n",
    "    df_merged['month_sin'] = np.sin(2 * np.pi * df_merged['month'] / 12)\n",
    "    df_merged['month_cos'] = np.cos(2 * np.pi * df_merged['month'] / 12)\n",
    "    df_merged['weekday_sin'] = np.sin(2 * np.pi * df_merged['weekday'] / 7)\n",
    "    df_merged['weekday_cos'] = np.cos(2 * np.pi * df_merged['weekday'] / 7)\n",
    "\n",
    "\n",
    "    # 2. Interaction Features\n",
    "    df_merged['snow_depth_radiation'] = df_merged['snow_depth:cm'] * df_merged['direct_rad:W']\n",
    "\n",
    "\n",
    "    # 6. NaN Handling\n",
    "    df_merged['snow_density:kgm3'].fillna(df_merged['snow_density:kgm3'].median(), inplace=True)\n",
    "    df_merged['ceiling_height_agl:m'].fillna(df_merged['ceiling_height_agl:m'].median(), inplace=True)\n",
    "    df_merged['cloud_base_agl:m'].fillna(df_merged['cloud_base_agl:m'].median(), inplace=True)\n",
    "\n",
    "    # 7. Binning\n",
    "    bins = [-90, 0, 45, 90]\n",
    "    labels = ['Low', 'Medium', 'High']\n",
    "    df_merged['sun_elevation_binned'] = pd.cut(df_merged['sun_elevation:d'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "\n",
    "\n",
    "    # 4. Derivative Features\n",
    "    df_merged['rad_change'] = df_merged['direct_rad:W'].diff().fillna(0)\n",
    "    # Similar calculations for other columns if deemed important.\n",
    "\n",
    "\n",
    "    # One-hot encode 'sun_elevation_binned'\n",
    "    df_merged = pd.get_dummies(df_merged, columns=['sun_elevation_binned'])\n",
    "\n",
    "    # Drop Constant Features\n",
    "    df_merged.drop(['elevation:m', 'snow_drift:idx', 'snow_melt_10min:mm', 'wind_speed_w_1000hPa:ms'], axis=1, inplace=True)\n",
    "    df_merged = manipulate_single_feature(df_merged, 'clear_sky_rad:W')\n",
    "    # Get the name of the target column (assuming it's the last column in df_target)\n",
    "    target_column = df_target.columns[-1]\n",
    "    \n",
    "    # Calculate correlation of each feature with the target column\n",
    "    correlation_scores = df_merged.drop(['date_forecast', 'time'], axis=1).corr()[target_column].drop(target_column)\n",
    "\n",
    "    # Rank features based on the absolute value of their correlation scores\n",
    "    ranked_features = correlation_scores.abs().sort_values(ascending=False)\n",
    "\n",
    "    if threshold is not None:\n",
    "        ranked_features = ranked_features[ranked_features > threshold]\n",
    "\n",
    "    if num_features_to_keep is not None:\n",
    "        ranked_features = ranked_features.head(num_features_to_keep)\n",
    "    \n",
    "    return ranked_features\n",
    "\n",
    "\n",
    "\n",
    "# Usage example\n",
    "data_parquet_file = \"data/A/X_train_estimated.parquet\"\n",
    "observed_parquet_file = \"data/A/X_train_observed.parquet\"\n",
    "target_parquet_file = \"data/A/train_targets.parquet\"\n",
    "\n",
    "ranked_features = rank_features_by_correlation(data_parquet_file=data_parquet_file, observed_parquet_file=observed_parquet_file, target_parquet_file=target_parquet_file, threshold=0.1)\n",
    "# ranked_features = rank_features_by_correlation(data_parquet_file, target_parquet_file=target_parquet_file, threshold=0.1)\n",
    "ranked_features = rank_features_by_correlation(data_parquet_file, target_parquet_file=target_parquet_file, threshold=0.1)\n",
    "\n",
    "\n",
    "print(ranked_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering to new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_and_save_engineered_features(estimated_parquet_file=None, observed_parquet_file=None, target_parquet_file=None, output_parquet_file=\"engineered_data.parquet\"):\n",
    "    # Load main dataset, observed dataset, and target dataset\n",
    "    df_estimated = pd.read_parquet(estimated_parquet_file)\n",
    "    df_observed = pd.read_parquet(observed_parquet_file)\n",
    "\n",
    "    df_target = pd.read_parquet(target_parquet_file)\n",
    "    df_merged = pd.concat([df_observed, df_estimated], axis=0, ignore_index=True)\n",
    "    print(df_merged.head())\n",
    "\n",
    "    # Check if 'date_forecast' and 'time' columns exist in their respective DataFrames\n",
    "    if 'date_forecast' not in df_merged.columns or 'time' not in df_target.columns:\n",
    "        raise ValueError(\"'date_forecast' or 'time' column not found in one or both datasets\")\n",
    "\n",
    "\n",
    "    # Feature Engineering\n",
    "    df_merged = feature_engineering(df_merged)\n",
    "\n",
    "    # Save the engineered data\n",
    "    df_merged.to_parquet(output_parquet_file)\n",
    "    print(f\"Engineered data saved to {output_parquet_file}\")\n",
    "\n",
    "\n",
    "def feature_engineering(df_merged):\n",
    "    # Time-based Features\n",
    "    df_merged['hour'] = df_merged['date_forecast'].dt.hour\n",
    "    df_merged['month'] = df_merged['date_forecast'].dt.month\n",
    "    df_merged['weekday'] = df_merged['date_forecast'].dt.weekday\n",
    "\n",
    "    # Sinusoidal transformations for cyclical time features\n",
    "    df_merged['hour_sin'] = np.sin(2 * np.pi * df_merged['hour'] / 24)\n",
    "    df_merged['hour_cos'] = np.cos(2 * np.pi * df_merged['hour'] / 24)\n",
    "    df_merged['month_sin'] = np.sin(2 * np.pi * df_merged['month'] / 12)\n",
    "    df_merged['month_cos'] = np.cos(2 * np.pi * df_merged['month'] / 12)\n",
    "    df_merged['weekday_sin'] = np.sin(2 * np.pi * df_merged['weekday'] / 7)\n",
    "    df_merged['weekday_cos'] = np.cos(2 * np.pi * df_merged['weekday'] / 7)\n",
    "\n",
    "    # Binning\n",
    "    bins = [-90, 0, 45, 90]\n",
    "    labels = ['1', '2', '3']\n",
    "    df_merged['sun_elevation_binned'] = pd.cut(df_merged['sun_elevation:d'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "\n",
    "    return df_merged\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Usage example\n",
    "estimated_parquet_file = \"data/C/X_train_estimated.parquet\"\n",
    "observed_parquet_file = \"data/C/X_train_observed.parquet\"\n",
    "target_parquet_file = \"data/C/train_targets.parquet\"\n",
    "output_file = \"cleaned_data/C/X_train_engineered.parquet\"\n",
    "\n",
    "generate_and_save_engineered_features(estimated_parquet_file=estimated_parquet_file, observed_parquet_file=observed_parquet_file, target_parquet_file=target_parquet_file, output_parquet_file=output_file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features to include and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = [\"A\", \"B\", \"C\"]\n",
    "features_order = []\n",
    "A_B_ratio = 6.73\n",
    "A_C_ratio = 8.17\n",
    "\n",
    "LAGGED_COLUMNS_TO_KEEP = [\n",
    "    'direct_rad:W_lag_1h', \n",
    "    'direct_rad:W_lag_forward_1h', \n",
    "    'clear_sky_rad:W_lag_1h', \n",
    "    'clear_sky_rad:W_lag_forward_1h', \n",
    "    'diffuse_rad:W_lag_1h', \n",
    "    'diffuse_rad:W_lag_forward_1h', \n",
    "    'direct_rad_1h:J_lag_1h', \n",
    "    'direct_rad_1h:J_lag_forward_1h', \n",
    "    'is_in_shadow:idx_lag_1h', \n",
    "    'is_in_shadow:idx_lag_forward_1h', \n",
    "    'clear_sky_energy_1h:J_lag_1h', \n",
    "    'clear_sky_energy_1h:J_lag_forward_1h', \n",
    "    'effective_cloud_cover:p_lag_1h', \n",
    "    'effective_cloud_cover:p_lag_forward_1h', \n",
    "    'visibility:m_lag_1h', \n",
    "    'visibility:m_lag_forward_1h', \n",
    "    'total_cloud_cover:p_lag_1h', \n",
    "    'total_cloud_cover:p_lag_forward_1h', \n",
    "\n",
    "    # Did not work well with lagged features >1h \n",
    "    \n",
    "    # 'direct_rad:W_lag_2h', \n",
    "    # 'direct_rad:W_lag_forward_2h', \n",
    "    # 'clear_sky_rad:W_lag_2h', \n",
    "    # 'clear_sky_rad:W_lag_forward_2h', \n",
    "    # 'diffuse_rad:W_lag_2h', \n",
    "    # 'diffuse_rad:W_lag_forward_2h', \n",
    "    # 'direct_rad_1h:J_lag_2h', \n",
    "    # 'direct_rad_1h:J_lag_forward_2h', \n",
    "    # 'is_in_shadow:idx_lag_2h', \n",
    "    # 'is_in_shadow:idx_lag_forward_2h', \n",
    "    # 'clear_sky_energy_1h:J_lag_2h', \n",
    "    # 'clear_sky_energy_1h:J_lag_forward_2h', \n",
    "    # 'effective_cloud_cover:p_lag_2h', \n",
    "    # 'effective_cloud_cover:p_lag_forward_2h', \n",
    "    # 'visibility:m_lag_2h', \n",
    "    # 'visibility:m_lag_forward_2h', \n",
    "    # 'total_cloud_cover:p_lag_2h', \n",
    "    # 'total_cloud_cover:p_lag_forward_2h', \n",
    "\n",
    "    # 'direct_rad:W_lag_3h', \n",
    "    # 'direct_rad:W_lag_forward_3h', \n",
    "    # 'clear_sky_rad:W_lag_3h', \n",
    "    # 'clear_sky_rad:W_lag_forward_3h', \n",
    "    # 'diffuse_rad:W_lag_3h', \n",
    "    # 'diffuse_rad:W_lag_forward_3h', \n",
    "    # 'direct_rad_1h:J_lag_3h', \n",
    "    # 'direct_rad_1h:J_lag_forward_3h', \n",
    "    # 'is_in_shadow:idx_lag_3h', \n",
    "    # 'is_in_shadow:idx_lag_forward_3h', \n",
    "    # 'clear_sky_energy_1h:J_lag_3h', \n",
    "    # 'clear_sky_energy_1h:J_lag_forward_3h', \n",
    "    # 'effective_cloud_cover:p_lag_3h', \n",
    "    # 'effective_cloud_cover:p_lag_forward_3h', \n",
    "    # 'visibility:m_lag_3h', \n",
    "    # 'visibility:m_lag_forward_3h', \n",
    "    # 'total_cloud_cover:p_lag_3h', \n",
    "    # 'total_cloud_cover:p_lag_forward_3h'\n",
    "]\n",
    "\n",
    "CUSTOM_COLUMNS_TO_KEEP = [\n",
    "    \"hour_cos\",\n",
    "    \"hour_sin\",\n",
    "    \"month_sin\",\n",
    "    \"month_cos\",\n",
    "    \"hours_since_forecast\",\n",
    "]\n",
    "\n",
    "WEATHER_FEATURES = [\n",
    "    \"direct_rad:W\",\n",
    "    \"clear_sky_rad:W\",\n",
    "    \"diffuse_rad:W\",\n",
    "    \"direct_rad_1h:J\",\n",
    "    \"is_in_shadow:idx\",\n",
    "    \"clear_sky_energy_1h:J\",\n",
    "    \"effective_cloud_cover:p\",\n",
    "    \"visibility:m\",\n",
    "    \"total_cloud_cover:p\",\n",
    "]\n",
    "\n",
    "\n",
    "TEST_COLUMNS_TO_KEEP = [\n",
    "    \"direct_rad:W\",\n",
    "    \"clear_sky_rad:W\",\n",
    "    \"diffuse_rad:W\",\n",
    "    \"direct_rad_1h:J\",\n",
    "    \"is_in_shadow:idx\",\n",
    "    \"clear_sky_energy_1h:J\",\n",
    "    \"diffuse_rad_1h:J\",\n",
    "    \"is_day:idx\",\n",
    "    \"sun_elevation:d\",\n",
    "    \"ceiling_height_agl:m\",\n",
    "    \"effective_cloud_cover:p\",\n",
    "    \"visibility:m\",\n",
    "    \"total_cloud_cover:p\",\n",
    "    \"air_density_2m:kgm3\",\n",
    "    \"wind_speed_v_10m:ms\",\n",
    "    \"dew_point_2m:K\",\n",
    "    \"wind_speed_u_10m:ms\",\n",
    "    \"t_1000hPa:K\",\n",
    "    \"absolute_humidity_2m:gm3\",\n",
    "    \"snow_water:kgm2\",\n",
    "    \"relative_humidity_1000hPa:p\",\n",
    "    \"fresh_snow_24h:cm\",\n",
    "    \"cloud_base_agl:m\",\n",
    "    \"fresh_snow_12h:cm\",\n",
    "    \"snow_depth:cm\",\n",
    "    \"dew_or_rime:idx\",\n",
    "    \"fresh_snow_6h:cm\",\n",
    "    \"super_cooled_liquid_water:kgm2\",\n",
    "    \"fresh_snow_3h:cm\",\n",
    "    \"rain_water:kgm2\",\n",
    "    \"precip_type_5min:idx\",\n",
    "    \"precip_5min:mm\",\n",
    "    \"fresh_snow_1h:cm\",\n",
    "    \"sun_azimuth:d\",\n",
    "    \"msl_pressure:hPa\",\n",
    "    \"pressure_100m:hPa\",\n",
    "    \"pressure_50m:hPa\",\n",
    "    \"sfc_pressure:hPa\",\n",
    "    \"prob_rime:p\",\n",
    "    \"wind_speed_10m:ms\",\n",
    "    \"elevation:m\",\n",
    "    \"snow_density:kgm3\",\n",
    "    \"snow_drift:idx\",\n",
    "    \"snow_melt_10min:mm\",\n",
    "    \"wind_speed_w_1000hPa:ms\",\n",
    "    \"observed_or_estimated\",\n",
    "    # \"date_calc\",\n",
    "] + CUSTOM_COLUMNS_TO_KEEP  +  LAGGED_COLUMNS_TO_KEEP\n",
    "\n",
    "COLUMNS_TO_KEEP = TEST_COLUMNS_TO_KEEP + [\"pv_measurement\"]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weather_lagged_features(df, weather_features):\n",
    "    for feature in weather_features:\n",
    "    \n",
    "        # Creating lagged features for 1, 2 and 3 hours\n",
    "        df[f'{feature}_lag_1h'] = df[feature].shift(1)\n",
    "        # df[f'{feature}_lag_2h'] = df[feature].shift(2)\n",
    "        # df[f'{feature}_lag_3h'] = df[feature].shift(3)\n",
    "\n",
    "        df[f'{feature}_lag_forward_1h'] = df[feature].shift(-1)\n",
    "        # df[f'{feature}_lag_forward_2h'] = df[feature].shift(-2)\n",
    "        # df[f'{feature}_lag_forward_3h'] = df[feature].shift(-3)\n",
    "\n",
    "    # Handling edges by filling NaNs with appropriate values\n",
    "    df.fillna(method='ffill', inplace=True)  # Forward fill\n",
    "    df.fillna(method='bfill', inplace=True)  # Backward fill\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_lagged_features(df, column_name='pv_measurement'):\n",
    "    # Deprecated because of jumps in dates to be predicted \n",
    "\n",
    "    df[f'{column_name}_prev_month'] = df[column_name].shift(24*7) # previous week\n",
    "    df[f'{column_name}_prev_year'] = df[column_name].shift(24*365) # previous year\n",
    "    df[f'{column_name}_2years_ago'] = df[column_name].shift(24*365*2) # next year\n",
    "\n",
    "\n",
    "    df.fillna(method='ffill', inplace=True)  # Forward fill\n",
    "    df.fillna(method='bfill', inplace=True)  # Backward fill\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_custom_fields(df):\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['date_forecast'].dt.hour / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['date_forecast'].dt.hour / 24)\n",
    "\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['date_forecast'].dt.month / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['date_forecast'].dt.month / 12)\n",
    "    df['day-of-year'] = df['date_forecast'].dt.dayofyear\n",
    "   \n",
    "    return df\n",
    "\n",
    "def add_calc_date(df_observed, df_estimated, df_test):\n",
    "    # Function to calculate the difference in hours\n",
    "    def calculate_hour_difference(row):\n",
    "        diff = row['date_calc'] - row['date_forecast']\n",
    "        return diff.total_seconds() / 3600  # Convert difference to hours\n",
    "\n",
    "    # Apply the function to calculate the hour difference for df_estimated and df_test\n",
    "    df_estimated['hours_since_forecast'] = df_estimated.apply(calculate_hour_difference, axis=1)\n",
    "    df_test['hours_since_forecast'] = df_test.apply(calculate_hour_difference, axis=1)\n",
    "\n",
    "    # Fill in zero for df_observed\n",
    "    df_observed['hours_since_forecast'] = 0\n",
    "\n",
    "    return df_observed, df_estimated, df_test\n",
    "\n",
    "\n",
    "def remove_outliers(df):\n",
    "    # Use a mask to filter out the rows where rolling std is zero but keep the rows where the value itself is zero\n",
    "    mask = (df['pv_measurement'].rolling(5).std() == 0) & (df['pv_measurement'] != 0)\n",
    "    df = df[~mask]\n",
    "    return df\n",
    "\n",
    "def resample_add_data(df, is_test_data):\n",
    "    df = add_custom_fields(df)\n",
    "    df.set_index('date_forecast', inplace=True)\n",
    "    df = df.resample('1H').mean()\n",
    "    \n",
    "    # Remove empty dates if test data\n",
    "    if is_test_data:\n",
    "        non_nan_threshold = len(df.columns) // 2  \n",
    "        df.dropna(thresh=non_nan_threshold, inplace=True)\n",
    "    df.interpolate(method=\"linear\", inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_location_feature(X, location):\n",
    "      # Treat location as a categorical feature by converting it to a category type\n",
    "    \n",
    "    # Tried to bin sun elevation, but had a negative impact\n",
    "    # bins = [-90, 0, 45, 90]\n",
    "    # labels = ['Low', 'Medium', 'High']\n",
    "    # X['sun_elevation:d'] = pd.cut(X['sun_elevation:d'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "    X['location'] = location\n",
    "    X['location'] = X['location'].astype(str)  # Convert to string if 'location' is not an int\n",
    "    X['dew_or_rime:idx'] = X['dew_or_rime:idx'].astype(str)\n",
    "    X['is_day:idx'] = X['is_day:idx'].astype(str)\n",
    "    X['is_in_shadow:idx'] = X['is_in_shadow:idx'].astype(str)\n",
    "    categorical_columns = ['location', 'dew_or_rime:idx', 'is_day:idx', 'is_in_shadow:idx', 'sun_elevation:d']\n",
    "\n",
    "    # Before filling NaN values, add 'missing' as a category for each categorical column.\n",
    "    for column in categorical_columns:\n",
    "        X[column] = X[column].astype('category')  # Ensure the column is of type 'category'.\n",
    "        if 'missing' not in X[column].cat.categories:\n",
    "            X[column] = X[column].cat.add_categories(['missing'])  # Add 'missing' as a new category.\n",
    "        X[column] = X[column].fillna('missing')\n",
    "    X['location'] = X['location'].astype('category')\n",
    "    X['dew_or_rime:idx'] = X['dew_or_rime:idx'].astype('category')\n",
    "    X['is_day:idx'] = X['is_day:idx'].astype('category')\n",
    "    X['is_in_shadow:idx'] = X['is_in_shadow:idx'].astype('category')\n",
    "    # X['sun_elevation:d'] = X['sun_elevation:d'].astype('category')\n",
    "    \n",
    "    return X\n",
    "\n",
    "def add_scaling(X_test, X_training, location):\n",
    "    global scalers\n",
    "    continuous_columns = X_training.select_dtypes(include=['float32', 'int32']).columns\n",
    "    if location not in scalers:\n",
    "        scalers[location] = MinMaxScaler()\n",
    "    X_training[continuous_columns] = scalers[location].fit_transform(X_training[continuous_columns])\n",
    "    \n",
    "    X_test[continuous_columns] = scalers[location].transform(X_test[continuous_columns])\n",
    "\n",
    "    return X_test, X_training\n",
    "\n",
    "def make_observed_and_estimated_category(df_observed, df_estimated, df_test):\n",
    "    # Hot encode in wether observed or estimated\n",
    "    df_observed['observed_or_estimated'] = 'observed'\n",
    "    df_estimated['observed_or_estimated'] = 'estimated'\n",
    "    df_test['observed_or_estimated'] = 'estimated'\n",
    "    # Concatenate observed and estimated\n",
    "    df_training = pd.concat([df_observed, df_estimated], axis=0).sort_values(by=\"date_forecast\")\n",
    "    df_training['observed_or_estimated'] = df_training['observed_or_estimated'].astype('category')\n",
    "    df_test['observed_or_estimated'] = df_test['observed_or_estimated'].astype('category')\n",
    "\n",
    "    return df_training, df_test\n",
    "\n",
    "def analyze_pv_measurements(dataframe):\n",
    "    # Filter the DataFrame for rows where 'is_day' is 0\n",
    "    night_data = dataframe[dataframe['is_day:idx'] == 0]\n",
    "\n",
    "    # Analysis of 'pv_measurements' during night time\n",
    "    pv_stats = night_data['pv_measurement'].describe()\n",
    "\n",
    "    return pv_stats\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual preparing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_custom_fields(df):\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['date_forecast'].dt.hour / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['date_forecast'].dt.hour / 24)\n",
    "\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['date_forecast'].dt.month / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['date_forecast'].dt.month / 12)\n",
    "    df['day-of-year'] = df['date_forecast'].dt.dayofyear\n",
    "   \n",
    "    return df\n",
    "\n",
    "def add_calc_date(df_observed, df_estimated, df_test):\n",
    "    # Function to calculate the difference in hours\n",
    "    def calculate_hour_difference(row):\n",
    "        diff = row['date_calc'] - row['date_forecast']\n",
    "        return diff.total_seconds() / 3600  # Convert difference to hours\n",
    "\n",
    "    # Apply the function to calculate the hour difference for df_estimated and df_test\n",
    "    df_estimated['hours_since_forecast'] = df_estimated.apply(calculate_hour_difference, axis=1)\n",
    "    df_test['hours_since_forecast'] = df_test.apply(calculate_hour_difference, axis=1)\n",
    "\n",
    "    # Fill in zero for df_observed\n",
    "    df_observed['hours_since_forecast'] = 0\n",
    "\n",
    "    return df_observed, df_estimated, df_test\n",
    "\n",
    "\n",
    "def remove_outliers(df):\n",
    "    # Use a mask to filter out the rows where rolling std is zero but keep the rows where the value itself is zero\n",
    "    mask = (df['pv_measurement'].rolling(5).std() == 0) & (df['pv_measurement'] != 0)\n",
    "    df = df[~mask]\n",
    "    return df\n",
    "\n",
    "def resample_add_data(df, is_test_data):\n",
    "    df = add_custom_fields(df)\n",
    "    df.set_index('date_forecast', inplace=True)\n",
    "    df = df.resample('1H').mean()\n",
    "    \n",
    "    # Remove empty dates if test data\n",
    "    if is_test_data:\n",
    "        non_nan_threshold = len(df.columns) // 2  \n",
    "        df.dropna(thresh=non_nan_threshold, inplace=True)\n",
    "    df.interpolate(method=\"linear\", inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_location_feature(X, location):\n",
    "      # Treat location as a categorical feature by converting it to a category type\n",
    "    \n",
    "    # bins = [-90, 0, 45, 90]\n",
    "    # labels = ['Low', 'Medium', 'High']\n",
    "    # X['sun_elevation:d'] = pd.cut(X['sun_elevation:d'], bins=bins, labels=labels, include_lowest=True)\n",
    "    X['location'] = location\n",
    "    X['location'] = X['location'].astype(str)  # Convert to string if 'location' is not an int\n",
    "    X['dew_or_rime:idx'] = X['dew_or_rime:idx'].astype(str)\n",
    "    X['is_day:idx'] = X['is_day:idx'].astype(str)\n",
    "    X['is_in_shadow:idx'] = X['is_in_shadow:idx'].astype(str)\n",
    "    categorical_columns = ['location', 'dew_or_rime:idx', 'is_day:idx', 'is_in_shadow:idx', 'sun_elevation:d']\n",
    "\n",
    "    # Before filling NaN values, add 'missing' as a category for each categorical column.\n",
    "    for column in categorical_columns:\n",
    "        X[column] = X[column].astype('category')  # Ensure the column is of type 'category'.\n",
    "        if 'missing' not in X[column].cat.categories:\n",
    "            X[column] = X[column].cat.add_categories(['missing'])  # Add 'missing' as a new category.\n",
    "        X[column] = X[column].fillna('missing')\n",
    "    X['location'] = X['location'].astype('category')\n",
    "    X['dew_or_rime:idx'] = X['dew_or_rime:idx'].astype('category')\n",
    "    X['is_day:idx'] = X['is_day:idx'].astype('category')\n",
    "    X['is_in_shadow:idx'] = X['is_in_shadow:idx'].astype('category')\n",
    "    # X['sun_elevation:d'] = X['sun_elevation:d'].astype('category')\n",
    "    \n",
    "    return X\n",
    "\n",
    "def add_scaling(X_test, X_training, location):\n",
    "    global scalers\n",
    "    continuous_columns = X_training.select_dtypes(include=['float32', 'int32']).columns\n",
    "    if location not in scalers:\n",
    "        scalers[location] = MinMaxScaler()\n",
    "    X_training[continuous_columns] = scalers[location].fit_transform(X_training[continuous_columns])\n",
    "    \n",
    "    X_test[continuous_columns] = scalers[location].transform(X_test[continuous_columns])\n",
    "\n",
    "    return X_test, X_training\n",
    "\n",
    "def make_observed_and_estimated_category(df_observed, df_estimated, df_test):\n",
    "     # Hot encode in wether observed or estimated\n",
    "    df_observed['observed_or_estimated'] = 'observed'\n",
    "    df_estimated['observed_or_estimated'] = 'estimated'\n",
    "    df_test['observed_or_estimated'] = 'estimated'\n",
    "    # Concatenate observed and estimated\n",
    "    df_training = pd.concat([df_observed, df_estimated], axis=0).sort_values(by=\"date_forecast\")\n",
    "    df_training['observed_or_estimated'] = df_training['observed_or_estimated'].astype('category')\n",
    "    df_test['observed_or_estimated'] = df_test['observed_or_estimated'].astype('category')\n",
    "\n",
    "    return df_training, df_test\n",
    "\n",
    "def analyze_pv_measurements(dataframe):\n",
    "    # Filter the DataFrame for rows where 'is_day' is 0\n",
    "    night_data = dataframe[dataframe['is_day:idx'] == 0]\n",
    "\n",
    "    # Analysis of 'pv_measurements' during night time\n",
    "    pv_stats = night_data['pv_measurement'].describe()\n",
    "\n",
    "    return pv_stats\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost model 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, X_val, y_val, location):\n",
    "    cat_features = [index for index, col in enumerate(X_train.columns) if col in ['location', 'dew_or_rime:idx', 'is_day:idx', 'is_in_shadow:idx', 'observed_or_estimated']]\n",
    "    \n",
    "    model = CatBoostRegressor(\n",
    "        iterations=6000,\n",
    "        learning_rate=0.007,\n",
    "        depth=12,  # assuming you decided to keep the depth reduced\n",
    "        loss_function='MAE',\n",
    "        verbose=200,\n",
    "        cat_features=cat_features,\n",
    "    )\n",
    "\n",
    "    # Use the provided validation set for early stopping\n",
    "    \n",
    "    model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=False)\n",
    "    model.save_model(f\"catboost_model_merged.cbm\")\n",
    "\n",
    "def evaluate_model(X_val, y_val, location):\n",
    "    global scaling_target  \n",
    "    model = CatBoostRegressor()\n",
    "    model.load_model(f\"catboost_model_merged.cbm\")\n",
    "    \n",
    "    # Unique locations in the X_val dataframe\n",
    "    locations = X_val['location'].unique()\n",
    "    \n",
    "    for location in locations:\n",
    "        # Filter X_val and y_val for the current location\n",
    "        X_val_loc = X_val[X_val['location'] == location]\n",
    "        y_val_loc = y_val[X_val['location'] == location]\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred_loc = model.predict(X_val_loc)\n",
    "        \n",
    "        # Apply the transformation if needed (assuming y_val_loc and y_pred_loc are log1p transformed)\n",
    "        # y_val_loc = np.expm1(y_val_loc)\n",
    "        # y_pred_loc = np.expm1(y_pred_loc)\n",
    "        \n",
    "        # Calculate MAE for the current location\n",
    "        mae = mean_absolute_error(y_val_loc, y_pred_loc)\n",
    "\n",
    "        if scaling_target: \n",
    "            if location == \"B\": \n",
    "                mae = mae/A_B_ratio\n",
    "            if location == \"C\":\n",
    "                mae = mae/A_C_ratio\n",
    "                \n",
    "        print(f'Location {location}, Mean Absolute Error: {mae}')\n",
    "\n",
    "\n",
    "# Train the model using all available training data and the initial validation set for early stopping\n",
    "train_model(combined_X_train, combined_Y_train, combined_X_val, combined_Y_val, location)\n",
    "# Evaluate the model using the same validation set\n",
    "evaluate_model(combined_X_val, combined_Y_val, location)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance():\n",
    "    model = CatBoostRegressor()\n",
    "    model.load_model(f\"catboost_model_merged.cbm\")\n",
    "\n",
    "    # Getting feature importances\n",
    "    feature_importances = model.get_feature_importance()\n",
    "    feature_names = model.feature_names_\n",
    "\n",
    "    # Creating a DataFrame from feature importances\n",
    "    df_feature_importances = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importances\n",
    "    })\n",
    "\n",
    "    # Sorting the DataFrame by importance in descending order\n",
    "    df_feature_importances = df_feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    return df_feature_importances\n",
    "\n",
    "\n",
    "feature_importances = feature_importance()\n",
    "# Calling the function\n",
    "print(feature_importances.head(25))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_predictions(df_test_pred):\n",
    "    model = CatBoostRegressor()\n",
    "    model.load_model(f\"catboost_model_merged.cbm\")\n",
    "    \n",
    "    preds = model.predict(df_test_pred)\n",
    "    \n",
    "    # Inverse transform the predictions [Did not work well for us]\n",
    "    # preds = np.expm1(preds)\n",
    "    return preds\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model_locally(location, X_val_loc, y_val_loc):\n",
    "    # Load the test data\n",
    "    global scaling_target\n",
    "    \n",
    "    # Make predictions\n",
    "    preds = make_predictions(X_val_loc)[-200:]\n",
    "    target = y_val_loc.values[-200:]\n",
    "    if scaling_target:\n",
    "        if location == \"B\": \n",
    "            preds = (preds)/A_B_ratio\n",
    "        \n",
    "        if location == \"C\":\n",
    "            preds = (preds)/A_C_ratio\n",
    "\n",
    "    differences = preds - target\n",
    "\n",
    "    # Count predictions lower than the actual\n",
    "    lower_predictions = (differences < 0) & (target != 0)\n",
    "    # Count predictions higher than the actual\n",
    "    higher_predictions = (differences > 0) & (target != 0)\n",
    "\n",
    "    # # Biggest misreads\n",
    "    absolute_differences = abs(differences)\n",
    "    max_diff_index = absolute_differences.argmax()  # Index of the biggest difference\n",
    "    max_diff_value = absolute_differences[max_diff_index]  # Value of the biggest difference\n",
    "    print(f\"Number of predictions that are a lower value than the actual, given that the actual is not 0: {lower_predictions.sum()}\")\n",
    "    print(f\"Number of predictions that are larger than the target, given that the target is not 0: {higher_predictions.sum()}\")\n",
    "    print(f\"The biggest misread is at index {max_diff_index} with a difference of {max_diff_value}\")\n",
    "\n",
    "    # write best preds to a csv file (to compare with other tries)¨\n",
    "\n",
    "    # df = pd.DataFrame(preds)\n",
    "    # df.to_csv(f\"best_preds_{location}.csv\")\n",
    "    # make array of indices of numpy array target \n",
    "\n",
    "    # plot the preds from the csv file \"best_preds_{location}.csv\"\n",
    "    df = pd.read_csv(f\"best_preds_{location}.csv\")[-200:]\n",
    "    # df.reset_index(inplace=True)\n",
    "    best_preds = df[\"0\"]\n",
    "    print(preds)\n",
    "    index = np.arange(len(target))\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(60,6))\n",
    "    plt.plot(index, target, label=\"Target\")\n",
    "    plt.plot(index, preds, label=\"Predictions\")\n",
    "    plt.plot(index, best_preds.values, label=\"Best predictions\")\n",
    "\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Target vs Predictions')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "    plt.show()\n",
    "\n",
    "for location in locations:\n",
    "    X_val_loc = combined_X_val[combined_X_val['location'] == location].sort_index()\n",
    "    y_val_loc = combined_Y_val[combined_X_val['location'] == location].sort_index()\n",
    "    evaluate_model_locally(location, X_val_loc, y_val_loc)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export predictions to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "preds = make_predictions(combined_X_test)\n",
    "print(len(preds))\n",
    "print(len(df_submission))\n",
    "df_submission[\"prediction\"] = preds\n",
    "\n",
    "# Save the results to a new submission file\n",
    "df_submission[[\"id\", \"prediction\"]].to_csv(\"kaggle_submission_catboost_17.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
