{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General introduction\n",
    "\n",
    "Kaggle name: K Klustering Klan\n",
    "\n",
    "- Simen Seeberg-Rommetveit \n",
    "- Erik Hæstad Bjørnstad\n",
    "- Mathias Haakon Aas\n",
    "\n",
    "This is the long notebook containing these parts: \n",
    "- Data exploration \n",
    "- Feature engineering \n",
    "- Different models \n",
    "- Model interpratation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "direct_rad:W                      0.754767\n",
      "clear_sky_rad:W                   0.685952\n",
      "diffuse_rad:W                     0.665438\n",
      "direct_rad_1h:J                   0.661076\n",
      "is_in_shadow:idx                  0.617804\n",
      "clear_sky_energy_1h:J             0.612760\n",
      "diffuse_rad_1h:J                  0.605134\n",
      "is_day:idx                        0.569564\n",
      "sun_elevation:d                   0.440104\n",
      "ceiling_height_agl:m              0.228913\n",
      "effective_cloud_cover:p           0.227951\n",
      "visibility:m                      0.227326\n",
      "date_calc                         0.203918\n",
      "total_cloud_cover:p               0.190475\n",
      "air_density_2m:kgm3               0.147271\n",
      "wind_speed_v_10m:ms               0.124835\n",
      "dew_point_2m:K                    0.123936\n",
      "wind_speed_u_10m:ms               0.122025\n",
      "t_1000hPa:K                       0.120597\n",
      "absolute_humidity_2m:gm3          0.112695\n",
      "snow_water:kgm2                   0.097222\n",
      "relative_humidity_1000hPa:p       0.096870\n",
      "fresh_snow_24h:cm                 0.095611\n",
      "cloud_base_agl:m                  0.084773\n",
      "fresh_snow_12h:cm                 0.080159\n",
      "snow_depth:cm                     0.079995\n",
      "dew_or_rime:idx                   0.073287\n",
      "fresh_snow_6h:cm                  0.070095\n",
      "super_cooled_liquid_water:kgm2    0.069481\n",
      "fresh_snow_3h:cm                  0.060710\n",
      "rain_water:kgm2                   0.057966\n",
      "precip_type_5min:idx              0.056906\n",
      "precip_5min:mm                    0.055967\n",
      "fresh_snow_1h:cm                  0.051029\n",
      "sun_azimuth:d                     0.049702\n",
      "msl_pressure:hPa                  0.030976\n",
      "pressure_100m:hPa                 0.030839\n",
      "pressure_50m:hPa                  0.028767\n",
      "sfc_pressure:hPa                  0.026407\n",
      "prob_rime:p                       0.017702\n",
      "wind_speed_10m:ms                 0.008200\n",
      "elevation:m                            NaN\n",
      "snow_density:kgm3                      NaN\n",
      "snow_drift:idx                         NaN\n",
      "snow_melt_10min:mm                     NaN\n",
      "wind_speed_w_1000hPa:ms                NaN\n",
      "Name: pv_measurement, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def rank_features_by_correlation(data_parquet_file, target_parquet_file):\n",
    "    # Load main dataset and target dataset\n",
    "    df_data = pd.read_parquet(data_parquet_file).head(5000)\n",
    "    df_target = pd.read_parquet(target_parquet_file)\n",
    "\n",
    "    # Check if 'date_forecast' and 'time' columns exist in their respective DataFrames\n",
    "    if 'date_forecast' not in df_data.columns or 'time' not in df_target.columns:\n",
    "        raise ValueError(\"'date_forecast' or 'time' column not found in one or both datasets\")\n",
    "\n",
    "    # Merge the datasets based on 'date_forecast' and 'time'\n",
    "    df_merged = pd.merge(df_data, df_target, left_on='date_forecast', right_on='time', how='inner')\n",
    "\n",
    "    # Get the name of the target column (assuming it's the last column in df_target)\n",
    "    target_column = df_target.columns[-1]\n",
    "\n",
    "    # Check if target column exists in merged DataFrame\n",
    "    if target_column not in df_merged.columns:\n",
    "        raise ValueError(f\"Target column '{target_column}' not found in merged dataset\")\n",
    "\n",
    "    # Calculate correlation of each feature with the target column\n",
    "    correlation_scores = df_merged.drop(['date_forecast', 'time'], axis=1).corr()[target_column].drop(target_column)\n",
    "\n",
    "    # Rank features based on the absolute value of their correlation scores\n",
    "    ranked_features = correlation_scores.abs().sort_values(ascending=False)\n",
    "\n",
    "    return ranked_features\n",
    "\n",
    "# Usage example\n",
    "data_parquet_file = \"data/A/X_train_estimated.parquet\"\n",
    "target_parquet_file = \"data/A/train_targets.parquet\"\n",
    "ranked_features = rank_features_by_correlation(data_parquet_file, target_parquet_file)\n",
    "print(ranked_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended feature importance with data engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "direct_rad:W                   0.754767\n",
      "clear_sky_rad:W                0.685952\n",
      "diffuse_rad:W                  0.665438\n",
      "direct_rad_1h:J                0.661076\n",
      "is_in_shadow:idx               0.617804\n",
      "clear_sky_energy_1h:J          0.612760\n",
      "diffuse_rad_1h:J               0.605134\n",
      "is_day:idx                     0.569564\n",
      "sun_elevation_binned_Medium    0.569564\n",
      "sun_elevation_binned_Low       0.569564\n",
      "clear_sky_rad:W_no_outliers    0.560152\n",
      "sun_elevation:d                0.440104\n",
      "rad_change                     0.428878\n",
      "hour_cos                       0.395176\n",
      "effective_cloud_cover:p        0.227951\n",
      "visibility:m                   0.227326\n",
      "date_calc                      0.203918\n",
      "total_cloud_cover:p            0.190475\n",
      "month_sin                      0.180140\n",
      "month                          0.178678\n",
      "hour_sin                       0.170814\n",
      "month_cos                      0.155833\n",
      "air_density_2m:kgm3            0.147271\n",
      "ceiling_height_agl:m           0.129813\n",
      "wind_speed_v_10m:ms            0.124835\n",
      "dew_point_2m:K                 0.123936\n",
      "wind_speed_u_10m:ms            0.122025\n",
      "t_1000hPa:K                    0.120597\n",
      "absolute_humidity_2m:gm3       0.112695\n",
      "Name: pv_measurement, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def manipulate_single_feature(df, feature_name):\n",
    "    \"\"\"\n",
    "    Manipulate a single feature: \n",
    "    - Removing outliers\n",
    "    - Applying log transformation (if all values are positive)\n",
    "    - Returns a DataFrame with the manipulated feature and potential new features.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Outlier Removal\n",
    "    low, high = np.percentile(df[feature_name], [3, 97])\n",
    "    median = df[feature_name].median()\n",
    "    df[feature_name + '_no_outliers'] = np.where(df[feature_name] < low, median, df[feature_name])\n",
    "    df[feature_name + '_no_outliers'] = np.where(df[feature_name] > high, median, df[feature_name])\n",
    "    \n",
    "    # 2. Log Transformation\n",
    "    if df[feature_name].min() > 0:\n",
    "        df[feature_name + '_log'] = np.log(df[feature_name])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def rank_features_by_correlation(data_parquet_file, observed_parquet_file = None, target_parquet_file = None, threshold=None, num_features_to_keep=None):\n",
    "    # Load main dataset, observed dataset and target dataset\n",
    "    df_data = pd.read_parquet(data_parquet_file).head(5000)\n",
    "    if observed_parquet_file is not None:\n",
    "        df_observed = pd.read_parquet(observed_parquet_file).head(5000)\n",
    "    \n",
    "        # Concatenate the estimated and observed data vertically\n",
    "        df_data = pd.concat([df_data, df_observed], axis=0, ignore_index=True)\n",
    "\n",
    "    df_target = pd.read_parquet(target_parquet_file)\n",
    "\n",
    "    # Check if 'date_forecast' and 'time' columns exist in their respective DataFrames\n",
    "    if 'date_forecast' not in df_data.columns or 'time' not in df_target.columns:\n",
    "        raise ValueError(\"'date_forecast' or 'time' column not found in one or both datasets\")\n",
    "\n",
    "    # Merge the datasets based on 'date_forecast' and 'time'\n",
    "    df_merged = pd.merge(df_data, df_target, left_on='date_forecast', right_on='time', how='inner')\n",
    "\n",
    "    # Feature Engineering\n",
    "\n",
    "    # 1. Time-based Features\n",
    "    df_merged['hour'] = df_merged['time'].dt.hour\n",
    "    df_merged['month'] = df_merged['time'].dt.month\n",
    "    df_merged['weekday'] = df_merged['time'].dt.weekday\n",
    "\n",
    "    # Sinusoidal transformations for cyclical time features\n",
    "    df_merged['hour_sin'] = np.sin(2 * np.pi * df_merged['hour'] / 24)\n",
    "    df_merged['hour_cos'] = np.cos(2 * np.pi * df_merged['hour'] / 24)\n",
    "    df_merged['month_sin'] = np.sin(2 * np.pi * df_merged['month'] / 12)\n",
    "    df_merged['month_cos'] = np.cos(2 * np.pi * df_merged['month'] / 12)\n",
    "    df_merged['weekday_sin'] = np.sin(2 * np.pi * df_merged['weekday'] / 7)\n",
    "    df_merged['weekday_cos'] = np.cos(2 * np.pi * df_merged['weekday'] / 7)\n",
    "\n",
    "\n",
    "    # 2. Interaction Features\n",
    "    df_merged['snow_depth_radiation'] = df_merged['snow_depth:cm'] * df_merged['direct_rad:W']\n",
    "\n",
    "\n",
    "    # 6. NaN Handling\n",
    "    df_merged['snow_density:kgm3'].fillna(df_merged['snow_density:kgm3'].median(), inplace=True)\n",
    "    df_merged['ceiling_height_agl:m'].fillna(df_merged['ceiling_height_agl:m'].median(), inplace=True)\n",
    "    df_merged['cloud_base_agl:m'].fillna(df_merged['cloud_base_agl:m'].median(), inplace=True)\n",
    "\n",
    "    # 7. Binning\n",
    "    bins = [-90, 0, 45, 90]\n",
    "    labels = ['Low', 'Medium', 'High']\n",
    "    df_merged['sun_elevation_binned'] = pd.cut(df_merged['sun_elevation:d'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "\n",
    "\n",
    "    # 4. Derivative Features\n",
    "    df_merged['rad_change'] = df_merged['direct_rad:W'].diff().fillna(0)\n",
    "    # Similar calculations for other columns if deemed important.\n",
    "\n",
    "\n",
    "    # One-hot encode 'sun_elevation_binned'\n",
    "    df_merged = pd.get_dummies(df_merged, columns=['sun_elevation_binned'])\n",
    "\n",
    "    # Drop Constant Features\n",
    "    df_merged.drop(['elevation:m', 'snow_drift:idx', 'snow_melt_10min:mm', 'wind_speed_w_1000hPa:ms'], axis=1, inplace=True)\n",
    "    df_merged = manipulate_single_feature(df_merged, 'clear_sky_rad:W')\n",
    "    # Get the name of the target column (assuming it's the last column in df_target)\n",
    "    target_column = df_target.columns[-1]\n",
    "    \n",
    "    # Calculate correlation of each feature with the target column\n",
    "    correlation_scores = df_merged.drop(['date_forecast', 'time'], axis=1).corr()[target_column].drop(target_column)\n",
    "\n",
    "    # Rank features based on the absolute value of their correlation scores\n",
    "    ranked_features = correlation_scores.abs().sort_values(ascending=False)\n",
    "\n",
    "    if threshold is not None:\n",
    "        ranked_features = ranked_features[ranked_features > threshold]\n",
    "\n",
    "    if num_features_to_keep is not None:\n",
    "        ranked_features = ranked_features.head(num_features_to_keep)\n",
    "    \n",
    "    return ranked_features\n",
    "\n",
    "\n",
    "\n",
    "# Usage example\n",
    "data_parquet_file = \"data/A/X_train_estimated.parquet\"\n",
    "observed_parquet_file = \"data/A/X_train_observed.parquet\"\n",
    "target_parquet_file = \"data/A/train_targets.parquet\"\n",
    "\n",
    "ranked_features = rank_features_by_correlation(data_parquet_file=data_parquet_file, observed_parquet_file=observed_parquet_file, target_parquet_file=target_parquet_file, threshold=0.1)\n",
    "# ranked_features = rank_features_by_correlation(data_parquet_file, target_parquet_file=target_parquet_file, threshold=0.1)\n",
    "ranked_features = rank_features_by_correlation(data_parquet_file, target_parquet_file=target_parquet_file, threshold=0.1)\n",
    "\n",
    "\n",
    "print(ranked_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering to new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_and_save_engineered_features(estimated_parquet_file=None, observed_parquet_file=None, target_parquet_file=None, output_parquet_file=\"engineered_data.parquet\"):\n",
    "    # Load main dataset, observed dataset, and target dataset\n",
    "    df_estimated = pd.read_parquet(estimated_parquet_file)\n",
    "    df_observed = pd.read_parquet(observed_parquet_file)\n",
    "\n",
    "    df_target = pd.read_parquet(target_parquet_file)\n",
    "    df_merged = pd.concat([df_observed, df_estimated], axis=0, ignore_index=True)\n",
    "    print(df_merged.head())\n",
    "\n",
    "    # Check if 'date_forecast' and 'time' columns exist in their respective DataFrames\n",
    "    if 'date_forecast' not in df_merged.columns or 'time' not in df_target.columns:\n",
    "        raise ValueError(\"'date_forecast' or 'time' column not found in one or both datasets\")\n",
    "\n",
    "\n",
    "    # Feature Engineering\n",
    "    df_merged = feature_engineering(df_merged)\n",
    "\n",
    "    # Save the engineered data\n",
    "    df_merged.to_parquet(output_parquet_file)\n",
    "    print(f\"Engineered data saved to {output_parquet_file}\")\n",
    "\n",
    "\n",
    "def feature_engineering(df_merged):\n",
    "    # Time-based Features\n",
    "    df_merged['hour'] = df_merged['date_forecast'].dt.hour\n",
    "    df_merged['month'] = df_merged['date_forecast'].dt.month\n",
    "    df_merged['weekday'] = df_merged['date_forecast'].dt.weekday\n",
    "\n",
    "    # Sinusoidal transformations for cyclical time features\n",
    "    df_merged['hour_sin'] = np.sin(2 * np.pi * df_merged['hour'] / 24)\n",
    "    df_merged['hour_cos'] = np.cos(2 * np.pi * df_merged['hour'] / 24)\n",
    "    df_merged['month_sin'] = np.sin(2 * np.pi * df_merged['month'] / 12)\n",
    "    df_merged['month_cos'] = np.cos(2 * np.pi * df_merged['month'] / 12)\n",
    "    df_merged['weekday_sin'] = np.sin(2 * np.pi * df_merged['weekday'] / 7)\n",
    "    df_merged['weekday_cos'] = np.cos(2 * np.pi * df_merged['weekday'] / 7)\n",
    "\n",
    "    # Binning\n",
    "    bins = [-90, 0, 45, 90]\n",
    "    labels = ['1', '2', '3']\n",
    "    df_merged['sun_elevation_binned'] = pd.cut(df_merged['sun_elevation:d'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "\n",
    "    return df_merged\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Usage example\n",
    "estimated_parquet_file = \"data/C/X_train_estimated.parquet\"\n",
    "observed_parquet_file = \"data/C/X_train_observed.parquet\"\n",
    "target_parquet_file = \"data/C/train_targets.parquet\"\n",
    "output_file = \"cleaned_data/C/X_train_engineered.parquet\"\n",
    "\n",
    "generate_and_save_engineered_features(estimated_parquet_file=estimated_parquet_file, observed_parquet_file=observed_parquet_file, target_parquet_file=target_parquet_file, output_parquet_file=output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
