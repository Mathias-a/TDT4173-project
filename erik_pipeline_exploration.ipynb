{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constans and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = [\"A\", \"B\", \"C\"]\n",
    "features_order = []\n",
    "A_B_ratio = 6.73\n",
    "A_C_ratio = 8.17\n",
    "\n",
    "LAGGED_COLUMNS_TO_KEEP = [\n",
    "    # 'direct_rad:W_lag_1h', \n",
    "    'direct_rad:W_lag_forward_1h', \n",
    "    # 'clear_sky_rad:W_lag_1h', \n",
    "    'clear_sky_rad:W_lag_forward_1h', \n",
    "    # 'diffuse_rad:W_lag_1h', \n",
    "    'diffuse_rad:W_lag_forward_1h', \n",
    "    # 'direct_rad_1h:J_lag_1h', \n",
    "    'direct_rad_1h:J_lag_forward_1h', \n",
    "    # 'is_in_shadow:idx_lag_1h', \n",
    "    'is_in_shadow:idx_lag_forward_1h', \n",
    "    # 'clear_sky_energy_1h:J_lag_1h', \n",
    "    'clear_sky_energy_1h:J_lag_forward_1h', \n",
    "    # 'effective_cloud_cover:p_lag_1h', \n",
    "    'effective_cloud_cover:p_lag_forward_1h', \n",
    "    # 'visibility:m_lag_1h', \n",
    "    'visibility:m_lag_forward_1h', \n",
    "    # 'total_cloud_cover:p_lag_1h', \n",
    "    'total_cloud_cover:p_lag_forward_1h', \n",
    "\n",
    "\n",
    "    # 'direct_rad:W_lag_2h', \n",
    "    # 'direct_rad:W_lag_forward_2h', \n",
    "    # 'clear_sky_rad:W_lag_2h', \n",
    "    # 'clear_sky_rad:W_lag_forward_2h', \n",
    "    # 'diffuse_rad:W_lag_2h', \n",
    "    # 'diffuse_rad:W_lag_forward_2h', \n",
    "    # 'direct_rad_1h:J_lag_2h', \n",
    "    # 'direct_rad_1h:J_lag_forward_2h', \n",
    "    # 'is_in_shadow:idx_lag_2h', \n",
    "    # 'is_in_shadow:idx_lag_forward_2h', \n",
    "    # 'clear_sky_energy_1h:J_lag_2h', \n",
    "    # 'clear_sky_energy_1h:J_lag_forward_2h', \n",
    "    # 'effective_cloud_cover:p_lag_2h', \n",
    "    # 'effective_cloud_cover:p_lag_forward_2h', \n",
    "    # 'visibility:m_lag_2h', \n",
    "    # 'visibility:m_lag_forward_2h', \n",
    "    # 'total_cloud_cover:p_lag_2h', \n",
    "    # 'total_cloud_cover:p_lag_forward_2h', \n",
    "\n",
    "    # 'direct_rad:W_lag_3h', \n",
    "    # 'direct_rad:W_lag_forward_3h', \n",
    "    # 'clear_sky_rad:W_lag_3h', \n",
    "    # 'clear_sky_rad:W_lag_forward_3h', \n",
    "    # 'diffuse_rad:W_lag_3h', \n",
    "    # 'diffuse_rad:W_lag_forward_3h', \n",
    "    # 'direct_rad_1h:J_lag_3h', \n",
    "    # 'direct_rad_1h:J_lag_forward_3h', \n",
    "    # 'is_in_shadow:idx_lag_3h', \n",
    "    # 'is_in_shadow:idx_lag_forward_3h', \n",
    "    # 'clear_sky_energy_1h:J_lag_3h', \n",
    "    # 'clear_sky_energy_1h:J_lag_forward_3h', \n",
    "    # 'effective_cloud_cover:p_lag_3h', \n",
    "    # 'effective_cloud_cover:p_lag_forward_3h', \n",
    "    # 'visibility:m_lag_3h', \n",
    "    # 'visibility:m_lag_forward_3h', \n",
    "    # 'total_cloud_cover:p_lag_3h', \n",
    "    # 'total_cloud_cover:p_lag_forward_3h'\n",
    "]\n",
    "\n",
    "CUSTOM_COLUMNS_TO_KEEP = [\n",
    "    \"hour_cos\",\n",
    "    \"hour_sin\",\n",
    "    \"month_sin\",\n",
    "    \"month_cos\",\n",
    "    \"day-of-year\",\n",
    "    \"hours_since_forecast\",\n",
    "    # \"sun_azimuth_cos\",\n",
    "    # \"sun_azimuth_sin\",\n",
    "]\n",
    "\n",
    "WEATHER_FEATURES = [\n",
    "    \"direct_rad:W\",\n",
    "    \"clear_sky_rad:W\",\n",
    "    \"diffuse_rad:W\",\n",
    "    \"direct_rad_1h:J\",\n",
    "    \"is_in_shadow:idx\",\n",
    "    \"clear_sky_energy_1h:J\",\n",
    "    \"effective_cloud_cover:p\",\n",
    "    \"visibility:m\",\n",
    "    \"total_cloud_cover:p\",\n",
    "]\n",
    "\n",
    "\n",
    "TEST_COLUMNS_TO_KEEP = [\n",
    "    \"direct_rad:W\",\n",
    "    \"clear_sky_rad:W\",\n",
    "    \"diffuse_rad:W\",\n",
    "    \"direct_rad_1h:J\",\n",
    "    \"is_in_shadow:idx\",\n",
    "    \"clear_sky_energy_1h:J\",\n",
    "    \"diffuse_rad_1h:J\",\n",
    "    \"is_day:idx\",\n",
    "    \"sun_elevation:d\",\n",
    "    \"ceiling_height_agl:m\",\n",
    "    \"effective_cloud_cover:p\",\n",
    "    \"visibility:m\",\n",
    "    \"total_cloud_cover:p\",\n",
    "    \"air_density_2m:kgm3\",\n",
    "    \"wind_speed_v_10m:ms\",\n",
    "    \"dew_point_2m:K\",\n",
    "    \"wind_speed_u_10m:ms\",\n",
    "    \"t_1000hPa:K\",\n",
    "    \"absolute_humidity_2m:gm3\",\n",
    "    \"snow_water:kgm2\",\n",
    "    \"relative_humidity_1000hPa:p\",\n",
    "    \"fresh_snow_24h:cm\",\n",
    "    \"cloud_base_agl:m\",\n",
    "    \"fresh_snow_12h:cm\",\n",
    "    \"snow_depth:cm\",\n",
    "    \"dew_or_rime:idx\",\n",
    "    \"fresh_snow_6h:cm\",\n",
    "    \"super_cooled_liquid_water:kgm2\",\n",
    "    \"fresh_snow_3h:cm\",\n",
    "    \"rain_water:kgm2\",\n",
    "    \"precip_type_5min:idx\",\n",
    "    \"precip_5min:mm\",\n",
    "    \"fresh_snow_1h:cm\",\n",
    "    \"sun_azimuth:d\",\n",
    "    \"msl_pressure:hPa\",\n",
    "    \"pressure_100m:hPa\",\n",
    "    \"pressure_50m:hPa\",\n",
    "    \"sfc_pressure:hPa\",\n",
    "    \"prob_rime:p\",\n",
    "    \"wind_speed_10m:ms\",\n",
    "    \"elevation:m\",\n",
    "    # \"snow_density:kgm3\",\n",
    "    \"snow_drift:idx\",\n",
    "    \"snow_melt_10min:mm\",\n",
    "    \"wind_speed_w_1000hPa:ms\",\n",
    "    # \"observed_or_estimated\",\n",
    "    # \"location_A\",\n",
    "    # \"location_B\",\n",
    "    # \"location_C\",\n",
    "    # \"date_calc\",\n",
    "] + CUSTOM_COLUMNS_TO_KEEP  +  LAGGED_COLUMNS_TO_KEEP\n",
    "\n",
    "COLUMNS_TO_KEEP = TEST_COLUMNS_TO_KEEP + [\"pv_measurement\"]\n",
    "\n",
    "\n",
    "\n",
    "def create_weather_lagged_features(df, weather_features):\n",
    "    # Choose the weather features for which you want to create lagged versions\n",
    "    for feature in weather_features:\n",
    "        # Assuming hourly data, adjust the lags for your specific dataset\n",
    "        # Creating lagged features for 1 hour, 1 day, and 1 week\n",
    "        # df[f'{feature}_lag_1h'] = df[feature].shift(1)\n",
    "        # df[f'{feature}_lag_2h'] = df[feature].shift(2)\n",
    "        # df[f'{feature}_lag_3h'] = df[feature].shift(3)\n",
    "\n",
    "        df[f'{feature}_lag_forward_1h'] = df[feature].shift(-1)\n",
    "        # df[f'{feature}_lag_forward_2h'] = df[feature].shift(-2)\n",
    "        # df[f'{feature}_lag_forward_3h'] = df[feature].shift(-3)\n",
    "        # df[f'{feature}_lag_24h'] = df[feature].shift(24*4)\n",
    "        # df[f'{feature}_lag_168h'] = df[feature].shift(24 * 7 * 4 * 365)\n",
    "        # df[f'{feature}_front_lag_1h'] = df[feature].shift(-4)\n",
    "        # df[f'{feature}_front_lag_24h'] = df[feature].shift(-24*4)\n",
    "\n",
    "\n",
    "    # Handling edges by filling NaNs with appropriate values or dropping them\n",
    "    # You may choose to fill with zeroes or interpolate, based on what makes more sense for your data\n",
    "    # df.fillna(method='ffill', inplace=True)  # Forward fill  # Autogluon should handle this for us.\n",
    "    # df.fillna(method='bfill', inplace=True)  # Backward fill  # Autogluon should handle this for us.\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_custom_fields(df):\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['date_forecast'].dt.hour / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['date_forecast'].dt.hour / 24)\n",
    "\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['date_forecast'].dt.month / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['date_forecast'].dt.month / 12)\n",
    "    df['day-of-year'] = df['date_forecast'].dt.dayofyear\n",
    "\n",
    "    # df['sun_azimuth_cos'] = np.cos(np.radians(df['sun_azimuth:d']))\n",
    "    # df['sun_azimuth_sin'] = np.sin(np.radians(df['sun_azimuth:d']))\n",
    "    # df.drop(columns=['sun_azimuth:d'], inplace=True)\n",
    "   \n",
    "    return df\n",
    "\n",
    "def add_calc_date(df_observed, df_estimated, df_test):\n",
    "    # Function to calculate the difference in hours\n",
    "    def calculate_hour_difference(row):\n",
    "        diff = row['date_calc'] - row['date_forecast']\n",
    "        return diff.total_seconds() / 3600  # Convert difference to hours\n",
    "\n",
    "    # Apply the function to calculate the hour difference for df_estimated and df_test\n",
    "    df_estimated['hours_since_forecast'] = df_estimated.apply(calculate_hour_difference, axis=1)\n",
    "    df_test['hours_since_forecast'] = df_test.apply(calculate_hour_difference, axis=1)\n",
    "\n",
    "    # Fill in zero for df_observed\n",
    "    df_observed['hours_since_forecast'] = 0\n",
    "\n",
    "    return df_observed, df_estimated, df_test\n",
    "\n",
    "\n",
    "def remove_outliers(df):\n",
    "    # Use a mask to filter out the rows where rolling std is zero but keep the rows where the value itself is zero\n",
    "    # Because some places in the data, the pv-measurements are messed up and are repeating.\n",
    "    mask = (df['pv_measurement'].rolling(2).std() == 0) & (df['pv_measurement'] != 0)\n",
    "    df[mask] = np.NaN  # Put this to NaN and hope autoGluon Handles.\n",
    "    return df\n",
    "\n",
    "def resample_add_data(df, is_test_data):\n",
    "    df = add_custom_fields(df)\n",
    "    df.set_index('date_forecast', inplace=True)\n",
    "    df = df.resample('1H').mean()\n",
    "    \n",
    "    # Remove empty dates if test data\n",
    "    if is_test_data:\n",
    "        non_nan_threshold = len(df.columns) // 2\n",
    "        df.dropna(thresh=non_nan_threshold, inplace=True)\n",
    "\n",
    "    # df.interpolate(method=\"linear\", inplace=True)  # Autogluon should handle this for us.\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_location_feature(X, location):\n",
    "      # Treat location as a categorical feature by converting it to a category type\n",
    "    \n",
    "    # bins = [-90, 0, 45, 90]\n",
    "    # labels = ['Low', 'Medium', 'High']\n",
    "    # X['sun_elevation:d'] = pd.cut(X['sun_elevation:d'], bins=bins, labels=labels, include_lowest=True)\n",
    "    X['location'] = location\n",
    "    X['location'] = X['location'].astype(str)  # Convert to string if 'location' is not an int\n",
    "    # AutoGluon should pick up these by itself.\n",
    "    # X['dew_or_rime:idx'] = X['dew_or_rime:idx'].astype(str)\n",
    "    # X['is_day:idx'] = X['is_day:idx'].astype(str)\n",
    "    # X['is_in_shadow:idx'] = X['is_in_shadow:idx'].astype(str)\n",
    "    # categorical_columns = ['location', 'dew_or_rime:idx', 'is_day:idx', 'is_in_shadow:idx']\n",
    "\n",
    "    # Before filling NaN values, add 'missing' as a category for each categorical column.\n",
    "    # for column in categorical_columns:\n",
    "    #     X[column] = X[column].astype('category')  # Ensure the column is of type 'category'.\n",
    "    #     if 'missing' not in X[column].cat.categories:\n",
    "    #         X[column] = X[column].cat.add_categories(['missing'])  # Add 'missing' as a new category.\n",
    "    #     X[column] = X[column].fillna('missing')\n",
    "    # X['location'] = X['location'].astype('category')\n",
    "    # X['dew_or_rime:idx'] = X['dew_or_rime:idx'].astype('category')\n",
    "    # X['is_day:idx'] = X['is_day:idx'].astype('category')\n",
    "    # X['is_in_shadow:idx'] = X['is_in_shadow:idx'].astype('category')\n",
    "    # X['sun_elevation:d'] = X['sun_elevation:d'].astype('category')\n",
    "    \n",
    "    return X\n",
    "\n",
    "def add_scaling(X_test, X_training, location):\n",
    "    global scalers\n",
    "    continuous_columns = X_training.select_dtypes(include=['float32', 'int32']).columns\n",
    "    if location not in scalers:\n",
    "        scalers[location] = MinMaxScaler()\n",
    "    X_training[continuous_columns] = scalers[location].fit_transform(X_training[continuous_columns])\n",
    "    \n",
    "    X_test[continuous_columns] = scalers[location].transform(X_test[continuous_columns])\n",
    "\n",
    "    return X_test, X_training\n",
    "\n",
    "# Skip this as we have hours since forecast as a feature.\n",
    "# Deprecated as the concat is moved to main function.\n",
    "def make_observed_and_estimated_category(df_observed, df_estimated, df_test):\n",
    "     # Hot encode in wether observed or estimated\n",
    "    df_observed['observed_or_estimated'] = 'observed'\n",
    "    df_estimated['observed_or_estimated'] = 'estimated'\n",
    "    df_test['observed_or_estimated'] = 'estimated'\n",
    "    # Concatenate observed and estimated\n",
    "    df_training = pd.concat([df_observed, df_estimated], axis=0).sort_values(by=\"date_forecast\")\n",
    "    df_training['observed_or_estimated'] = df_training['observed_or_estimated'].astype('category')\n",
    "    df_test['observed_or_estimated'] = df_test['observed_or_estimated'].astype('category')\n",
    "\n",
    "    return df_training, df_test\n",
    "\n",
    "def analyze_pv_measurements(dataframe):\n",
    "    # Filter the DataFrame for rows where 'is_day' is 0\n",
    "    night_data = dataframe[dataframe['is_day:idx'] == 0]\n",
    "\n",
    "    # Analysis of 'pv_measurements' during night time\n",
    "    pv_stats = night_data['pv_measurement'].describe()\n",
    "\n",
    "    return pv_stats\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    15952.000000\n",
      "mean         0.087424\n",
      "std          0.952186\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max         30.140000\n",
      "Name: pv_measurement, dtype: float64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    14756.000000\n",
      "mean         0.072512\n",
      "std          2.906017\n",
      "min         -0.000000\n",
      "25%         -0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max        273.412500\n",
      "Name: pv_measurement, dtype: float64\n",
      "count    13165.000000\n",
      "mean         0.042133\n",
      "std          0.679659\n",
      "min         -0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max         27.440000\n",
      "Name: pv_measurement, dtype: float64\n",
      "count    15952.000000\n",
      "mean         0.087424\n",
      "std          0.952186\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max         30.140000\n",
      "Name: pv_measurement, dtype: float64\n",
      "count    14756.000000\n",
      "mean         0.072512\n",
      "std          2.906017\n",
      "min         -0.000000\n",
      "25%         -0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max        273.412500\n",
      "Name: pv_measurement, dtype: float64\n",
      "count    13165.000000\n",
      "mean         0.042133\n",
      "std          0.679659\n",
      "min         -0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max         27.440000\n",
      "Name: pv_measurement, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize a dictionary to hold the scalers for each location\n",
    "\n",
    "scalers = {}\n",
    "scaling_target = False\n",
    "\n",
    "def prepare_data(location):\n",
    "    # Load data\n",
    "\n",
    "    scaling = False  # Set scaling to True to enable individual scaling for each location  # Autogluon should handle this for us.\n",
    "    global scalers\n",
    "    global scaling_target\n",
    "    # Load training data\n",
    "    df_observed = pd.read_parquet(f\"data/{location}/X_train_observed.parquet\")\n",
    "    df_estimated = pd.read_parquet(f\"data/{location}/X_train_estimated.parquet\")\n",
    "    df_target = pd.read_parquet(f\"data/{location}/train_targets.parquet\")\n",
    "\n",
    "\n",
    "    # Load test data\n",
    "    df_test = pd.read_parquet(f\"data/{location}/X_test_estimated.parquet\")\n",
    "\n",
    "    # Add calculated date\n",
    "    df_observed, df_estimated, df_test = add_calc_date(df_observed, df_estimated, df_test)\n",
    "    # Hot encode in wether observed or estimated\n",
    "\n",
    "    # Resample and add custom fields\n",
    "\n",
    "    df_observed = resample_add_data(df_observed, False)\n",
    "    df_estimated = resample_add_data(df_estimated, False)\n",
    "    df_test = resample_add_data(df_test, True)\n",
    "\n",
    "    # df_training, df_test = make_observed_and_estimated_category(df_observed, df_estimated, df_test)  # Skip this as we have hours since forecast as a feature.\n",
    "    # Concat observed and estimated\n",
    "    df_training = pd.concat([df_observed, df_estimated], axis=0).sort_values(by=\"date_forecast\")\n",
    "    \n",
    "    # Merge training with target data\n",
    "    df_training = pd.merge(df_training, df_target, left_on=\"date_forecast\", right_on=\"time\", how=\"inner\")\n",
    "\n",
    "    # Create lagged features and remove outliers training\n",
    "    df_training = create_weather_lagged_features(df_training, WEATHER_FEATURES)\n",
    "    df_training = df_training[COLUMNS_TO_KEEP]\n",
    "    df_training = remove_outliers(df_training)\n",
    "\n",
    "    # Create lagged features test\n",
    "    df_test = create_weather_lagged_features(df_test, WEATHER_FEATURES)\n",
    "    df_test = df_test[TEST_COLUMNS_TO_KEEP]\n",
    "\n",
    "    print(analyze_pv_measurements(df_training))\n",
    "    # Make training x and y\n",
    "    y_training = df_training[\"pv_measurement\"]\n",
    "    X_training = df_training.drop(\"pv_measurement\", axis=1)\n",
    "\n",
    "    # Add categories\n",
    "    X_training = add_location_feature(X_training, location)\n",
    "    X_test = add_location_feature(df_test, location)\n",
    "    \n",
    "    # Add scaling\n",
    "    if scaling:\n",
    "        X_test, X_training = add_scaling(X_test, X_training, location)\n",
    "    X_test.reset_index(inplace=True)\n",
    "    X_test.drop(\"date_forecast\", axis=1, inplace=True)\n",
    "    # y_training = np.log1p(y_training)  # Autogluon will fix\n",
    "    if scaling_target:\n",
    "        if location == \"B\":\n",
    "            y_training = A_B_ratio*(y_training)\n",
    "        if location == \"C\":\n",
    "            y_training = A_C_ratio*(y_training)\n",
    "    return X_training, X_test, y_training\n",
    "\n",
    "\n",
    "# Use prepare_data function\n",
    "combined_X_train = pd.DataFrame()\n",
    "combined_X_val = pd.DataFrame()\n",
    "combined_Y_train = pd.DataFrame()\n",
    "combined_Y_val = pd.DataFrame()\n",
    "combined_X_test = pd.DataFrame()\n",
    "\n",
    "# For validation locally \n",
    "for location in locations:\n",
    "    # Prepare the training data\n",
    "    X_training, X_test, y_training = prepare_data(location)\n",
    "    \n",
    "    # Split and concatenate the training data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_training, y_training, test_size=0.1, random_state=42)  # Set to 0.0001 to train on everything\n",
    "    combined_X_train = pd.concat([combined_X_train, X_train])\n",
    "    combined_X_val = pd.concat([combined_X_val, X_val])\n",
    "    combined_Y_train = pd.concat([combined_Y_train, y_train])\n",
    "    combined_Y_val = pd.concat([combined_Y_val, y_val])\n",
    "    combined_X_test = pd.concat([combined_X_test, X_test])\n",
    "\n",
    "\n",
    "combined_X_test\n",
    "combined_X_train, combined_Y_train = shuffle(combined_X_train, combined_Y_train, random_state=42)\n",
    "combined_X_val, combined_Y_val = shuffle(combined_X_val, combined_Y_val, random_state=42)\n",
    "\n",
    "for location in locations: \n",
    "    prepare_data(location)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
