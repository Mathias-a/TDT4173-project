{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constans and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = [\"A\", \"B\", \"C\"]\n",
    "features_order = []\n",
    "\n",
    "LAGGED_COLUMNS_TO_KEEP = [\n",
    "    'direct_rad:W_lag_1h', \n",
    "    'direct_rad:W_lag_forward_1h', \n",
    "    'clear_sky_rad:W_lag_1h', \n",
    "    'clear_sky_rad:W_lag_forward_1h', \n",
    "    'diffuse_rad:W_lag_1h', \n",
    "    'diffuse_rad:W_lag_forward_1h', \n",
    "    'direct_rad_1h:J_lag_1h', \n",
    "    'direct_rad_1h:J_lag_forward_1h', \n",
    "    'is_in_shadow:idx_lag_1h', \n",
    "    'is_in_shadow:idx_lag_forward_1h', \n",
    "    'clear_sky_energy_1h:J_lag_1h', \n",
    "    'clear_sky_energy_1h:J_lag_forward_1h', \n",
    "    'effective_cloud_cover:p_lag_1h', \n",
    "    'effective_cloud_cover:p_lag_forward_1h', \n",
    "    'visibility:m_lag_1h', \n",
    "    'visibility:m_lag_forward_1h', \n",
    "    'total_cloud_cover:p_lag_1h', \n",
    "    'total_cloud_cover:p_lag_forward_1h', \n",
    "\n",
    "\n",
    "    # 'direct_rad:W_lag_2h', \n",
    "    # 'direct_rad:W_lag_forward_2h', \n",
    "    # 'clear_sky_rad:W_lag_2h', \n",
    "    # 'clear_sky_rad:W_lag_forward_2h', \n",
    "    # 'diffuse_rad:W_lag_2h', \n",
    "    # 'diffuse_rad:W_lag_forward_2h', \n",
    "    # 'direct_rad_1h:J_lag_2h', \n",
    "    # 'direct_rad_1h:J_lag_forward_2h', \n",
    "    # 'is_in_shadow:idx_lag_2h', \n",
    "    # 'is_in_shadow:idx_lag_forward_2h', \n",
    "    # 'clear_sky_energy_1h:J_lag_2h', \n",
    "    # 'clear_sky_energy_1h:J_lag_forward_2h', \n",
    "    # 'effective_cloud_cover:p_lag_2h', \n",
    "    # 'effective_cloud_cover:p_lag_forward_2h', \n",
    "    # 'visibility:m_lag_2h', \n",
    "    # 'visibility:m_lag_forward_2h', \n",
    "    # 'total_cloud_cover:p_lag_2h', \n",
    "    # 'total_cloud_cover:p_lag_forward_2h', \n",
    "\n",
    "    # 'direct_rad:W_lag_3h', \n",
    "    # 'direct_rad:W_lag_forward_3h', \n",
    "    # 'clear_sky_rad:W_lag_3h', \n",
    "    # 'clear_sky_rad:W_lag_forward_3h', \n",
    "    # 'diffuse_rad:W_lag_3h', \n",
    "    # 'diffuse_rad:W_lag_forward_3h', \n",
    "    # 'direct_rad_1h:J_lag_3h', \n",
    "    # 'direct_rad_1h:J_lag_forward_3h', \n",
    "    # 'is_in_shadow:idx_lag_3h', \n",
    "    # 'is_in_shadow:idx_lag_forward_3h', \n",
    "    # 'clear_sky_energy_1h:J_lag_3h', \n",
    "    # 'clear_sky_energy_1h:J_lag_forward_3h', \n",
    "    # 'effective_cloud_cover:p_lag_3h', \n",
    "    # 'effective_cloud_cover:p_lag_forward_3h', \n",
    "    # 'visibility:m_lag_3h', \n",
    "    # 'visibility:m_lag_forward_3h', \n",
    "    # 'total_cloud_cover:p_lag_3h', \n",
    "    # 'total_cloud_cover:p_lag_forward_3h'\n",
    "]\n",
    "\n",
    "CUSTOM_COLUMNS_TO_KEEP = [\n",
    "    \"hour_cos\",\n",
    "    \"hour_sin\",\n",
    "    \"month_sin\",\n",
    "    \"month_cos\",\n",
    "    \"day-of-year\",\n",
    "]\n",
    "\n",
    "WEATHER_FEATURES = [\n",
    "    \"direct_rad:W\",\n",
    "    \"clear_sky_rad:W\",\n",
    "    \"diffuse_rad:W\",\n",
    "    \"direct_rad_1h:J\",\n",
    "    \"is_in_shadow:idx\",\n",
    "    \"clear_sky_energy_1h:J\",\n",
    "    \"effective_cloud_cover:p\",\n",
    "    \"visibility:m\",\n",
    "    \"total_cloud_cover:p\",\n",
    "]\n",
    "\n",
    "\n",
    "TEST_COLUMNS_TO_KEEP = [\n",
    "    \"direct_rad:W\",\n",
    "    \"clear_sky_rad:W\",\n",
    "    \"diffuse_rad:W\",\n",
    "    \"direct_rad_1h:J\",\n",
    "    \"is_in_shadow:idx\",\n",
    "    \"clear_sky_energy_1h:J\",\n",
    "    \"diffuse_rad_1h:J\",\n",
    "    \"is_day:idx\",\n",
    "    \"sun_elevation:d\",\n",
    "    \"ceiling_height_agl:m\",\n",
    "    \"effective_cloud_cover:p\",\n",
    "    \"visibility:m\",\n",
    "    \"total_cloud_cover:p\",\n",
    "    \"air_density_2m:kgm3\",\n",
    "    \"wind_speed_v_10m:ms\",\n",
    "    \"dew_point_2m:K\",\n",
    "    \"wind_speed_u_10m:ms\",\n",
    "    \"t_1000hPa:K\",\n",
    "    \"absolute_humidity_2m:gm3\",\n",
    "    \"snow_water:kgm2\",\n",
    "    \"relative_humidity_1000hPa:p\",\n",
    "    \"fresh_snow_24h:cm\",\n",
    "    \"cloud_base_agl:m\",\n",
    "    \"fresh_snow_12h:cm\",\n",
    "    \"snow_depth:cm\",\n",
    "    \"dew_or_rime:idx\",\n",
    "    \"fresh_snow_6h:cm\",\n",
    "    \"super_cooled_liquid_water:kgm2\",\n",
    "    \"fresh_snow_3h:cm\",\n",
    "    \"rain_water:kgm2\",\n",
    "    \"precip_type_5min:idx\",\n",
    "    \"precip_5min:mm\",\n",
    "    \"fresh_snow_1h:cm\",\n",
    "    \"sun_azimuth:d\",\n",
    "    \"msl_pressure:hPa\",\n",
    "    \"pressure_100m:hPa\",\n",
    "    \"pressure_50m:hPa\",\n",
    "    \"sfc_pressure:hPa\",\n",
    "    \"prob_rime:p\",\n",
    "    \"wind_speed_10m:ms\",\n",
    "    \"elevation:m\",\n",
    "    # \"snow_density:kgm3\",\n",
    "    \"snow_drift:idx\",\n",
    "    \"snow_melt_10min:mm\",\n",
    "    \"wind_speed_w_1000hPa:ms\",\n",
    "    \"observed_or_estimated\"\n",
    "    # \"location_A\",\n",
    "    # \"location_B\",\n",
    "    # \"location_C\",\n",
    "    # \"date_calc\",\n",
    "] + CUSTOM_COLUMNS_TO_KEEP  +  LAGGED_COLUMNS_TO_KEEP\n",
    "\n",
    "COLUMNS_TO_KEEP = TEST_COLUMNS_TO_KEEP + [\"pv_measurement\"]\n",
    "\n",
    "\n",
    "\n",
    "def create_weather_lagged_features(df, weather_features):\n",
    "    # Choose the weather features for which you want to create lagged versions\n",
    "    for feature in weather_features:\n",
    "        # Assuming hourly data, adjust the lags for your specific dataset\n",
    "        # Creating lagged features for 1 hour, 1 day, and 1 week\n",
    "        df[f'{feature}_lag_1h'] = df[feature].shift(1)\n",
    "        # df[f'{feature}_lag_2h'] = df[feature].shift(2)\n",
    "        # df[f'{feature}_lag_3h'] = df[feature].shift(3)\n",
    "\n",
    "        df[f'{feature}_lag_forward_1h'] = df[feature].shift(-1)\n",
    "        # df[f'{feature}_lag_forward_2h'] = df[feature].shift(-2)\n",
    "        # df[f'{feature}_lag_forward_3h'] = df[feature].shift(-3)\n",
    "        # df[f'{feature}_lag_24h'] = df[feature].shift(24*4)\n",
    "        # df[f'{feature}_lag_168h'] = df[feature].shift(24 * 7 * 4 * 365)\n",
    "        # df[f'{feature}_front_lag_1h'] = df[feature].shift(-4)\n",
    "        # df[f'{feature}_front_lag_24h'] = df[feature].shift(-24*4)\n",
    "\n",
    "\n",
    "    # Handling edges by filling NaNs with appropriate values or dropping them\n",
    "    # You may choose to fill with zeroes or interpolate, based on what makes more sense for your data\n",
    "    df.fillna(method='ffill', inplace=True)  # Forward fill\n",
    "    df.fillna(method='bfill', inplace=True)  # Backward fill\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_lagged_features(df, column_name='pv_measurement'):\n",
    "    # Assuming 'date_forecast' is the datetime column used for sorting\n",
    "\n",
    "    df[f'{column_name}_prev_month'] = df[column_name].shift(24*7) # previous week\n",
    "\n",
    "    # For yearly lag, you would need to calculate the number of observations per year\n",
    "    # If the data is not consistent (leap years, etc.), you may need a more complex method\n",
    "    # Here's a simple version assuming 365 days a year:\n",
    "    df[f'{column_name}_prev_year'] = df[column_name].shift(24*365) # previous year\n",
    "    df[f'{column_name}_2years_ago'] = df[column_name].shift(24*365*2) # next year\n",
    "\n",
    "    # Handling edges by filling NaNs with appropriate values or dropping them\n",
    "    df.fillna(method='ffill', inplace=True)  # Forward fill\n",
    "    df.fillna(method='bfill', inplace=True)  # Backward fill\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_custom_fields(df):\n",
    "     df['hour_sin'] = np.sin(2 * np.pi * df['date_forecast'].dt.hour / 24)\n",
    "     df['hour_cos'] = np.cos(2 * np.pi * df['date_forecast'].dt.hour / 24)\n",
    "\n",
    "     df['month_sin'] = np.sin(2 * np.pi * df['date_forecast'].dt.month / 12)\n",
    "     df['month_cos'] = np.cos(2 * np.pi * df['date_forecast'].dt.month / 12)\n",
    "     df['day-of-year'] = df['date_forecast'].dt.dayofyear\n",
    "     return df\n",
    "\n",
    "def remove_outliers(df):\n",
    "    # Use a mask to filter out the rows where rolling std is zero but keep the rows where the value itself is zero\n",
    "    mask = (df['pv_measurement'].rolling(5).std() == 0) & (df['pv_measurement'] != 0)\n",
    "    df = df[~mask]\n",
    "    return df\n",
    "\n",
    "def resample_add_data(df, is_test_data):\n",
    "    df = add_custom_fields(df)\n",
    "    df.set_index('date_forecast', inplace=True)\n",
    "    df = df.resample('1H').mean()\n",
    "    # Remove empty dates if test data\n",
    "    if is_test_data:\n",
    "        non_nan_threshold = len(df.columns) // 2  \n",
    "        df.dropna(thresh=non_nan_threshold, inplace=True)\n",
    "    df.interpolate(method=\"linear\", inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_location_feature(X, location):\n",
    "      # Treat location as a categorical feature by converting it to a category type\n",
    "    X['location'] = location\n",
    "    X['location'] = X['location'].astype(str)  # Convert to string if 'location' is not an int\n",
    "    X['dew_or_rime:idx'] = X['dew_or_rime:idx'].astype(str)\n",
    "    X['is_day:idx'] = X['is_day:idx'].astype(str)\n",
    "    X['is_in_shadow:idx'] = X['is_in_shadow:idx'].astype(str)\n",
    "    categorical_columns = ['location', 'dew_or_rime:idx', 'is_day:idx', 'is_in_shadow:idx']\n",
    "\n",
    "    # Before filling NaN values, add 'missing' as a category for each categorical column.\n",
    "    for column in categorical_columns:\n",
    "        X[column] = X[column].astype('category')  # Ensure the column is of type 'category'.\n",
    "        if 'missing' not in X[column].cat.categories:\n",
    "            X[column] = X[column].cat.add_categories(['missing'])  # Add 'missing' as a new category.\n",
    "        X[column] = X[column].fillna('missing')\n",
    "    X['location'] = X['location'].astype('category')\n",
    "    X['dew_or_rime:idx'] = X['dew_or_rime:idx'].astype('category')\n",
    "    X['is_day:idx'] = X['is_day:idx'].astype('category')\n",
    "    X['is_in_shadow:idx'] = X['is_in_shadow:idx'].astype('category')\n",
    "    \n",
    "    return X\n",
    "\n",
    "def add_scaling(X_test, X_training, location):\n",
    "    global scalers\n",
    "    continuous_columns = X_training.select_dtypes(include=['float32', 'int32']).columns\n",
    "    if location not in scalers:\n",
    "        scalers[location] = MinMaxScaler()\n",
    "    X_training[continuous_columns] = scalers[location].fit_transform(X_training[continuous_columns])\n",
    "    \n",
    "    X_test[continuous_columns] = scalers[location].transform(X_test[continuous_columns])\n",
    "\n",
    "    return X_test, X_training\n",
    "\n",
    "def make_observed_and_estimated_category(df_observed, df_estimated, df_test):\n",
    "     # Hot encode in wether observed or estimated\n",
    "    df_observed['observed_or_estimated'] = 'observed'\n",
    "    df_estimated['observed_or_estimated'] = 'estimated'\n",
    "    df_test['observed_or_estimated'] = 'estimated'\n",
    "    # Concatenate observed and estimated\n",
    "    df_training = pd.concat([df_observed, df_estimated], axis=0).sort_values(by=\"date_forecast\")\n",
    "    df_training['observed_or_estimated'] = df_training['observed_or_estimated'].astype('category')\n",
    "    df_test['observed_or_estimated'] = df_test['observed_or_estimated'].astype('category')\n",
    "\n",
    "    return df_training, df_test\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize a dictionary to hold the scalers for each location\n",
    "\n",
    "scalers = {}\n",
    "\n",
    "\n",
    "def prepare_data(location):\n",
    "    # Load data\n",
    "    scaling = False  # Set scaling to True to enable individual scaling for each location\n",
    "    global scalers\n",
    "\n",
    "    # Load training data\n",
    "    df_observed = pd.read_parquet(f\"data/{location}/X_train_observed.parquet\")\n",
    "    df_estimated = pd.read_parquet(f\"data/{location}/X_train_estimated.parquet\")\n",
    "    df_target = pd.read_parquet(f\"data/{location}/train_targets.parquet\")\n",
    "\n",
    "   \n",
    "    # Load test data\n",
    "    df_test = pd.read_parquet(f\"data/{location}/X_test_estimated.parquet\")\n",
    "\n",
    "   \n",
    "    # Hot encode in wether observed or estimated\n",
    "\n",
    "    # Resample and add custom fields\n",
    "\n",
    "    df_observed = resample_add_data(df_observed, False)\n",
    "    df_estimated = resample_add_data(df_estimated, False)\n",
    "    df_test = resample_add_data(df_test, True)\n",
    "\n",
    "    df_training, df_test = make_observed_and_estimated_category(df_observed, df_estimated, df_test)\n",
    "\n",
    "    # Merge training with target data\n",
    "    df_training = pd.merge(df_training, df_target, left_on=\"date_forecast\", right_on=\"time\", how=\"inner\")\n",
    "    \n",
    "    # Create lagged features and remove outliers training\n",
    "    df_training = create_weather_lagged_features(df_training, WEATHER_FEATURES)\n",
    "    df_training = df_training[COLUMNS_TO_KEEP]\n",
    "    df_training = remove_outliers(df_training)\n",
    "\n",
    "    # Create lagged features test\n",
    "    df_test = create_weather_lagged_features(df_test, WEATHER_FEATURES)\n",
    "    df_test = df_test[TEST_COLUMNS_TO_KEEP]\n",
    "\n",
    "    # Make training x and y\n",
    "    y_training = df_training[\"pv_measurement\"]\n",
    "    X_training = df_training.drop(\"pv_measurement\", axis=1)\n",
    "\n",
    "    # Add categories\n",
    "    X_training = add_location_feature(X_training, location)\n",
    "    X_test = add_location_feature(df_test, location)\n",
    "    \n",
    "    # Add scaling\n",
    "    if scaling:\n",
    "        X_test, X_training = add_scaling(X_test, X_training, location)\n",
    "    X_test.reset_index(inplace=True)\n",
    "    X_test.drop(\"date_forecast\", axis=1, inplace=True)\n",
    "    # y_training = np.log1p(y_training)\n",
    "    return X_training, X_test, y_training\n",
    "\n",
    "\n",
    "\n",
    "# Use prepare_data function\n",
    "\n",
    "combined_X_train = pd.DataFrame()\n",
    "combined_X_val = pd.DataFrame()\n",
    "combined_Y_train = pd.DataFrame()\n",
    "combined_Y_val = pd.DataFrame()\n",
    "combined_X_test = pd.DataFrame()\n",
    "\n",
    "for location in locations:\n",
    "    # Prepare the training data\n",
    "    X_training, X_test, y_training = prepare_data(location)\n",
    "    \n",
    "    # Split and concatenate the training data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_training, y_training, test_size=0.0001, random_state=42)\n",
    "    combined_X_train = pd.concat([combined_X_train, X_train])\n",
    "    combined_X_val = pd.concat([combined_X_val, X_val])\n",
    "    combined_Y_train = pd.concat([combined_Y_train, y_train])\n",
    "    combined_Y_val = pd.concat([combined_Y_val, y_val])\n",
    "\n",
    "    combined_X_test = pd.concat([combined_X_test, X_test])\n",
    "\n",
    "    # Spl\n",
    "\n",
    "combined_X_test\n",
    "combined_X_train, combined_Y_train = shuffle(combined_X_train, combined_Y_train, random_state=42)\n",
    "combined_X_val, combined_Y_val = shuffle(combined_X_val, combined_Y_val, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 291.1747725\ttest: 748.0749860\tbest: 748.0749860 (0)\ttotal: 147ms\tremaining: 14m 40s\n",
      "200:\tlearn: 120.4355877\ttest: 340.3530055\tbest: 340.3530055 (200)\ttotal: 20.1s\tremaining: 9m 38s\n",
      "400:\tlearn: 87.9791543\ttest: 258.5880338\tbest: 258.4648335 (399)\ttotal: 39.7s\tremaining: 9m 14s\n",
      "600:\tlearn: 77.9862214\ttest: 231.5580906\tbest: 231.5580906 (600)\ttotal: 59.8s\tremaining: 8m 57s\n",
      "800:\tlearn: 73.9952100\ttest: 219.2543227\tbest: 219.2543227 (800)\ttotal: 1m 18s\tremaining: 8m 31s\n",
      "1000:\tlearn: 70.9904915\ttest: 214.3781165\tbest: 214.3781165 (1000)\ttotal: 1m 37s\tremaining: 8m 8s\n",
      "1200:\tlearn: 69.7845306\ttest: 214.7317033\tbest: 214.0832978 (1128)\ttotal: 1m 58s\tremaining: 7m 53s\n",
      "1400:\tlearn: 68.8523475\ttest: 214.5354891\tbest: 214.0832978 (1128)\ttotal: 2m 18s\tremaining: 7m 33s\n",
      "1600:\tlearn: 67.5184148\ttest: 213.3025232\tbest: 213.2961239 (1598)\ttotal: 2m 36s\tremaining: 7m 10s\n",
      "1800:\tlearn: 65.9367814\ttest: 212.6226062\tbest: 212.5849724 (1799)\ttotal: 2m 55s\tremaining: 6m 48s\n",
      "2000:\tlearn: 64.5477038\ttest: 210.6184980\tbest: 210.5779418 (1984)\ttotal: 3m 14s\tremaining: 6m 28s\n",
      "2200:\tlearn: 63.5823349\ttest: 210.5102013\tbest: 210.4371593 (2171)\ttotal: 3m 33s\tremaining: 6m 8s\n",
      "2400:\tlearn: 62.4426302\ttest: 210.0582024\tbest: 210.0259646 (2396)\ttotal: 3m 52s\tremaining: 5m 48s\n",
      "2600:\tlearn: 61.2460544\ttest: 209.0466412\tbest: 209.0346514 (2599)\ttotal: 4m 11s\tremaining: 5m 28s\n",
      "2800:\tlearn: 60.3771031\ttest: 208.4823475\tbest: 208.3892060 (2681)\ttotal: 4m 30s\tremaining: 5m 9s\n",
      "3000:\tlearn: 59.2792646\ttest: 208.5747306\tbest: 208.3892060 (2681)\ttotal: 4m 50s\tremaining: 4m 50s\n",
      "3200:\tlearn: 58.1997882\ttest: 207.5298793\tbest: 207.5208626 (3197)\ttotal: 5m 10s\tremaining: 4m 31s\n",
      "3400:\tlearn: 57.1938073\ttest: 207.3101884\tbest: 206.9104722 (3348)\ttotal: 5m 28s\tremaining: 4m 11s\n",
      "3600:\tlearn: 56.0766838\ttest: 205.8996209\tbest: 205.8763439 (3596)\ttotal: 5m 45s\tremaining: 3m 50s\n",
      "3800:\tlearn: 55.0455358\ttest: 204.6698232\tbest: 204.3385891 (3796)\ttotal: 6m 2s\tremaining: 3m 29s\n",
      "4000:\tlearn: 54.1956812\ttest: 204.9475649\tbest: 204.3385891 (3796)\ttotal: 6m 19s\tremaining: 3m 9s\n",
      "4200:\tlearn: 53.3266474\ttest: 204.2184840\tbest: 204.1772173 (4177)\ttotal: 6m 36s\tremaining: 2m 49s\n",
      "4400:\tlearn: 52.5834212\ttest: 203.6567340\tbest: 203.6356193 (4356)\ttotal: 6m 53s\tremaining: 2m 30s\n",
      "4600:\tlearn: 51.8641305\ttest: 203.6481464\tbest: 203.0538063 (4536)\ttotal: 7m 10s\tremaining: 2m 10s\n",
      "4800:\tlearn: 51.2328246\ttest: 203.5731603\tbest: 203.0538063 (4536)\ttotal: 7m 27s\tremaining: 1m 51s\n",
      "5000:\tlearn: 50.5869984\ttest: 203.4510246\tbest: 203.0538063 (4536)\ttotal: 7m 44s\tremaining: 1m 32s\n",
      "5200:\tlearn: 50.2631691\ttest: 203.2469832\tbest: 203.0538063 (4536)\ttotal: 8m 1s\tremaining: 1m 14s\n",
      "5400:\tlearn: 50.0617474\ttest: 203.2767836\tbest: 203.0538063 (4536)\ttotal: 8m 18s\tremaining: 55.3s\n",
      "5600:\tlearn: 49.8364993\ttest: 203.0269690\tbest: 202.9809362 (5505)\ttotal: 8m 35s\tremaining: 36.8s\n",
      "5800:\tlearn: 49.6145011\ttest: 203.1481166\tbest: 202.9204233 (5721)\ttotal: 8m 53s\tremaining: 18.3s\n",
      "5999:\tlearn: 49.3233244\ttest: 203.4878996\tbest: 202.9204233 (5721)\ttotal: 9m 10s\tremaining: 0us\n",
      "\n",
      "bestTest = 202.9204233\n",
      "bestIteration = 5721\n",
      "\n",
      "Location C, Mean Absolute Error: 10.931930181606651\n",
      "Location A, Mean Absolute Error: 496.5297191197997\n",
      "Location B, Mean Absolute Error: 5.321513121540725\n"
     ]
    }
   ],
   "source": [
    "def train_model(X_train, y_train, X_val, y_val, location):\n",
    "    cat_features = [index for index, col in enumerate(X_train.columns) if col in ['location', 'dew_or_rime:idx', 'is_day:idx', 'is_in_shadow:idx', 'observed_or_estimated']]\n",
    "    \n",
    "    model = CatBoostRegressor(\n",
    "        iterations=6000,\n",
    "        learning_rate=0.007,\n",
    "        depth=12,  # assuming you decided to keep the depth reduced\n",
    "        loss_function='MAE',\n",
    "        verbose=200,\n",
    "        cat_features=cat_features,\n",
    "    )\n",
    "\n",
    "    # Use the provided validation set for early stopping\n",
    "    \n",
    "    model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=False)\n",
    "    model.save_model(f\"catboost_model_merged.cbm\")\n",
    "\n",
    "def evaluate_model(X_val, y_val, location):\n",
    "    model = CatBoostRegressor()\n",
    "    model.load_model(f\"catboost_model_merged.cbm\")\n",
    "    \n",
    "    # Unique locations in the X_val dataframe\n",
    "    locations = X_val['location'].unique()\n",
    "    \n",
    "    for location in locations:\n",
    "        # Filter X_val and y_val for the current location\n",
    "        X_val_loc = X_val[X_val['location'] == location]\n",
    "        y_val_loc = y_val[X_val['location'] == location]\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred_loc = model.predict(X_val_loc)\n",
    "        \n",
    "        # Apply the transformation if needed (assuming y_val_loc and y_pred_loc are log1p transformed)\n",
    "        # y_val_loc = np.expm1(y_val_loc)\n",
    "        # y_pred_loc = np.expm1(y_pred_loc)\n",
    "        \n",
    "        # Calculate MAE for the current location\n",
    "        mae = mean_absolute_error(y_val_loc, y_pred_loc)\n",
    "        print(f'Location {location}, Mean Absolute Error: {mae}')\n",
    "\n",
    "\n",
    "# Train the model using all available training data and the initial validation set for early stopping\n",
    "train_model(combined_X_train, combined_Y_train, combined_X_val, combined_Y_val, location)\n",
    "# Evaluate the model using the same validation set\n",
    "evaluate_model(combined_X_val, combined_Y_val, location)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming you have defined WEATHER_FEATURES, TEST_COLUMNS_TO_KEEP, and other functions previously\n",
    "\n",
    "def make_predictions(df_test_pred):\n",
    "    \n",
    "    # Load model \n",
    "    model = CatBoostRegressor()\n",
    "    model.load_model(f\"catboost_model_merged.cbm\")\n",
    "\n",
    "    preds = model.predict(df_test_pred)\n",
    "    \n",
    "    # Inverse transform the predictions\n",
    "    # preds = np.expm1(preds)\n",
    "    return preds\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate locally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "make_predictions() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/simenseeberg/Documents/ntnu/h23/tdt4173/project/TDT4173-project/catboost_model_merged_vol3.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simenseeberg/Documents/ntnu/h23/tdt4173/project/TDT4173-project/catboost_model_merged_vol3.ipynb#Y125sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simenseeberg/Documents/ntnu/h23/tdt4173/project/TDT4173-project/catboost_model_merged_vol3.ipynb#Y125sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mfor\u001b[39;00m loc \u001b[39min\u001b[39;00m locations:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/simenseeberg/Documents/ntnu/h23/tdt4173/project/TDT4173-project/catboost_model_merged_vol3.ipynb#Y125sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     evaluate_model_locally(loc, scalers)\n",
      "\u001b[1;32m/Users/simenseeberg/Documents/ntnu/h23/tdt4173/project/TDT4173-project/catboost_model_merged_vol3.ipynb Cell 12\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/simenseeberg/Documents/ntnu/h23/tdt4173/project/TDT4173-project/catboost_model_merged_vol3.ipynb#Y125sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m target_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_parquet(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdata/\u001b[39m\u001b[39m{\u001b[39;00mlocation\u001b[39m}\u001b[39;00m\u001b[39m/train_targets.parquet\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/simenseeberg/Documents/ntnu/h23/tdt4173/project/TDT4173-project/catboost_model_merged_vol3.ipynb#Y125sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Make predictions\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/simenseeberg/Documents/ntnu/h23/tdt4173/project/TDT4173-project/catboost_model_merged_vol3.ipynb#Y125sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m preds \u001b[39m=\u001b[39m make_predictions(X_training, location)[\u001b[39m-\u001b[39m\u001b[39m720\u001b[39m:]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simenseeberg/Documents/ntnu/h23/tdt4173/project/TDT4173-project/catboost_model_merged_vol3.ipynb#Y125sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m target \u001b[39m=\u001b[39m target_df\u001b[39m.\u001b[39mtail(\u001b[39m720\u001b[39m)[\u001b[39m\"\u001b[39m\u001b[39mpv_measurement\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto_numpy()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simenseeberg/Documents/ntnu/h23/tdt4173/project/TDT4173-project/catboost_model_merged_vol3.ipynb#Y125sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m differences \u001b[39m=\u001b[39m preds \u001b[39m-\u001b[39m target\n",
      "\u001b[0;31mTypeError\u001b[0m: make_predictions() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def evaluate_model_locally(location, scalers):\n",
    "    # Load the test data\n",
    "    target_df = pd.read_parquet(f\"data/{location}/train_targets.parquet\")\n",
    "    \n",
    "    # Make predictions\n",
    "    preds = make_predictions(X_training)[-720:]\n",
    "    target = target_df.tail(720)[\"pv_measurement\"].to_numpy()\n",
    "    \n",
    "    differences = preds - target\n",
    "    # Count predictions lower than the actual\n",
    "    lower_predictions = (differences < 0) & (target != 0)\n",
    "    # Count predictions higher than the actual\n",
    "    higher_predictions = (differences > 0) & (target != 0)\n",
    "\n",
    "    # Biggest misreads\n",
    "    absolute_differences = abs(differences)\n",
    "    max_diff_index = absolute_differences.argmax()  # Index of the biggest difference\n",
    "    max_diff_value = absolute_differences[max_diff_index]  # Value of the biggest difference\n",
    "    print(f\"Number of predictions that are a lower value than the actual, given that the actual is not 0: {lower_predictions.sum()}\")\n",
    "    print(f\"Number of predictions that are larger than the target, given that the target is not 0: {higher_predictions.sum()}\")\n",
    "    print(f\"The biggest misread is at index {max_diff_index} with a difference of {max_diff_value}\")\n",
    "    \n",
    "    index = target_df.index[-720:]\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(60,6))\n",
    "    plt.plot(index, target, label=\"Target\")\n",
    "    plt.plot(index, preds, label=\"Predictions\")\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Target vs Predictions')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "    plt.show()\n",
    "\n",
    "for loc in locations:\n",
    "    evaluate_model_locally(loc, scalers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit to csv 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160\n",
      "2160\n"
     ]
    }
   ],
   "source": [
    "df_submission = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "preds = make_predictions(combined_X_test)\n",
    "print(len(preds))\n",
    "print(len(df_submission))\n",
    "df_submission[\"prediction\"] = preds\n",
    "\n",
    "# Save the results to a new submission file\n",
    "df_submission[[\"id\", \"prediction\"]].to_csv(\"kaggle_submission_catboost_10.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
