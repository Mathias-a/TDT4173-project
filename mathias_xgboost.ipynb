{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CUSTOM_COLUMNS_TO_KEEP = [\n",
    "    # \"hour_cos\",\n",
    "    # \"month_sin\",\n",
    "    # \"hour_sin\",\n",
    "    # \"month_cos\",\n",
    "]\n",
    "\n",
    "COLUMNS_TO_KEEP = [\n",
    "    \"direct_rad:W\",\n",
    "    \"clear_sky_rad:W\",\n",
    "    \"diffuse_rad:W\",\n",
    "    \"direct_rad_1h:J\",\n",
    "    \"is_in_shadow:idx\",\n",
    "    \"clear_sky_energy_1h:J\",\n",
    "    \"diffuse_rad_1h:J\",\n",
    "    \"is_day:idx\",\n",
    "    \"sun_elevation:d\",\n",
    "    \"ceiling_height_agl:m\",\n",
    "    \"effective_cloud_cover:p\",\n",
    "    \"visibility:m\",\n",
    "    \"total_cloud_cover:p\",\n",
    "    \"air_density_2m:kgm3\",\n",
    "    \"wind_speed_v_10m:ms\",\n",
    "    \"dew_point_2m:K\",\n",
    "    \"wind_speed_u_10m:ms\",\n",
    "    \"t_1000hPa:K\",\n",
    "    \"absolute_humidity_2m:gm3\",\n",
    "     \"snow_water:kgm2\",\n",
    "    \"relative_humidity_1000hPa:p\",\n",
    "    \"fresh_snow_24h:cm\",\n",
    "    \"cloud_base_agl:m\",\n",
    "    \"fresh_snow_12h:cm\",\n",
    "    \"snow_depth:cm\",\n",
    "    \"dew_or_rime:idx\",\n",
    "    \"fresh_snow_6h:cm\",\n",
    "    \"super_cooled_liquid_water:kgm2\",\n",
    "    \"fresh_snow_3h:cm\",\n",
    "    \"rain_water:kgm2\",\n",
    "    \"precip_type_5min:idx\",\n",
    "    \"precip_5min:mm\",\n",
    "    \"fresh_snow_1h:cm\",\n",
    "    \"sun_azimuth:d\",\n",
    "    \"msl_pressure:hPa\",\n",
    "    \"pressure_100m:hPa\",\n",
    "    \"pressure_50m:hPa\",\n",
    "    \"sfc_pressure:hPa\",\n",
    "    \"prob_rime:p\",\n",
    "    \"wind_speed_10m:ms\",\n",
    "    \"elevation:m\",\n",
    "    \"snow_density:kgm3\",\n",
    "    \"snow_drift:idx\",\n",
    "    \"snow_melt_10min:mm\",\n",
    "    \"wind_speed_w_1000hPa:ms\",\n",
    "    # \"date_calc\",\n",
    "    \"pv_measurement\",\n",
    "] + CUSTOM_COLUMNS_TO_KEEP\n",
    "\n",
    "LOCATION = \"A\"\n",
    "lag_features = [\"pv_measurement\", \"direct_rad:W\"]\n",
    "SHIFTS = [1, 2, 3, 24]\n",
    "MODEL_FILENAME = f'models/xgboost_model_{LOCATION}.json'\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load data\n",
    "df_observed = pd.read_parquet(f\"data/{LOCATION}/X_train_observed.parquet\")\n",
    "df_estimated = pd.read_parquet(f\"data/{LOCATION}/X_train_estimated.parquet\")\n",
    "df_target = pd.read_parquet(f\"data/{LOCATION}/train_targets.parquet\")\n",
    "\n",
    "# 2. Combine observed and estimated datasets\n",
    "df_combined = pd.concat([df_observed, df_estimated], axis=0).sort_values(\n",
    "    by=\"date_forecast\"\n",
    ")\n",
    "\n",
    "# 3. Merge with target data\n",
    "df_merged = pd.merge(\n",
    "    df_combined, df_target, left_on=\"date_forecast\", right_on=\"time\", how=\"inner\"\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Downsampling and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling the dataframe to hourly intervals\n",
    "df_merged = df_merged.resample(\"H\", on=\"date_forecast\").mean()\n",
    "\n",
    "# Keep only relevant columns\n",
    "df_merged = df_merged[COLUMNS_TO_KEEP]\n",
    "\n",
    "# 4. Extract features and target\n",
    "df_merged = df_merged.dropna(subset=[\"pv_measurement\"])\n",
    "df_merged.fillna(0, inplace=True)  # Fill NaN values\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Add lagged features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lagged_features(df, features, shift):\n",
    "    \"\"\"\n",
    "    This function takes in a dataframe, a list of features, and a shift interval.\n",
    "    It returns the dataframe with the lagged features added.\n",
    "    \"\"\"\n",
    "    for feature in features:\n",
    "        if feature != \"pv_measurement\":\n",
    "            df[f\"{feature}_lagged_{shift}h\"] = df[feature].shift(shift)\n",
    "    return df\n",
    "\n",
    "\n",
    "for shift in SHIFTS:\n",
    "    df_merged = add_lagged_features(df_merged, COLUMNS_TO_KEEP, shift)\n",
    "\n",
    "# Remember to drop NaN values introduced by shifting\n",
    "df_merged.dropna(inplace=True)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "def remove_outliers(df):\n",
    "    \"\"\"\n",
    "    Removes outliers in a dataframe based on IQR for each column.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): input dataframe\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: dataframe with outliers removed\n",
    "    \"\"\"\n",
    "    Q1 = df.quantile(0.05)\n",
    "    Q3 = df.quantile(0.95)\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_condition = ~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR)))\n",
    "    return df[outlier_condition].dropna()\n",
    "\n",
    "\n",
    "# Remove outliers from the merged dataset\n",
    "# df_merged = remove_outliers(df_merged)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Split Data into Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "y = df_merged[\"pv_measurement\"]\n",
    "X = df_merged.drop(\"pv_measurement\", axis=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=False\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def naive_forecast(time_series, steps=1):\n",
    "    \"\"\"\n",
    "    This function returns the time_series shifted by a given number of steps.\n",
    "    For our purpose, we'll use 24-hour shift.\n",
    "    \"\"\"\n",
    "    return time_series.shift(steps)\n",
    "\n",
    "\n",
    "# Calculate the predictions using naive forecast\n",
    "baseline_predictions = naive_forecast(y_train, steps=24)\n",
    "\n",
    "# Only consider the predictions where both the actual and predicted values are available\n",
    "mask = (~baseline_predictions.isna()) & (~y_train.isna())\n",
    "\n",
    "# Calculate MAE\n",
    "baseline_mae = np.mean(np.abs(baseline_predictions[mask] - y_train[mask]))\n",
    "print(f\"Baseline MAE: {baseline_mae}\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert data to DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "# Modified XGBoost parameters to prevent overfitting\n",
    "xgb_params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"mae\",\n",
    "    \"eta\": 0.0005,  # Reduced learning rate\n",
    "    \"max_depth\": 7,  # Reduced tree depth\n",
    "    \"subsample\": 0.7,  # Reduced subsampling\n",
    "    \"colsample_bytree\": 0.8,  # Reduced column sampling\n",
    "    \"min_child_weight\": 7,  # Increased min_child_weight\n",
    "    \"alpha\": 0.4,  # L1 regularization\n",
    "    \"lambda\": 1,  # L2 regularization\n",
    "}\n",
    "\n",
    "# Train XGBoost model with modifications\n",
    "num_rounds = 50000  # Increased boosting rounds due to reduced eta\n",
    "xgb_model = xgb.train(\n",
    "    params=xgb_params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=num_rounds,\n",
    "    evals=[(dtrain, \"train\"), (dval, \"eval\")],\n",
    "    early_stopping_rounds=200,  # Increased early stopping rounds\n",
    "    verbose_eval=10,\n",
    ")\n",
    "xgb_model.save_model(MODEL_FILENAME)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # XGBoost Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xgb_loaded_model = xgb.Booster()\n",
    "xgb_loaded_model.load_model(MODEL_FILENAME)\n",
    "\n",
    "# Predict with XGBoost\n",
    "y_pred_xgb = xgb_loaded_model.predict(dval, iteration_range=(0, xgb_loaded_model.best_iteration + 1))\n",
    "\n",
    "# Calculate MAE for XGBoost\n",
    "xgb_mae = np.mean(np.abs(y_pred_xgb - y_val))\n",
    "print(f\"XGBoost MAE: {xgb_mae}\")\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_val.reset_index(drop=True), label=\"Actual\", color='blue')\n",
    "plt.plot(y_pred_xgb, label=\"Predicted\", color='red')\n",
    "plt.title(\"XGBoost Predictions vs Actuals\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "xgb.plot_importance(xgb_loaded_model)\n",
    "plt.show()\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # XGBoost finding opptimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test_data(df, columns_to_keep, shifts):\n",
    "    # Ensure 'pv_measurement' is not in columns_to_keep for test data processing\n",
    "    if 'pv_measurement' in columns_to_keep:\n",
    "        columns_to_keep.remove('pv_measurement')\n",
    "\n",
    "    # Ensure the index is a datetime\n",
    "    df['date_forecast'] = pd.to_datetime(df['date_forecast'])\n",
    "    df.set_index('date_forecast', inplace=True)\n",
    "    # Resample to 1-hour intervals\n",
    "    df = df.resample('1H').mean()\n",
    "    df = df.dropna(how='all').reset_index(drop=True)\n",
    "    # Keep only the columns used during training (minus the target column)\n",
    "    df = df[columns_to_keep]\n",
    "    # Add lagged features\n",
    "    for shift in shifts:\n",
    "        df = add_lagged_features(df, columns_to_keep, shift)\n",
    "    # Drop NaN values introduced by shifting\n",
    "    df.dropna(inplace=True)\n",
    "    # Return processed dataframe\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Read the Kaggle test.csv to get the location and ids\n",
    "df_submission = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "locations = [\"A\", \"B\", \"C\"]\n",
    "\n",
    "# Create a list to store our final predictions and IDs\n",
    "final_predictions = []\n",
    "final_ids = []\n",
    "\n",
    "for loc in locations:\n",
    "    print(f\"Processing location: {loc}\")\n",
    "    # Load forecasted weather data for testing for the current location\n",
    "    df_loc = pd.read_parquet(f\"data/{loc}/X_test_estimated.parquet\")\n",
    "    df_loc_processed = preprocess_test_data(df_loc, COLUMNS_TO_KEEP, SHIFTS)\n",
    "    # Convert data to DMatrix for XGBoost\n",
    "    dtest = xgb.DMatrix(df_loc_processed)\n",
    "    # Predict using XGBoost model\n",
    "    preds = xgb_model.predict(dtest, iteration_range=(0, xgb_model.best_iteration + 1))\n",
    "    final_predictions.extend(preds)\n",
    "    final_ids.extend(df_submission[df_submission[\"location\"] == loc][\"id\"].values)\n",
    "\n",
    "# Create a DataFrame for the final predictions and save to CSV\n",
    "df_final_submission = pd.DataFrame({\n",
    "    \"id\": final_ids,\n",
    "    \"prediction\": final_predictions\n",
    "})\n",
    "\n",
    "# Save the results to a new submission file\n",
    "df_final_submission.to_csv(\"xgboost_kaggle_submission.csv\", index=False)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter space\n",
    "def optimize_xgb():\n",
    "    param_grid = {\n",
    "        \"objective\": [\"reg:squarederror\"],\n",
    "        \"eval_metric\": [\"mae\"],\n",
    "        \"eta\": [\n",
    "            0.001,  # Best value I\n",
    "            0.005,\n",
    "            0.01,\n",
    "            # 0.05,\n",
    "            # 0.1,\n",
    "            # 0.3,\n",
    "        ],\n",
    "        \"max_depth\": [\n",
    "            # 3,\n",
    "            # 4,\n",
    "            # 5,\n",
    "            6,\n",
    "            7,  # Best value II\n",
    "            8,\n",
    "            # 9,\n",
    "            # 10,\n",
    "            # 12,\n",
    "        ],\n",
    "        \"subsample\": [\n",
    "            # 0.3,\n",
    "            # 0.4,\n",
    "            # 0.5,\n",
    "            # 0.6,\n",
    "            0.7,  # Best value II\n",
    "            0.8,  # Best value II\n",
    "            0.9,\n",
    "            # 1.0,\n",
    "        ],\n",
    "        \"colsample_bytree\": [\n",
    "            # 0.3,\n",
    "            # 0.4,\n",
    "            # 0.5,\n",
    "            # 0.6,\n",
    "            0.7,\n",
    "            0.8,  # Best value II\n",
    "            0.9,  # Best value II\n",
    "            # 1.0,\n",
    "        ],\n",
    "        \"min_child_weight\": [\n",
    "            # 1,\n",
    "            # 2,\n",
    "            # 3,\n",
    "            # 4,\n",
    "            # 5,\n",
    "            6,\n",
    "            7,  # Best value III\n",
    "            8,  # Best value I\n",
    "            # 9,\n",
    "            # 10,\n",
    "            # 11,\n",
    "            # 12,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Initialize XGBoost Regressor\n",
    "    xgb_optimization_model = xgb.XGBModel(\n",
    "        learning_rate=0.02,\n",
    "        n_estimators=600,\n",
    "        objective=\"reg:squarederror\",\n",
    "        silent=True,\n",
    "        nthread=1,\n",
    "    )\n",
    "\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        xgb_optimization_model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=\"neg_mean_absolute_error\",\n",
    "        cv=None,\n",
    "        verbose=0,  # make it silent\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # Fit the model with early stopping rounds and validation data\n",
    "    fit_params = {\n",
    "        \"early_stopping_rounds\": 50,\n",
    "        \"eval_set\": [(X_val, y_val)],\n",
    "        \"verbose\": False,\n",
    "    }\n",
    "\n",
    "    grid_search.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "    # Print best parameters\n",
    "    print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "    # Predict on validation set\n",
    "    y_pred_optimized = grid_search.predict(X_val)\n",
    "\n",
    "    # Calculate MAE for optimized XGBoost\n",
    "    mae_optimized = np.mean(np.abs(y_pred_optimized - y_val))\n",
    "    print(f\"Optimized XGBoost MAE: {mae_optimized}\")\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 }
}