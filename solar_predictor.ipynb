{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_TO_KEEP = [\n",
    "    \"direct_rad:W\",\n",
    "    \"clear_sky_rad:W\",\n",
    "    \"diffuse_rad:W\",\n",
    "    \"direct_rad_1h:J\",\n",
    "    \"is_in_shadow:idx\",\n",
    "    \"clear_sky_energy_1h:J\",\n",
    "    \"diffuse_rad_1h:J\",\n",
    "    \"is_day:idx\",\n",
    "    \"sun_elevation:d\",\n",
    "    \"ceiling_height_agl:m\",\n",
    "    \"effective_cloud_cover:p\",\n",
    "    \"visibility:m\",\n",
    "    'total_cloud_cover:p',\n",
    "    'air_density_2m:kgm3',\n",
    "    'wind_speed_v_10m:ms',\n",
    "    'dew_point_2m:K',\n",
    "    'wind_speed_u_10m:ms',\n",
    "    't_1000hPa:K',\n",
    "    'absolute_humidity_2m:gm3',\n",
    "    'snow_water:kgm2',\n",
    "    'relative_humidity_1000hPa:p',\n",
    "    'fresh_snow_24h:cm',\n",
    "    'cloud_base_agl:m',\n",
    "    'fresh_snow_12h:cm',\n",
    "    'snow_depth:cm',\n",
    "    'dew_or_rime:idx',\n",
    "    'fresh_snow_6h:cm',\n",
    "    'super_cooled_liquid_water:kgm2',\n",
    "    'fresh_snow_3h:cm',\n",
    "    'rain_water:kgm2',\n",
    "    'precip_type_5min:idx',\n",
    "    'precip_5min:mm',\n",
    "    'fresh_snow_1h:cm',\n",
    "    'sun_azimuth:d',\n",
    "    'msl_pressure:hPa',\n",
    "    'pressure_100m:hPa',\n",
    "    'pressure_50m:hPa',\n",
    "    'sfc_pressure:hPa',\n",
    "    'prob_rime:p',\n",
    "    'wind_speed_10m:ms',\n",
    "    'elevation:m',\n",
    "    'snow_density:kgm3',\n",
    "    'snow_drift:idx',\n",
    "    'snow_melt_10min:mm',\n",
    "    'wind_speed_w_1000hPa:ms',\n",
    "    # \"date_calc\", something wrong with this column\n",
    "    \"pv_measurement\",\n",
    "]\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "NUM_FEATURES = len(COLUMNS_TO_KEEP) - 1  # -1 because pv_measurement is the target\n",
    "FEATURE_SIZE = 4  # 7 days of hourly data\n",
    "WEIGHT_DECAY = 0.01\n",
    "SEQUENCE_LENGTH = 14*24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, sequence_length):\n",
    "    \"\"\"\n",
    "    Converts time series data into overlapping sequences/windows.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    target_length = 1\n",
    "    for i in range(len(data) - sequence_length - target_length + 1):\n",
    "        seq = data[i : i + sequence_length]\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)\n",
    "\n",
    "class SolarPredictionNet(nn.Module):\n",
    "    def __init__(self, sequence_length, num_channels):\n",
    "        super(SolarPredictionNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(num_channels, 32, kernel_size=sequence_length, stride=1)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=1, stride=1)\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Parquet files\n",
    "df_data = pd.read_parquet(\"data/B/X_train_observed.parquet\")\n",
    "df_target = pd.read_parquet(\"data/B/train_targets.parquet\")\n",
    "\n",
    "# Merge the datasets\n",
    "df_merged = pd.merge(\n",
    "    df_data, df_target, left_on=\"date_forecast\", right_on=\"time\", how=\"inner\"\n",
    ")\n",
    "\n",
    "# Downsampling the dataframe to hourly intervals\n",
    "df_merged = df_merged.resample('H', on=\"date_forecast\").mean()\n",
    "\n",
    "df_merged = df_merged[COLUMNS_TO_KEEP]\n",
    "\n",
    "# Set all NaN values to 0\n",
    "df_merged.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "y = df_merged[\"pv_measurement\"]\n",
    "X = df_merged.drop(\"pv_measurement\", axis=1)\n",
    "\n",
    "# Convert dataframes to sequences\n",
    "X_sequences = create_sequences(X.values, SEQUENCE_LENGTH)\n",
    "# Adjust the sequence creation for y\n",
    "y_sequences = y.values[SEQUENCE_LENGTH-1:-1]  # Aligned with the end of each sequence and remove the last element\n",
    "\n",
    "# Sequential Split\n",
    "train_size = int(0.8 * len(X_sequences))\n",
    "X_train, X_val = X_sequences[:train_size], X_sequences[train_size:]\n",
    "y_train, y_val = y_sequences[:train_size], y_sequences[train_size:]\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_val = scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).transpose(1, 2)  # Adjust shape to [batch, channels, sequence]\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).transpose(1, 2)  # Adjust shape to [batch, channels, sequence]\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "\n",
    "# Create a custom dataset\n",
    "class SolarDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# Create datasets for training and validation\n",
    "train_dataset = SolarDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = SolarDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ... [Neural Network and Training code from previous messages]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader):\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Average validation loss\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Training Loss: {loss.item()}, Validation Loss: {val_loss}\"\n",
    "        )\n",
    "\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 101.9988784790039, Validation Loss: 34.25285643849583\n",
      "Epoch 2/100, Training Loss: 90.08866119384766, Validation Loss: 33.40768072122286\n",
      "Epoch 3/100, Training Loss: 121.97625732421875, Validation Loss: 34.845036467671065\n",
      "Epoch 4/100, Training Loss: 131.37075805664062, Validation Loss: 34.34315785760502\n",
      "Epoch 5/100, Training Loss: 134.6566619873047, Validation Loss: 33.92464612052102\n",
      "Epoch 6/100, Training Loss: 139.3184814453125, Validation Loss: 34.459578983732676\n",
      "Epoch 7/100, Training Loss: 173.90777587890625, Validation Loss: 38.732122387839944\n",
      "Epoch 8/100, Training Loss: 183.73184204101562, Validation Loss: 39.253562171838006\n",
      "Epoch 9/100, Training Loss: 136.56338500976562, Validation Loss: 33.251044010496194\n",
      "Epoch 10/100, Training Loss: 150.40426635742188, Validation Loss: 40.194606494890444\n",
      "Epoch 11/100, Training Loss: 137.59530639648438, Validation Loss: 36.05945878553729\n",
      "Epoch 12/100, Training Loss: 131.32803344726562, Validation Loss: 45.366628578025335\n",
      "Epoch 13/100, Training Loss: 144.23622131347656, Validation Loss: 39.43823098591934\n",
      "Epoch 14/100, Training Loss: 177.74212646484375, Validation Loss: 53.84080929036808\n",
      "Epoch 15/100, Training Loss: 190.5567626953125, Validation Loss: 43.000036563879604\n",
      "Epoch 16/100, Training Loss: 184.00411987304688, Validation Loss: 42.29412150728254\n",
      "Epoch 17/100, Training Loss: 150.35882568359375, Validation Loss: 41.44817224399594\n",
      "Epoch 18/100, Training Loss: 170.19143676757812, Validation Loss: 40.47009578499511\n",
      "Epoch 19/100, Training Loss: 170.56036376953125, Validation Loss: 44.08219608126429\n",
      "Epoch 20/100, Training Loss: 134.85308837890625, Validation Loss: 52.75694640087818\n",
      "Epoch 21/100, Training Loss: 129.26437377929688, Validation Loss: 33.4295318262119\n",
      "Epoch 22/100, Training Loss: 120.89796447753906, Validation Loss: 42.527006278380505\n",
      "Epoch 23/100, Training Loss: 162.53297424316406, Validation Loss: 39.52110515266286\n",
      "Epoch 24/100, Training Loss: 94.66877746582031, Validation Loss: 37.221210385581415\n",
      "Epoch 25/100, Training Loss: 134.46157836914062, Validation Loss: 32.97962743520495\n",
      "Epoch 26/100, Training Loss: 94.19355773925781, Validation Loss: 36.207891096752036\n",
      "Epoch 27/100, Training Loss: 97.30210876464844, Validation Loss: 36.62879748607231\n",
      "Epoch 28/100, Training Loss: 202.75772094726562, Validation Loss: 33.02653507762511\n",
      "Epoch 29/100, Training Loss: 285.68896484375, Validation Loss: 40.805735857650454\n",
      "Epoch 30/100, Training Loss: 194.78952026367188, Validation Loss: 40.07571781190541\n",
      "Epoch 31/100, Training Loss: 214.1725616455078, Validation Loss: 35.0536705948469\n",
      "Epoch 32/100, Training Loss: 253.037841796875, Validation Loss: 44.40239882678017\n",
      "Epoch 33/100, Training Loss: 260.5244140625, Validation Loss: 68.48085285022917\n",
      "Epoch 34/100, Training Loss: 227.6367950439453, Validation Loss: 44.342527067710684\n",
      "Epoch 35/100, Training Loss: 260.3739929199219, Validation Loss: 41.68011841956315\n",
      "Epoch 36/100, Training Loss: 230.66708374023438, Validation Loss: 56.1087594902431\n",
      "Epoch 37/100, Training Loss: 167.33990478515625, Validation Loss: 47.37714991006918\n",
      "Epoch 38/100, Training Loss: 263.9566955566406, Validation Loss: 44.612817649622784\n",
      "Epoch 39/100, Training Loss: 147.1511688232422, Validation Loss: 38.619737255697665\n",
      "Epoch 40/100, Training Loss: 174.34994506835938, Validation Loss: 47.17299752348381\n",
      "Epoch 41/100, Training Loss: 166.26629638671875, Validation Loss: 42.41904364355122\n",
      "Epoch 42/100, Training Loss: 146.7158966064453, Validation Loss: 38.13645022926924\n",
      "Epoch 43/100, Training Loss: 307.0642395019531, Validation Loss: 44.60158885497785\n",
      "Epoch 44/100, Training Loss: 167.7068328857422, Validation Loss: 43.310914875152235\n",
      "Epoch 45/100, Training Loss: 384.5408935546875, Validation Loss: 44.77613377700449\n",
      "Epoch 46/100, Training Loss: 165.06105041503906, Validation Loss: 43.17475167584016\n",
      "Epoch 47/100, Training Loss: 300.4033203125, Validation Loss: 45.600433662136275\n",
      "Epoch 48/100, Training Loss: 196.99755859375, Validation Loss: 45.52106459750992\n",
      "Epoch 49/100, Training Loss: 387.13580322265625, Validation Loss: 43.82008354289062\n",
      "Epoch 50/100, Training Loss: 216.3711395263672, Validation Loss: 49.43066050145693\n",
      "Epoch 51/100, Training Loss: 391.89910888671875, Validation Loss: 50.92867486203573\n",
      "Epoch 52/100, Training Loss: 256.9886779785156, Validation Loss: 43.9208231559258\n",
      "Epoch 53/100, Training Loss: 243.7864227294922, Validation Loss: 46.39734104823981\n",
      "Epoch 54/100, Training Loss: 230.8120574951172, Validation Loss: 45.184062706715636\n"
     ]
    }
   ],
   "source": [
    "model = SolarPredictionNet(SEQUENCE_LENGTH, NUM_FEATURES)\n",
    "train_model(model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
