{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_TO_KEEP = [\n",
    "    \"direct_rad:W\",\n",
    "    \"clear_sky_rad:W\",\n",
    "    \"diffuse_rad:W\",\n",
    "    \"direct_rad_1h:J\",\n",
    "    \"is_in_shadow:idx\",\n",
    "    \"clear_sky_energy_1h:J\",\n",
    "    \"diffuse_rad_1h:J\",\n",
    "    \"is_day:idx\",\n",
    "    \"sun_elevation:d\",\n",
    "    # \"ceiling_height_agl:m\",\n",
    "    # \"effective_cloud_cover:p\",\n",
    "    # \"visibility:m\",\n",
    "    # \"date_calc\",\n",
    "    \"pv_measurement\",\n",
    "]\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 10\n",
    "NUM_FEATURES = len(COLUMNS_TO_KEEP) - 1  # -1 because pv_measurement is the target\n",
    "FEATURE_SIZE = 4  # 7 days of hourly data\n",
    "WEIGHT_DECAY = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolarPredictionNet(nn.Module):\n",
    "    def __init__(self, input_size, num_channels):\n",
    "        super(SolarPredictionNet, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv1d(num_channels, 32, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=1, stride=1, padding=0)\n",
    "        # self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        # Compute the output size after convolutions and pooling\n",
    "        out_size = input_size // 4  # Divided by 4 because of two pooling layers\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(out_size * 64, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        # x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        # x = self.pool(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Parquet files\n",
    "df_data = pd.read_parquet(\"data/A/X_train_observed.parquet\")\n",
    "df_target = pd.read_parquet(\"data/A/train_targets.parquet\")\n",
    "\n",
    "# Merge the datasets\n",
    "df_merged = pd.merge(\n",
    "    df_data, df_target, left_on=\"date_forecast\", right_on=\"time\", how=\"inner\"\n",
    ")\n",
    "df_merged = df_merged[COLUMNS_TO_KEEP]\n",
    "\n",
    "# Separate the features and targets\n",
    "y = df_merged[\"pv_measurement\"]\n",
    "X = df_merged.drop(\n",
    "    \"pv_measurement\", axis=1\n",
    ")  # Replace 'target_column_name' with the name of your target column\n",
    "\n",
    "\n",
    "# Create a custom dataset\n",
    "class SolarDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X.values, y.values, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(\n",
    "    2\n",
    ")  # [batch_size, channels, 1]\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).unsqueeze(2)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create datasets for training and validation\n",
    "train_dataset = SolarDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = SolarDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ... [Neural Network and Training code from previous messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader):\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Average validation loss\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Training Loss: {loss.item()}, Validation Loss: {val_loss}\"\n",
    "        )\n",
    "\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 71.20064544677734, Validation Loss: 298.2879333929582\n",
      "Epoch 2/100, Training Loss: 72.36201477050781, Validation Loss: 291.6570111605455\n",
      "Epoch 3/100, Training Loss: 67.1571044921875, Validation Loss: 282.69557085984485\n",
      "Epoch 4/100, Training Loss: 42.9172477722168, Validation Loss: 287.6387961183734\n",
      "Epoch 5/100, Training Loss: 83.09991455078125, Validation Loss: 292.15552679858223\n",
      "Epoch 6/100, Training Loss: 61.99330139160156, Validation Loss: 282.3620213370532\n",
      "Epoch 7/100, Training Loss: 74.4435043334961, Validation Loss: 289.86020496959236\n",
      "Epoch 8/100, Training Loss: 59.852500915527344, Validation Loss: 284.2400024948698\n",
      "Epoch 9/100, Training Loss: 75.8277816772461, Validation Loss: 280.56453491862777\n",
      "Epoch 10/100, Training Loss: 59.72639465332031, Validation Loss: 281.4918266595012\n",
      "Epoch 11/100, Training Loss: 75.80879974365234, Validation Loss: 287.5402664985721\n",
      "Epoch 12/100, Training Loss: 67.5129623413086, Validation Loss: 279.84710356121514\n",
      "Epoch 13/100, Training Loss: 66.92086029052734, Validation Loss: 285.17353347977405\n",
      "Epoch 14/100, Training Loss: 61.78990173339844, Validation Loss: 280.81877728985614\n",
      "Epoch 15/100, Training Loss: 68.96083068847656, Validation Loss: 297.3033816075887\n",
      "Epoch 16/100, Training Loss: 69.88605499267578, Validation Loss: 282.5460031329582\n",
      "Epoch 17/100, Training Loss: 66.771484375, Validation Loss: 283.2583634652674\n",
      "Epoch 18/100, Training Loss: 57.50382614135742, Validation Loss: 279.77277466283505\n",
      "Epoch 19/100, Training Loss: 53.42660903930664, Validation Loss: 279.9212472037434\n",
      "Epoch 20/100, Training Loss: 64.02754211425781, Validation Loss: 279.69433871985285\n",
      "Epoch 21/100, Training Loss: 63.53777313232422, Validation Loss: 282.23169126494565\n",
      "Epoch 22/100, Training Loss: 57.23481750488281, Validation Loss: 279.0180859585804\n",
      "Epoch 23/100, Training Loss: 77.49530029296875, Validation Loss: 280.4315691050456\n",
      "Epoch 24/100, Training Loss: 55.75416564941406, Validation Loss: 274.86363920298487\n",
      "Epoch 25/100, Training Loss: 83.38223266601562, Validation Loss: 277.60078370209897\n",
      "Epoch 26/100, Training Loss: 54.99749755859375, Validation Loss: 267.29197434303336\n",
      "Epoch 27/100, Training Loss: 58.2999267578125, Validation Loss: 280.0429293330671\n",
      "Epoch 28/100, Training Loss: 52.21221160888672, Validation Loss: 270.3066115860987\n",
      "Epoch 29/100, Training Loss: 71.41352081298828, Validation Loss: 289.00816925447\n",
      "Epoch 30/100, Training Loss: 37.683135986328125, Validation Loss: 288.54926014508465\n",
      "Epoch 31/100, Training Loss: 55.726707458496094, Validation Loss: 273.9707676890723\n",
      "Epoch 32/100, Training Loss: 44.5283203125, Validation Loss: 282.15040380786166\n",
      "Epoch 33/100, Training Loss: 52.82640838623047, Validation Loss: 263.1836102530611\n",
      "Epoch 34/100, Training Loss: 50.40855407714844, Validation Loss: 264.732627907586\n",
      "Epoch 35/100, Training Loss: 58.383628845214844, Validation Loss: 263.93091747335313\n",
      "Epoch 36/100, Training Loss: 34.549888610839844, Validation Loss: 260.73416820840805\n",
      "Epoch 37/100, Training Loss: 63.96484375, Validation Loss: 261.1158392605958\n",
      "Epoch 38/100, Training Loss: 53.90258026123047, Validation Loss: 269.22656637891777\n",
      "Epoch 39/100, Training Loss: 47.27642059326172, Validation Loss: 260.5304591880503\n",
      "Epoch 40/100, Training Loss: 45.086769104003906, Validation Loss: 266.4922460096854\n",
      "Epoch 41/100, Training Loss: 29.440675735473633, Validation Loss: 268.4903873991886\n",
      "Epoch 42/100, Training Loss: 32.7691650390625, Validation Loss: 258.48574407895404\n",
      "Epoch 43/100, Training Loss: 43.6246337890625, Validation Loss: 261.83572521514765\n",
      "Epoch 44/100, Training Loss: 45.76695251464844, Validation Loss: 262.7762103124901\n",
      "Epoch 45/100, Training Loss: 46.503173828125, Validation Loss: 260.95580884663747\n",
      "Epoch 46/100, Training Loss: 45.68946075439453, Validation Loss: 255.1174054402695\n",
      "Epoch 47/100, Training Loss: 9.387105941772461, Validation Loss: 255.03407791618145\n",
      "Epoch 48/100, Training Loss: 28.311594009399414, Validation Loss: 256.36375313736374\n",
      "Epoch 49/100, Training Loss: 38.55005645751953, Validation Loss: 260.0310117090591\n",
      "Epoch 50/100, Training Loss: 66.79312896728516, Validation Loss: 260.4364736196569\n",
      "Epoch 51/100, Training Loss: 51.702850341796875, Validation Loss: 274.5936443428399\n",
      "Epoch 52/100, Training Loss: 27.797636032104492, Validation Loss: 259.68338485438414\n",
      "Epoch 53/100, Training Loss: 13.61747932434082, Validation Loss: 247.06138395299814\n",
      "Epoch 54/100, Training Loss: 32.639137268066406, Validation Loss: 256.5633314718\n",
      "Epoch 55/100, Training Loss: 39.227752685546875, Validation Loss: 252.24801238939818\n",
      "Epoch 56/100, Training Loss: 76.20878601074219, Validation Loss: 269.58819619252625\n",
      "Epoch 57/100, Training Loss: 65.5853500366211, Validation Loss: 255.83650886811793\n",
      "Epoch 58/100, Training Loss: 27.294231414794922, Validation Loss: 256.45427887287207\n",
      "Epoch 59/100, Training Loss: 77.74378204345703, Validation Loss: 269.2344038639004\n",
      "Epoch 60/100, Training Loss: 60.569580078125, Validation Loss: 269.7013609770573\n",
      "Epoch 61/100, Training Loss: 54.106651306152344, Validation Loss: 248.68930844907408\n",
      "Epoch 62/100, Training Loss: 31.77361488342285, Validation Loss: 255.46996772489965\n",
      "Epoch 63/100, Training Loss: 75.75908660888672, Validation Loss: 282.5267737012118\n",
      "Epoch 64/100, Training Loss: 48.02014923095703, Validation Loss: 262.5751401310417\n",
      "Epoch 65/100, Training Loss: 40.28056335449219, Validation Loss: 246.00075219275575\n",
      "Epoch 66/100, Training Loss: 43.95508575439453, Validation Loss: 257.9134970160847\n",
      "Epoch 67/100, Training Loss: 12.261388778686523, Validation Loss: 248.78686602187878\n",
      "Epoch 68/100, Training Loss: 34.37556457519531, Validation Loss: 253.9984904989249\n",
      "Epoch 69/100, Training Loss: 57.61903381347656, Validation Loss: 294.1671969095866\n",
      "Epoch 70/100, Training Loss: 48.16743469238281, Validation Loss: 253.7359449437973\n",
      "Epoch 71/100, Training Loss: 49.06590270996094, Validation Loss: 255.02560541685983\n",
      "Epoch 72/100, Training Loss: 48.941162109375, Validation Loss: 247.8834644090447\n",
      "Epoch 73/100, Training Loss: 34.02641296386719, Validation Loss: 253.91644246249086\n",
      "Epoch 74/100, Training Loss: 31.112154006958008, Validation Loss: 257.01215375232374\n",
      "Epoch 75/100, Training Loss: 56.43345642089844, Validation Loss: 288.6542588372022\n",
      "Epoch 76/100, Training Loss: 31.284568786621094, Validation Loss: 242.3414688600033\n",
      "Epoch 77/100, Training Loss: 45.717857360839844, Validation Loss: 260.9250015034419\n",
      "Epoch 78/100, Training Loss: 30.557058334350586, Validation Loss: 248.0167924161712\n",
      "Epoch 79/100, Training Loss: 27.43475914001465, Validation Loss: 249.07613800912594\n",
      "Epoch 80/100, Training Loss: 35.48085021972656, Validation Loss: 253.95397018021606\n",
      "Epoch 81/100, Training Loss: 46.0211181640625, Validation Loss: 250.2241480065516\n",
      "Epoch 82/100, Training Loss: 31.107149124145508, Validation Loss: 242.4620743576525\n",
      "Epoch 83/100, Training Loss: 21.14914894104004, Validation Loss: 243.72587806608541\n",
      "Epoch 84/100, Training Loss: 54.795318603515625, Validation Loss: 244.46827971092378\n",
      "Epoch 85/100, Training Loss: 54.841270446777344, Validation Loss: 253.7453071087699\n",
      "Epoch 86/100, Training Loss: 28.17999839782715, Validation Loss: 245.7525286393535\n",
      "Epoch 87/100, Training Loss: 55.002357482910156, Validation Loss: 245.06877415790302\n",
      "Epoch 88/100, Training Loss: 53.82569885253906, Validation Loss: 259.6083400787328\n",
      "Epoch 89/100, Training Loss: 35.52278137207031, Validation Loss: 262.73297636381506\n",
      "Epoch 90/100, Training Loss: 29.2205867767334, Validation Loss: 241.69655086054945\n",
      "Epoch 91/100, Training Loss: 25.12272071838379, Validation Loss: 246.89552456281\n",
      "Epoch 92/100, Training Loss: 30.01588249206543, Validation Loss: 246.39530088644636\n",
      "Epoch 93/100, Training Loss: 17.475358963012695, Validation Loss: 251.50564115537136\n",
      "Epoch 94/100, Training Loss: 30.9525203704834, Validation Loss: 250.91950031964467\n",
      "Epoch 95/100, Training Loss: 47.512489318847656, Validation Loss: 253.61332463736485\n",
      "Epoch 96/100, Training Loss: 38.276695251464844, Validation Loss: 248.22222844679348\n",
      "Epoch 97/100, Training Loss: 49.14098358154297, Validation Loss: 258.68184725503727\n",
      "Epoch 98/100, Training Loss: 32.02586364746094, Validation Loss: 239.03405926203487\n",
      "Epoch 99/100, Training Loss: 31.3738956451416, Validation Loss: 245.27500342148724\n",
      "Epoch 100/100, Training Loss: 69.75791931152344, Validation Loss: 255.28781795642192\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "model = SolarPredictionNet(FEATURE_SIZE, NUM_FEATURES)\n",
    "train_model(model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
