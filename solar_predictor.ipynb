{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_TO_KEEP = [\n",
    "    \"direct_rad:W\",\n",
    "    \"clear_sky_rad:W\",\n",
    "    \"diffuse_rad:W\",\n",
    "    \"direct_rad_1h:J\",\n",
    "    \"is_in_shadow:idx\",\n",
    "    \"clear_sky_energy_1h:J\",\n",
    "    \"diffuse_rad_1h:J\",\n",
    "    \"is_day:idx\",\n",
    "    \"sun_elevation:d\",\n",
    "    \"ceiling_height_agl:m\",\n",
    "    \"effective_cloud_cover:p\",\n",
    "    \"visibility:m\",\n",
    "    'total_cloud_cover:p',\n",
    "    'air_density_2m:kgm3',\n",
    "    'wind_speed_v_10m:ms',\n",
    "    'dew_point_2m:K',\n",
    "    'wind_speed_u_10m:ms',\n",
    "    # 't_1000hPa:K',\n",
    "    'absolute_humidity_2m:gm3',\n",
    "    # 'snow_water:kgm2',\n",
    "    'relative_humidity_1000hPa:p',\n",
    "    # 'fresh_snow_24h:cm',\n",
    "    'cloud_base_agl:m',\n",
    "    # 'fresh_snow_12h:cm',\n",
    "    # 'snow_depth:cm',\n",
    "    # 'dew_or_rime:idx',\n",
    "    # 'fresh_snow_6h:cm',\n",
    "    # 'super_cooled_liquid_water:kgm2',\n",
    "    # 'fresh_snow_3h:cm',\n",
    "    'rain_water:kgm2',\n",
    "    # 'precip_type_5min:idx',\n",
    "    # 'precip_5min:mm',\n",
    "    # 'fresh_snow_1h:cm',\n",
    "    # 'sun_azimuth:d',\n",
    "    'msl_pressure:hPa',\n",
    "    # 'pressure_100m:hPa',\n",
    "    # 'pressure_50m:hPa',\n",
    "    # 'sfc_pressure:hPa',\n",
    "    # 'prob_rime:p',\n",
    "    # 'wind_speed_10m:ms',\n",
    "    # 'elevation:m',\n",
    "    # 'snow_density:kgm3',\n",
    "    # 'snow_drift:idx',\n",
    "    # 'snow_melt_10min:mm',\n",
    "    # 'wind_speed_w_1000hPa:ms',\n",
    "    # \"date_calc\", \n",
    "    \"pv_measurement\",\n",
    "]\n",
    "LEARNING_RATE = 0.00008\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "NUM_FEATURES = len(COLUMNS_TO_KEEP) - 1  # -1 because pv_measurement is the target\n",
    "FEATURE_SIZE = 4  # 7 days of hourly data\n",
    "WEIGHT_DECAY = 0.12\n",
    "SEQUENCE_LENGTH = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, sequence_length):\n",
    "    \"\"\"\n",
    "    Converts time series data into overlapping sequences/windows.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    target_length = 1\n",
    "    for i in range(len(data) - sequence_length + 1):\n",
    "        seq = data[i : i + sequence_length]\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)\n",
    "\n",
    "def apply_pca(data, q=None, center=True, niter=2):\n",
    "    \"\"\"\n",
    "    Applies PCA to the given data tensor.\n",
    "    Returns transformed data and PCA components.\n",
    "    \"\"\"\n",
    "    U, S, V = torch.pca_lowrank(data, q=q, center=center, niter=niter)\n",
    "    transformed_data = torch.matmul(data, V)\n",
    "    return transformed_data, V\n",
    "\n",
    "\n",
    "class SolarPredictionNet(nn.Module):\n",
    "    def __init__(self, sequence_length, num_channels):\n",
    "        super(SolarPredictionNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(num_channels, 32, kernel_size=sequence_length, stride=1)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=1, stride=1)\n",
    "        self.dropout = nn.Dropout(0.5)  # Add dropout layer\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Use dropout after first fully connected layer\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Parquet files\n",
    "location = \"A\"\n",
    "# 1. Load data\n",
    "df_observed = pd.read_parquet(f\"data/{location}/X_train_observed.parquet\")\n",
    "df_estimated = pd.read_parquet(f\"data/{location}/X_train_estimated.parquet\")\n",
    "df_target = pd.read_parquet(f\"data/{location}/train_targets.parquet\")\n",
    "\n",
    "# 2. Combine observed and estimated datasets\n",
    "df_combined = pd.concat([df_observed, df_estimated], axis=0).sort_values(by='date_forecast')\n",
    "\n",
    "# 3. Merge with target data\n",
    "df_merged = pd.merge(\n",
    "    df_combined, df_target, left_on=\"date_forecast\", right_on=\"time\", how=\"inner\"\n",
    ")\n",
    "\n",
    "# Downsampling the dataframe to hourly intervals\n",
    "df_merged = df_merged.resample('H', on=\"date_forecast\").mean()\n",
    "\n",
    "# Keep only relevant columns\n",
    "df_merged = df_merged[COLUMNS_TO_KEEP]\n",
    "\n",
    "# 4. Extract features and target\n",
    "df_merged.fillna(0, inplace=True)  # Fill NaN values\n",
    "\n",
    "y = df_merged[\"pv_measurement\"]\n",
    "X = df_merged.drop(\"pv_measurement\", axis=1)\n",
    "\n",
    "# Convert dataframes to sequences\n",
    "X_sequences = create_sequences(X.values, SEQUENCE_LENGTH)\n",
    "\n",
    "# Adjust the sequence creation for y\n",
    "y_sequences = y.values[SEQUENCE_LENGTH-1:]  # Aligned with the end of each sequence # Aligned with the end of each sequence and remove the last element\n",
    "\n",
    "# Sequential Split\n",
    "train_size = int(0.8 * len(X_sequences))\n",
    "X_train, X_val = X_sequences[:train_size], X_sequences[train_size:]\n",
    "y_train, y_val = y_sequences[:train_size], y_sequences[train_size:]\n",
    "\n",
    "# Normalize the data and apply PCA\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_val = scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n",
    "\n",
    "# Convert to tensors and apply PCA\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "\n",
    "# Here, q can be a smaller number than NUM_FEATURES if you want to reduce the number of features further\n",
    "X_train_tensor, pca_components = apply_pca(X_train_tensor, q=NUM_FEATURES)\n",
    "X_val_tensor, _ = apply_pca(X_val_tensor, q=NUM_FEATURES)\n",
    "\n",
    "# Ensure shapes are [batch, channels, sequence]\n",
    "X_train_tensor = X_train_tensor.transpose(1, 2)  \n",
    "X_val_tensor = X_val_tensor.transpose(1, 2)  \n",
    "\n",
    "# Update NUM_FEATURES to the size after PCA\n",
    "NUM_FEATURES = pca_components.shape[1]\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).transpose(1, 2)  # Adjust shape to [batch, channels, sequence]\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).transpose(1, 2)  # Adjust shape to [batch, channels, sequence]\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a custom dataset\n",
    "class SolarDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# Create datasets for training and validation\n",
    "train_dataset = SolarDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = SolarDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ... [Neural Network and Training code from previous messages]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader):\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "\n",
    "        total_train_loss = 0.0  # Initialize accumulated training loss\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            total_train_loss += loss.item()  # Accumulate the training loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)  # Compute the average training loss\n",
    "\n",
    "        # Evaluate the model on the validation set (This part remains unchanged)\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)  # Compute the average validation loss\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}\"\n",
    "        )\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 657.371711291764, Validation Loss: 409.4694804670506\n",
      "Epoch 2/100, Training Loss: 337.93757997465383, Validation Loss: 220.64079688122106\n",
      "Epoch 3/100, Training Loss: 282.26717600056, Validation Loss: 207.31011888093727\n",
      "Epoch 4/100, Training Loss: 269.8601480572112, Validation Loss: 197.38974555522032\n",
      "Epoch 5/100, Training Loss: 258.3044326064745, Validation Loss: 188.43943860306902\n",
      "Epoch 6/100, Training Loss: 251.32838731162607, Validation Loss: 182.5164016463301\n",
      "Epoch 7/100, Training Loss: 246.44788876933978, Validation Loss: 178.72871884952008\n",
      "Epoch 8/100, Training Loss: 243.46175483234288, Validation Loss: 173.59500411878068\n",
      "Epoch 9/100, Training Loss: 239.8860886778374, Validation Loss: 174.44583752670553\n",
      "Epoch 10/100, Training Loss: 236.2460063343635, Validation Loss: 170.11998238904914\n",
      "Epoch 11/100, Training Loss: 234.99973709505733, Validation Loss: 169.50905488640018\n",
      "Epoch 12/100, Training Loss: 233.8180434154727, Validation Loss: 167.91540117676783\n",
      "Epoch 13/100, Training Loss: 232.51230993759117, Validation Loss: 166.39258056412083\n",
      "Epoch 14/100, Training Loss: 230.1287849615377, Validation Loss: 167.94563890464258\n",
      "Epoch 15/100, Training Loss: 228.40731662963904, Validation Loss: 166.2946329248185\n",
      "Epoch 16/100, Training Loss: 229.20281767855587, Validation Loss: 164.05539347851402\n",
      "Epoch 17/100, Training Loss: 228.40351289750674, Validation Loss: 164.9591005378281\n",
      "Epoch 18/100, Training Loss: 226.91530843685916, Validation Loss: 164.31994616232194\n",
      "Epoch 19/100, Training Loss: 225.44372158112438, Validation Loss: 168.59246514180836\n",
      "Epoch 20/100, Training Loss: 225.22552890006276, Validation Loss: 164.01720503442488\n",
      "Epoch 21/100, Training Loss: 224.48953538241088, Validation Loss: 166.26975590054727\n",
      "Epoch 22/100, Training Loss: 224.15863588299328, Validation Loss: 163.3622324739518\n",
      "Epoch 23/100, Training Loss: 223.36635233494894, Validation Loss: 163.17634823229596\n",
      "Epoch 24/100, Training Loss: 224.6750337166262, Validation Loss: 163.5524359138899\n",
      "Epoch 25/100, Training Loss: 221.79075683189106, Validation Loss: 162.6186723525701\n",
      "Epoch 26/100, Training Loss: 221.84364373277756, Validation Loss: 164.15608040848963\n",
      "Epoch 27/100, Training Loss: 221.00557877549352, Validation Loss: 163.33000185373848\n",
      "Epoch 28/100, Training Loss: 220.46009922298833, Validation Loss: 162.2216896689589\n",
      "Epoch 29/100, Training Loss: 218.3311468749682, Validation Loss: 162.15095870960357\n",
      "Epoch 30/100, Training Loss: 220.33053943455707, Validation Loss: 164.20727670642935\n",
      "Epoch 31/100, Training Loss: 219.8345943374283, Validation Loss: 161.31478791473964\n",
      "Epoch 32/100, Training Loss: 216.92328125441853, Validation Loss: 160.3525234314404\n",
      "Epoch 33/100, Training Loss: 217.34390412818732, Validation Loss: 159.50196925791545\n",
      "Epoch 34/100, Training Loss: 217.2966378268589, Validation Loss: 159.85592260914612\n",
      "Epoch 35/100, Training Loss: 216.58334928535385, Validation Loss: 159.69892263993503\n",
      "Epoch 36/100, Training Loss: 216.05686583906376, Validation Loss: 160.20428481589892\n",
      "Epoch 37/100, Training Loss: 216.64626534444866, Validation Loss: 158.50340381920645\n",
      "Epoch 38/100, Training Loss: 213.61057055325116, Validation Loss: 159.08462611196143\n",
      "Epoch 39/100, Training Loss: 215.0624607509239, Validation Loss: 158.18626972432858\n",
      "Epoch 40/100, Training Loss: 213.54470687217864, Validation Loss: 159.8473209944229\n",
      "Epoch 41/100, Training Loss: 214.243428148251, Validation Loss: 158.80472109403982\n",
      "Epoch 42/100, Training Loss: 212.01729635638702, Validation Loss: 158.32796364336372\n",
      "Epoch 43/100, Training Loss: 212.8534620525261, Validation Loss: 156.7052204885585\n",
      "Epoch 44/100, Training Loss: 211.6129644238928, Validation Loss: 157.6230115429664\n",
      "Epoch 45/100, Training Loss: 212.33876701782577, Validation Loss: 156.96744778875276\n",
      "Epoch 46/100, Training Loss: 211.84926551415248, Validation Loss: 157.58301713882707\n",
      "Epoch 47/100, Training Loss: 211.7851930052007, Validation Loss: 155.84835329817304\n",
      "Epoch 48/100, Training Loss: 211.7322333682863, Validation Loss: 156.13583529683393\n",
      "Epoch 49/100, Training Loss: 210.06933170101468, Validation Loss: 155.92056506628444\n",
      "Epoch 50/100, Training Loss: 211.19107918503892, Validation Loss: 159.21567916357458\n",
      "Epoch 51/100, Training Loss: 208.86689817792407, Validation Loss: 156.59327149148706\n",
      "Epoch 52/100, Training Loss: 209.77153177689783, Validation Loss: 157.92609417931095\n",
      "Epoch 53/100, Training Loss: 208.8266929039599, Validation Loss: 156.0450180016471\n",
      "Epoch 54/100, Training Loss: 209.40811769060937, Validation Loss: 158.98728311480988\n",
      "Epoch 55/100, Training Loss: 210.4928782819102, Validation Loss: 157.21994880168492\n",
      "Epoch 56/100, Training Loss: 208.91185147700398, Validation Loss: 155.19699124004333\n",
      "Epoch 57/100, Training Loss: 208.41659765910518, Validation Loss: 156.99874489086085\n",
      "Epoch 58/100, Training Loss: 207.5212085609397, Validation Loss: 156.27442018970586\n",
      "Epoch 59/100, Training Loss: 209.35890591826154, Validation Loss: 155.7932676778117\n",
      "Epoch 60/100, Training Loss: 207.28321590353687, Validation Loss: 155.00018878141094\n",
      "Epoch 61/100, Training Loss: 207.75912095806285, Validation Loss: 155.95711395979487\n",
      "Epoch 62/100, Training Loss: 207.62747557616012, Validation Loss: 158.3553477538497\n",
      "Epoch 63/100, Training Loss: 206.07105523749507, Validation Loss: 154.31728321851568\n",
      "Epoch 64/100, Training Loss: 207.21031175764188, Validation Loss: 156.168587389392\n",
      "Epoch 65/100, Training Loss: 206.88172134849196, Validation Loss: 157.4002442743605\n",
      "Epoch 66/100, Training Loss: 207.40266040050997, Validation Loss: 158.29907001617397\n",
      "Epoch 67/100, Training Loss: 205.5300538265858, Validation Loss: 156.76176150988312\n",
      "Epoch 68/100, Training Loss: 204.82582895431662, Validation Loss: 156.0929396964024\n",
      "Epoch 69/100, Training Loss: 206.60302847498633, Validation Loss: 156.7949479419465\n",
      "Epoch 70/100, Training Loss: 206.26213147311256, Validation Loss: 157.20092465672548\n",
      "Epoch 71/100, Training Loss: 206.23521770743812, Validation Loss: 155.49246091856858\n",
      "Epoch 72/100, Training Loss: 204.36358972949213, Validation Loss: 155.40784049980283\n",
      "Epoch 73/100, Training Loss: 204.07687384940363, Validation Loss: 156.63198215091427\n",
      "Epoch 74/100, Training Loss: 205.401864468408, Validation Loss: 158.3089145113902\n",
      "Epoch 75/100, Training Loss: 204.8786608349519, Validation Loss: 157.58253171087924\n",
      "Epoch 76/100, Training Loss: 204.9688477955037, Validation Loss: 155.56462372587777\n",
      "Epoch 77/100, Training Loss: 203.9571655140527, Validation Loss: 154.96903355049906\n",
      "Epoch 78/100, Training Loss: 204.76299051107577, Validation Loss: 155.70675959350422\n",
      "Epoch 79/100, Training Loss: 204.09809335848902, Validation Loss: 158.05295087887777\n",
      "Epoch 80/100, Training Loss: 202.60928282572897, Validation Loss: 155.53119522483053\n",
      "Epoch 81/100, Training Loss: 205.4931006513997, Validation Loss: 156.4648941270209\n",
      "Epoch 82/100, Training Loss: 204.20973073316426, Validation Loss: 157.6757580217234\n",
      "Epoch 83/100, Training Loss: 203.93998380274869, Validation Loss: 155.84082689555146\n",
      "Epoch 84/100, Training Loss: 203.96856299630403, Validation Loss: 157.3478614521336\n",
      "Epoch 85/100, Training Loss: 202.36892982241085, Validation Loss: 156.57135293608812\n",
      "Epoch 86/100, Training Loss: 203.09093599781272, Validation Loss: 157.16288091996358\n",
      "Epoch 87/100, Training Loss: 203.49293857419923, Validation Loss: 155.86428642950148\n",
      "Epoch 88/100, Training Loss: 202.3176876320444, Validation Loss: 155.52295939878448\n",
      "Epoch 89/100, Training Loss: 202.9843467292805, Validation Loss: 155.67667709476595\n",
      "Epoch 90/100, Training Loss: 202.59545071723858, Validation Loss: 157.60502732523878\n",
      "Epoch 91/100, Training Loss: 202.38943816111134, Validation Loss: 156.51240846639655\n",
      "Epoch 92/100, Training Loss: 202.87711347582183, Validation Loss: 157.6016534219617\n",
      "Epoch 93/100, Training Loss: 201.1163068059897, Validation Loss: 156.56784721680071\n",
      "Epoch 94/100, Training Loss: 202.0828729829161, Validation Loss: 156.81394661020258\n",
      "Epoch 95/100, Training Loss: 202.4472458687727, Validation Loss: 156.26016849986\n",
      "Epoch 96/100, Training Loss: 203.17559859260933, Validation Loss: 157.75521762192705\n",
      "Epoch 97/100, Training Loss: 202.1239393373458, Validation Loss: 157.72463594615124\n",
      "Epoch 98/100, Training Loss: 201.48504846147506, Validation Loss: 156.94578170808086\n",
      "Epoch 99/100, Training Loss: 201.56822778136754, Validation Loss: 158.8254968013543\n",
      "Epoch 100/100, Training Loss: 201.4067343359024, Validation Loss: 156.48640701440456\n",
      "Training complete!\n",
      "Model saved as model_location_A.pt\n"
     ]
    }
   ],
   "source": [
    "def save_model(model, location):\n",
    "    filename = f\"model_location_{location}.pt\"\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    print(f\"Model saved as {filename}\")\n",
    "\n",
    "# ... [Training Loop]\n",
    "\n",
    "model = SolarPredictionNet(SEQUENCE_LENGTH, NUM_FEATURES)\n",
    "train_model(model, train_loader, val_loader)\n",
    "save_model(model, location) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "B\n",
      "C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fm/6k4602pj3xx1rcp_y10195140000gn/T/ipykernel_66234/3957842248.py:61: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[2.58934142e-06 2.65489075e-06 2.70440205e-06 2.78170842e-06\n",
      " 2.15334606e+00 2.21422253e+01 1.55246033e+02 4.59206421e+02\n",
      " 8.66337158e+02 1.12080969e+03 1.14144531e+03 1.17497144e+03\n",
      " 9.77783936e+02 1.09333643e+03 1.41139917e+03 1.06533069e+03\n",
      " 7.09244446e+02 3.79455627e+02 1.15042870e+02 3.28770921e-06\n",
      " 3.27935004e-06 3.25304791e-06 3.20458685e-06 3.17543822e-06\n",
      " 3.16835940e-06 3.20221170e-06 3.26968757e-06 1.82681717e+02\n",
      " 1.05808057e+03 1.82512134e+03 2.56051221e+03 3.44787695e+03\n",
      " 4.23896631e+03 4.56325049e+03 4.31683398e+03 4.41301416e+03\n",
      " 4.23042139e+03 3.89182812e+03 3.40924219e+03 2.76311084e+03\n",
      " 1.70245239e+03 8.12387451e+02 3.90298096e+02 8.35193100e+01\n",
      " 8.19496095e-01 3.31730052e-06 3.31015895e-06 3.30547459e-06\n",
      " 3.30810849e-06 3.31492106e-06 3.34504013e+01 2.50453979e+02\n",
      " 8.10433716e+02 1.92697290e+03 3.18616870e+03 4.25847168e+03\n",
      " 5.15312500e+03 5.57746973e+03 5.75160156e+03 5.29172852e+03\n",
      " 4.33784570e+03 3.76835767e+03 2.83749976e+03 1.71526575e+03\n",
      " 9.24757935e+02 6.71833801e+02 3.61337036e+02 1.52849197e+02\n",
      " 3.29362933e-06 3.29366003e-06 3.83254155e-06 5.27814776e-03\n",
      " 3.88489008e-01 2.04478302e+01 1.51395416e+02 3.47551086e+02\n",
      " 9.67657471e+02 1.94841785e+03 2.96657373e+03 3.94015063e+03\n",
      " 4.48733936e+03 4.55994922e+03 4.33474219e+03 4.02798901e+03\n",
      " 3.56082275e+03 3.23429419e+03 3.22643848e+03 2.88242139e+03\n",
      " 1.69027051e+03 7.02794250e+02 3.99414825e+02 1.97480499e+02\n",
      " 7.19550133e-01 3.44090631e-06 3.31535193e-06 3.31491924e-06\n",
      " 3.80383472e-06 1.23263378e+01 2.10258667e+02 5.73003906e+02\n",
      " 9.76948364e+02 1.67436780e+03 3.01881152e+03 4.16762598e+03\n",
      " 5.15490186e+03 5.25346680e+03 5.24067041e+03 5.20271436e+03\n",
      " 5.01579102e+03 4.46246484e+03 3.71301685e+03 2.93122949e+03\n",
      " 1.85537964e+03 9.25121826e+02 5.12418945e+02 3.01767517e+02\n",
      " 9.74647369e+01 3.31351362e-06 3.28339888e-06 3.30055991e-06\n",
      " 3.30608145e-06 6.31264925e+00 1.78819000e+02 5.60224670e+02\n",
      " 1.21707214e+03 2.09857495e+03 3.25188843e+03 4.29478809e+03\n",
      " 4.57487988e+03 4.35987109e+03 4.02898340e+03 3.68803125e+03\n",
      " 3.54182764e+03 3.65136499e+03 3.69804346e+03 2.73138281e+03\n",
      " 1.54546826e+03 8.31459229e+02 5.66858521e+02 2.88072876e+02\n",
      " 2.36688948e+00 3.31079673e-06 3.28599413e-06 3.35653704e-06\n",
      " 3.49216884e-06 5.75812263e+01 2.83128510e+02 6.23724365e+02\n",
      " 1.08418457e+03 2.16282324e+03 3.33726294e+03 4.26464648e+03\n",
      " 5.04958105e+03 5.39353418e+03 5.05633154e+03 4.70410742e+03\n",
      " 4.39898096e+03 4.03662695e+03 3.66221680e+03 2.71264551e+03\n",
      " 1.50860510e+03 9.27066284e+02 5.53374878e+02 2.73933044e+02\n",
      " 5.50983849e+01 5.35930712e-06 4.14868509e-06 3.34459560e-06\n",
      " 3.89108982e-06 8.81984901e+00 1.26044876e+02 2.12539154e+02\n",
      " 3.36178711e+02 3.70751953e+02 4.89776611e+02 6.20032104e+02\n",
      " 7.37382019e+02 9.12333862e+02 1.03381494e+03 1.21276929e+03\n",
      " 1.63026978e+03 2.15247729e+03 2.01465295e+03 1.82516968e+03\n",
      " 1.25269470e+03 6.55951111e+02 3.73841614e+02 1.58368378e+02\n",
      " 1.99431586e+00 3.56590317e-06 3.30821194e-06 3.31779211e-06\n",
      " 3.83011684e-06 1.96344147e+01 1.30962585e+02 1.74843689e+02\n",
      " 2.73296539e+02 4.00166901e+02 9.55469604e+02 2.15808740e+03\n",
      " 2.92488062e+03 2.96360156e+03 3.05151489e+03 3.37490479e+03\n",
      " 3.27106543e+03 3.02966064e+03 2.58396729e+03 1.91807495e+03\n",
      " 9.65168213e+02 4.58229553e+02 2.70646027e+02 1.24418579e+02\n",
      " 2.96348419e+01 4.09453560e-06 3.31968545e-06 3.29777254e-06\n",
      " 3.34222523e-06 1.18194342e+00 1.07061279e+02 2.44657440e+02\n",
      " 4.21924316e+02 6.01147217e+02 9.43678467e+02 1.23952808e+03\n",
      " 1.61819690e+03 1.79857544e+03 1.92752051e+03 2.17758398e+03\n",
      " 1.98396814e+03 1.72115771e+03 1.48454187e+03 1.25584290e+03\n",
      " 7.77973633e+02 4.80318604e+02 2.82315979e+02 1.04883369e+02\n",
      " 1.20773144e+01 4.68384906e-06 4.05557603e-06 3.45214812e-06\n",
      " 3.54998838e-06 3.13863707e+00 9.07635040e+01 2.01682068e+02\n",
      " 3.30607727e+02 4.88949188e+02 8.82303223e+02 2.16695703e+03\n",
      " 2.52407617e+03 2.00061536e+03 1.85784937e+03 1.97482959e+03\n",
      " 1.76308521e+03 1.59862085e+03 1.44784961e+03 1.28593481e+03\n",
      " 1.05310095e+03 6.45999878e+02 3.22950409e+02 1.46644089e+02\n",
      " 5.07083678e-06 3.31198294e-06 3.29401814e-06 3.30295325e-06\n",
      " 3.36979519e-06 4.50390005e+00 1.23625908e+02 2.85052002e+02\n",
      " 4.23375977e+02 7.22262390e+02 1.21213232e+03 2.09532812e+03\n",
      " 2.46886914e+03 2.50768042e+03 2.18958691e+03 2.20962866e+03\n",
      " 1.98396887e+03 1.50914453e+03 1.25630005e+03 1.04260376e+03\n",
      " 6.10132874e+02 3.25258362e+02 1.67788269e+02 5.57141724e+01\n",
      " 1.21663532e+01 5.34122046e-06 4.55137160e-06 4.05855053e-06\n",
      " 3.38581344e-06 7.63392830e+00 1.23176346e+02 2.22238007e+02\n",
      " 2.72908508e+02 4.35948853e+02 8.20200745e+02 1.18449902e+03\n",
      " 1.32055566e+03 1.65143921e+03 2.09844751e+03 2.44061523e+03\n",
      " 2.02550464e+03 1.78904907e+03 1.55973535e+03 1.18476416e+03\n",
      " 7.78622131e+02 4.91355347e+02 2.87092712e+02 1.01027451e+02\n",
      " 2.98794508e+00 3.31296201e-06 3.26692498e-06 3.23377299e-06\n",
      " 3.25672818e-06 1.83919411e+01 1.14483032e+02 2.53590973e+02\n",
      " 6.00315430e+02 8.79057129e+02 1.19525842e+03 1.80089990e+03\n",
      " 2.59019678e+03 2.98174854e+03 3.12857788e+03 3.00741357e+03\n",
      " 2.59335693e+03 2.11432812e+03 1.70697766e+03 1.28546558e+03\n",
      " 7.93588501e+02 4.22082031e+02 1.79420654e+02 3.07629242e+01\n",
      " 1.21663532e+01 5.03829096e-06 4.16464445e-06 3.92960192e-06\n",
      " 3.91335789e-06 9.11880493e+00 1.10611465e+02 2.19748535e+02\n",
      " 3.23556305e+02 5.05239197e+02 7.06216919e+02 1.00827295e+03\n",
      " 1.24966272e+03 1.61222156e+03 1.74081470e+03 1.71896106e+03\n",
      " 1.44632068e+03 1.18379321e+03 1.13539258e+03 9.62871521e+02\n",
      " 6.84423218e+02 4.24847412e+02 1.68888107e+02 3.32873993e+01\n",
      " 1.11083164e+01 4.50146354e-06 3.59598630e-06 3.31125671e-06\n",
      " 3.41421560e-06 2.62076130e+01 1.06680344e+02 2.09510178e+02\n",
      " 4.15549408e+02 6.55161621e+02 9.27865967e+02 1.58670532e+03\n",
      " 2.10725269e+03 2.38401172e+03 2.38966162e+03 2.40276465e+03\n",
      " 2.23203687e+03 1.90833203e+03 1.51160913e+03 1.14742114e+03\n",
      " 7.93518921e+02 5.77548157e+02 2.87935730e+02 1.15804108e+02\n",
      " 1.21663532e+01 2.05987263e+00 4.84957536e-06 4.69862516e-06\n",
      " 4.81983261e-06 2.73191833e+01 1.19311920e+02 2.09612152e+02\n",
      " 3.04023621e+02 4.56284241e+02 6.11130249e+02 8.86035645e+02\n",
      " 1.16896729e+03 1.39152075e+03 1.61725867e+03 1.74348779e+03\n",
      " 1.74821997e+03 1.73665027e+03 1.46154370e+03 1.09291284e+03\n",
      " 7.14890015e+02 5.58460999e+02 3.23887604e+02 1.30452286e+02\n",
      " 6.48582458e+00 3.48148637e-06 3.31111278e-06 3.70287034e-06\n",
      " 2.10440964e-01 7.91221161e+01 2.96902466e+02 6.86052979e+02\n",
      " 1.38842371e+03 2.35036743e+03 3.56255811e+03 4.70965039e+03\n",
      " 5.38759570e+03 5.85542139e+03 6.15588574e+03 6.17892676e+03\n",
      " 5.54980469e+03 5.04856201e+03 4.33177051e+03 3.46194360e+03\n",
      " 2.41891528e+03 1.40078076e+03 7.54789307e+02 4.02409119e+02\n",
      " 1.25143692e+02 3.26028317e-06 3.25470273e-06 3.30548005e-06\n",
      " 2.06196404e+00 1.31694794e+02 3.87227112e+02 7.54282532e+02\n",
      " 1.18675342e+03 2.01262732e+03 2.88504712e+03 3.84955615e+03\n",
      " 4.51296777e+03 5.13035352e+03 4.86428418e+03 4.09949561e+03\n",
      " 3.13631909e+03 2.41240186e+03 2.17453564e+03 1.68135254e+03\n",
      " 1.29985779e+03 7.24222656e+02 4.34285706e+02 2.66769470e+02\n",
      " 8.48248672e+01 8.97720873e-01 3.30012904e-06 3.30120929e-06\n",
      " 3.31502338e-06 7.57161942e+01 2.25173187e+02 3.63004639e+02\n",
      " 5.11848816e+02 8.76336487e+02 2.44499609e+03 3.29252930e+03\n",
      " 3.65403760e+03 3.96330273e+03 4.92036426e+03 5.39570996e+03\n",
      " 5.30284961e+03 4.75590674e+03 3.91077295e+03 2.93025269e+03\n",
      " 2.00586548e+03 1.13395239e+03 6.87527161e+02 3.94865967e+02\n",
      " 1.75209106e+02 8.74538040e+00 3.85314979e-06 1.32156146e+00\n",
      " 4.36080856e+01 1.27949989e+02 2.72132019e+02 5.59929077e+02\n",
      " 1.26119128e+03 2.16512109e+03 3.24203223e+03 4.30661182e+03\n",
      " 4.97999756e+03 5.38154639e+03 5.67593799e+03 5.62270508e+03\n",
      " 5.27370508e+03 4.71743066e+03 3.93579932e+03 3.02544092e+03\n",
      " 2.04407288e+03 1.14122156e+03 6.88358582e+02 4.67877380e+02\n",
      " 2.42151093e+02 6.35026663e-06 3.30773946e-06 3.40737756e-06\n",
      " 1.63312111e+01 1.19344460e+02 2.34631409e+02 5.47229370e+02\n",
      " 1.15155640e+03 2.03201257e+03 3.05347900e+03 4.15946484e+03\n",
      " 4.98361035e+03 5.39079980e+03 5.14739893e+03 4.90770898e+03\n",
      " 4.03470044e+03 4.06089185e+03 3.42526270e+03 2.52128564e+03\n",
      " 1.65999390e+03 9.67308350e+02 5.18549500e+02 2.73669067e+02\n",
      " 6.88432236e+01 4.38586330e+00 3.63861363e-06 3.35276150e-06\n",
      " 6.96807957e+00 1.47736130e+02 2.77321350e+02 4.10993164e+02\n",
      " 5.33539795e+02 6.66559082e+02 9.69133179e+02 1.21435486e+03\n",
      " 1.21745435e+03 1.15306396e+03 1.58726758e+03 3.19078711e+03\n",
      " 3.77754883e+03 4.27556934e+03 4.25026855e+03 3.44846558e+03\n",
      " 2.51589697e+03 1.75483044e+03 1.01129016e+03 5.04797058e+02\n",
      " 2.01475601e+02 4.26607132e+00 3.28772080e-06 1.54759848e+00\n",
      " 1.15897841e-04 1.23511086e+01 3.62266617e+01 1.11490723e+02\n",
      " 7.62308044e+01 1.44044891e+02 5.28165161e+02 9.07790527e+02\n",
      " 1.46627686e+03 1.85499060e+03 1.72220398e+03 1.38982520e+03\n",
      " 1.41372253e+03 1.93256274e+03 2.59627930e+03 2.55814209e+03\n",
      " 1.66124902e+03 6.94101318e+02 2.98625153e+02 1.09547600e+02\n",
      " 8.04202378e-01 3.25305814e-06 3.25296651e-06 3.25585370e-06\n",
      " 3.26763878e+01 1.10170921e+02 2.16823975e+02 4.54155334e+02\n",
      " 1.08820227e+03 2.21856445e+03 3.44971533e+03 4.43708301e+03\n",
      " 5.14478613e+03 5.37337549e+03 5.20208057e+03 4.72664844e+03\n",
      " 4.29331543e+03 3.81356738e+03 3.61448193e+03 2.67737061e+03\n",
      " 2.00869617e+03 1.23019336e+03 6.57658264e+02 4.14389282e+02\n",
      " 1.70684692e+02 1.36781330e+01 3.26903046e-06 3.31404226e-06\n",
      " 2.80971909e+01 1.43086029e+02 2.74841858e+02 5.74973877e+02\n",
      " 1.04993579e+03 2.06875244e+03 3.25931152e+03 4.04377051e+03\n",
      " 4.51570605e+03 5.04444141e+03 5.40782129e+03 5.42092285e+03\n",
      " 4.91165332e+03 4.32658594e+03 3.56065308e+03 2.68745068e+03\n",
      " 1.80120813e+03 9.77469604e+02 5.15494751e+02 3.08656891e+02\n",
      " 1.35887741e+02 1.47837877e-01 3.29740692e-06 3.82746748e-06\n",
      " 1.12051659e+01 9.39144440e+01 1.99030197e+02 3.48385559e+02\n",
      " 3.83294678e+02 4.79445129e+02 5.82733887e+02 7.04769470e+02\n",
      " 7.30614868e+02 8.20126892e+02 1.22415735e+03 1.21084912e+03\n",
      " 9.41457458e+02 9.70755981e+02 8.73676270e+02 6.94852295e+02\n",
      " 4.55844757e+02 2.36842499e+02 1.31565323e+02 2.47496014e+01\n",
      " 1.29698706e+00 3.28754254e-06 3.24398729e-06 3.23415270e-06\n",
      " 3.26572945e-06 3.03303452e+01 1.17899185e+02 2.20457184e+02\n",
      " 3.33906494e+02 4.38049255e+02 6.26076782e+02 1.11494897e+03\n",
      " 1.15544299e+03 8.07789612e+02 9.03005371e+02 1.58173035e+03\n",
      " 1.40650781e+03 1.21908374e+03 1.18667090e+03 1.02721497e+03\n",
      " 7.52926086e+02 5.24625244e+02 3.55767822e+02 1.37360138e+02\n",
      " 1.75025234e+01 4.65733671e+00 7.10069299e-01 1.85928774e+00\n",
      " 1.81613560e+01 9.89417419e+01 3.44398010e+02 6.53822144e+02\n",
      " 9.66106140e+02 2.12223511e+03 3.31538306e+03 4.28068750e+03\n",
      " 5.14775439e+03 5.67082129e+03 5.53653027e+03 5.06235840e+03\n",
      " 4.50511182e+03 4.23335498e+03 3.40007861e+03 2.62733887e+03\n",
      " 1.72760779e+03 8.67476929e+02 4.72511536e+02 2.49214630e+02\n",
      " 6.38715401e+01 1.02126942e-05 3.26794566e-06 3.31438787e-06\n",
      " 1.25194168e+01 1.14693375e+02 2.68616669e+02 5.32176697e+02\n",
      " 7.70081177e+02 1.01865625e+03 1.08294727e+03 1.39986353e+03\n",
      " 2.19011011e+03 3.34189062e+03 3.75452148e+03 3.83117896e+03\n",
      " 3.80209668e+03 3.20261743e+03 2.18046387e+03 1.26212976e+03\n",
      " 9.13832764e+02 5.84146973e+02 3.22580139e+02 1.79778625e+02\n",
      " 5.09690628e+01 4.14600390e-06 3.35486902e-06 4.04778621e-06]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df_submission.loc[mask, \"prediction\"] = preds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_model(location):\n",
    "    model = SolarPredictionNet(SEQUENCE_LENGTH, NUM_FEATURES)\n",
    "    model.load_state_dict(torch.load(f\"model_location_{location}.pt\"))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def pad_data(data, sequence_length):\n",
    "    \"\"\"\n",
    "    Pads the data with zeros at the beginning to ensure \n",
    "    the final number of sequences matches the original number of data points.\n",
    "    \"\"\"\n",
    "    padding = np.zeros((sequence_length - 1, data.shape[1]))\n",
    "    return np.vstack((padding, data))\n",
    "\n",
    "\n",
    "def make_predictions(location, df_test):\n",
    "    # Load model\n",
    "    model = load_model(location)\n",
    "    \n",
    "    # Ensure the index is a datetime\n",
    "    df_test['date_forecast'] = pd.to_datetime(df_test['date_forecast'])\n",
    "    # Set the date_calc column as the index for resampling\n",
    "    df_test.set_index('date_forecast', inplace=True)\n",
    "    # Resample to 1-hour intervals\n",
    "    df_test = df_test.resample('1H').mean()\n",
    "    df_test = df_test.dropna(how='all').reset_index(drop=True)\n",
    "    \n",
    "    # Keep only the columns used during training (minus the target column)\n",
    "    relevant_columns = [col for col in COLUMNS_TO_KEEP if col != \"pv_measurement\"]\n",
    "    df_test = df_test[relevant_columns]\n",
    "    \n",
    "    # Fill NaNs (if any after resampling)\n",
    "    df_test.fillna(0, inplace=True)\n",
    "    # Create sequences and normalize\n",
    "    padded_data = pad_data(df_test.values, SEQUENCE_LENGTH)\n",
    "    test_sequences = create_sequences(padded_data, SEQUENCE_LENGTH)\n",
    "    test_sequences = scaler.transform(test_sequences.reshape(-1, test_sequences.shape[-1])).reshape(test_sequences.shape)\n",
    "    test_tensor = torch.tensor(test_sequences, dtype=torch.float32).transpose(1, 2)\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        predictions = model(test_tensor)\n",
    "        predictions = predictions.numpy().flatten()\n",
    "    return predictions\n",
    "\n",
    "\n",
    "\n",
    "# Read the Kaggle test.csv to get the location and ids\n",
    "df_submission = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "locations = [\"A\", \"B\", \"C\"]\n",
    "\n",
    "# Iterate over the locations and fill in the predictions\n",
    "for loc in locations:\n",
    "    print(loc)\n",
    "    # Load forecasted weather data for testing for the current location\n",
    "    df_loc = pd.read_parquet(f\"data/{loc}/X_test_estimated.parquet\")\n",
    "    preds = make_predictions(loc, df_loc)\n",
    "    # Assign the predictions to df_submission for the current location\n",
    "    mask = df_submission[\"location\"] == loc\n",
    "    df_submission.loc[mask, \"prediction\"] = preds\n",
    "\n",
    "# Save the results to a new submission file\n",
    "df_submission[[\"id\", \"prediction\"]].to_csv(\"sample_kaggle_submission.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
