{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_TO_KEEP = [\n",
    "    \"direct_rad:W\",\n",
    "    \"clear_sky_rad:W\",\n",
    "    \"diffuse_rad:W\",\n",
    "    \"direct_rad_1h:J\",\n",
    "    \"is_in_shadow:idx\",\n",
    "    \"clear_sky_energy_1h:J\",\n",
    "    \"diffuse_rad_1h:J\",\n",
    "    \"is_day:idx\",\n",
    "    \"sun_elevation:d\",\n",
    "    # \"ceiling_height_agl:m\",\n",
    "    # \"effective_cloud_cover:p\",\n",
    "    # \"visibility:m\",\n",
    "    # \"date_calc\",\n",
    "    \"pv_measurement\",\n",
    "]\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "NUM_FEATURES = len(COLUMNS_TO_KEEP) - 1  # -1 because pv_measurement is the target\n",
    "FEATURE_SIZE = 4  # 7 days of hourly data\n",
    "WEIGHT_DECAY = 0.01\n",
    "SEQUENCE_LENGTH = 7*24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, sequence_length):\n",
    "    \"\"\"\n",
    "    Converts time series data into overlapping sequences/windows.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    target_length = 1\n",
    "    for i in range(len(data) - sequence_length - target_length + 1):\n",
    "        seq = data[i : i + sequence_length]\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)\n",
    "\n",
    "class SolarPredictionNet(nn.Module):\n",
    "    def __init__(self, sequence_length, num_channels):\n",
    "        super(SolarPredictionNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(num_channels, 32, kernel_size=sequence_length, stride=1)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=1, stride=1)\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Parquet files\n",
    "df_data = pd.read_parquet(\"data/A/X_train_observed.parquet\")\n",
    "df_target = pd.read_parquet(\"data/A/train_targets.parquet\")\n",
    "\n",
    "# Merge the datasets\n",
    "df_merged = pd.merge(\n",
    "    df_data, df_target, left_on=\"date_forecast\", right_on=\"time\", how=\"inner\"\n",
    ")\n",
    "df_merged = df_merged[COLUMNS_TO_KEEP]\n",
    "\n",
    "y = df_merged[\"pv_measurement\"]\n",
    "X = df_merged.drop(\"pv_measurement\", axis=1)\n",
    "\n",
    "# Convert dataframes to sequences\n",
    "X_sequences = create_sequences(X.values, SEQUENCE_LENGTH)\n",
    "# Adjust the sequence creation for y\n",
    "y_sequences = y.values[SEQUENCE_LENGTH-1:-1]  # Aligned with the end of each sequence and remove the last element\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_sequences, y_sequences, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_val = scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).transpose(1, 2)  # Adjust shape to [batch, channels, sequence]\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).transpose(1, 2)  # Adjust shape to [batch, channels, sequence]\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "\n",
    "# Create a custom dataset\n",
    "class SolarDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# Create datasets for training and validation\n",
    "train_dataset = SolarDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = SolarDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ... [Neural Network and Training code from previous messages]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader):\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Average validation loss\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Training Loss: {loss.item()}, Validation Loss: {val_loss}\"\n",
    "        )\n",
    "\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 182.9816131591797, Validation Loss: 255.39696378965635\n",
      "Epoch 2/100, Training Loss: 96.17887115478516, Validation Loss: 247.17413925995697\n",
      "Epoch 3/100, Training Loss: 88.5373764038086, Validation Loss: 231.7605849394927\n",
      "Epoch 4/100, Training Loss: 77.52349853515625, Validation Loss: 253.63495355554528\n",
      "Epoch 5/100, Training Loss: 40.405303955078125, Validation Loss: 225.09522533932247\n",
      "Epoch 6/100, Training Loss: 54.99589538574219, Validation Loss: 224.61707289412215\n",
      "Epoch 7/100, Training Loss: 78.37740325927734, Validation Loss: 231.013224050161\n",
      "Epoch 8/100, Training Loss: 46.74898147583008, Validation Loss: 228.03345011118296\n",
      "Epoch 9/100, Training Loss: 46.962799072265625, Validation Loss: 219.42545821731156\n",
      "Epoch 10/100, Training Loss: 74.39583587646484, Validation Loss: 223.9060064161146\n",
      "Epoch 11/100, Training Loss: 54.16368103027344, Validation Loss: 217.37908404582257\n",
      "Epoch 12/100, Training Loss: 36.5133171081543, Validation Loss: 217.42653062150285\n",
      "Epoch 13/100, Training Loss: 71.88379669189453, Validation Loss: 220.26656162158864\n",
      "Epoch 14/100, Training Loss: 60.18365478515625, Validation Loss: 216.44143586029878\n",
      "Epoch 15/100, Training Loss: 58.230892181396484, Validation Loss: 217.05670788739178\n",
      "Epoch 16/100, Training Loss: 84.57848358154297, Validation Loss: 218.86796800768053\n",
      "Epoch 17/100, Training Loss: 52.72126388549805, Validation Loss: 217.1530838528195\n",
      "Epoch 18/100, Training Loss: 115.6264419555664, Validation Loss: 228.15640512414882\n",
      "Epoch 19/100, Training Loss: 44.7137451171875, Validation Loss: 224.7099560505635\n",
      "Epoch 20/100, Training Loss: 46.5278434753418, Validation Loss: 226.90821871370883\n",
      "Epoch 21/100, Training Loss: 46.52833557128906, Validation Loss: 221.76704460350243\n",
      "Epoch 22/100, Training Loss: 101.5235595703125, Validation Loss: 234.2708800238532\n",
      "Epoch 23/100, Training Loss: 39.522098541259766, Validation Loss: 216.450447824839\n",
      "Epoch 24/100, Training Loss: 50.92014694213867, Validation Loss: 213.98038767221811\n",
      "Epoch 25/100, Training Loss: 52.3913688659668, Validation Loss: 220.19053359160552\n",
      "Epoch 26/100, Training Loss: 48.32164001464844, Validation Loss: 226.3758034577241\n",
      "Epoch 27/100, Training Loss: 54.36405563354492, Validation Loss: 217.4838967606828\n",
      "Epoch 28/100, Training Loss: 73.33619689941406, Validation Loss: 215.79862419334617\n",
      "Epoch 29/100, Training Loss: 122.2481689453125, Validation Loss: 216.736689449001\n",
      "Epoch 30/100, Training Loss: 45.98210144042969, Validation Loss: 216.61873238022264\n",
      "Epoch 31/100, Training Loss: 63.70717239379883, Validation Loss: 212.97848568478148\n",
      "Epoch 32/100, Training Loss: 43.9134521484375, Validation Loss: 219.6606866784998\n",
      "Epoch 33/100, Training Loss: 66.0608139038086, Validation Loss: 219.77235134227857\n",
      "Epoch 34/100, Training Loss: 68.41851043701172, Validation Loss: 213.9392580805598\n",
      "Epoch 35/100, Training Loss: 43.38411331176758, Validation Loss: 216.51968187899203\n",
      "Epoch 36/100, Training Loss: 51.49600601196289, Validation Loss: 215.28490047970334\n",
      "Epoch 37/100, Training Loss: 60.80771255493164, Validation Loss: 220.33591632327517\n",
      "Epoch 38/100, Training Loss: 60.88285827636719, Validation Loss: 218.46024648305533\n",
      "Epoch 39/100, Training Loss: 46.93551254272461, Validation Loss: 217.29383037670237\n",
      "Epoch 40/100, Training Loss: 38.476253509521484, Validation Loss: 220.42181173788535\n",
      "Epoch 41/100, Training Loss: 62.07913589477539, Validation Loss: 223.3061022371859\n",
      "Epoch 42/100, Training Loss: 44.79351806640625, Validation Loss: 219.2122667467272\n",
      "Epoch 43/100, Training Loss: 36.77022171020508, Validation Loss: 222.95532663706186\n",
      "Epoch 44/100, Training Loss: 55.0765495300293, Validation Loss: 214.9815409892314\n",
      "Epoch 45/100, Training Loss: 78.9311752319336, Validation Loss: 216.94781607550544\n",
      "Epoch 46/100, Training Loss: 50.29233932495117, Validation Loss: 219.65036948951516\n",
      "Epoch 47/100, Training Loss: 92.74066162109375, Validation Loss: 219.97408529745564\n",
      "Epoch 48/100, Training Loss: 119.68194580078125, Validation Loss: 224.49948994404562\n",
      "Epoch 49/100, Training Loss: 112.31649780273438, Validation Loss: 224.0505207783467\n",
      "Epoch 50/100, Training Loss: 99.0212631225586, Validation Loss: 218.96757278442382\n",
      "Epoch 51/100, Training Loss: 54.66055679321289, Validation Loss: 219.99263944883603\n",
      "Epoch 52/100, Training Loss: 69.35808563232422, Validation Loss: 219.00252460789036\n",
      "Epoch 53/100, Training Loss: 64.7566146850586, Validation Loss: 213.77199094617689\n",
      "Epoch 54/100, Training Loss: 75.05069732666016, Validation Loss: 216.62039755743902\n",
      "Epoch 55/100, Training Loss: 89.64395141601562, Validation Loss: 227.07474773510083\n",
      "Epoch 56/100, Training Loss: 56.883270263671875, Validation Loss: 215.12537157213364\n",
      "Epoch 57/100, Training Loss: 80.61772918701172, Validation Loss: 214.55205892614416\n",
      "Epoch 58/100, Training Loss: 95.0444107055664, Validation Loss: 227.37103316848342\n",
      "Epoch 59/100, Training Loss: 90.61392211914062, Validation Loss: 220.71943518148885\n",
      "Epoch 60/100, Training Loss: 59.765445709228516, Validation Loss: 220.74348373413085\n",
      "Epoch 61/100, Training Loss: 88.77801513671875, Validation Loss: 216.51064527356948\n",
      "Epoch 62/100, Training Loss: 60.666500091552734, Validation Loss: 213.18720319593274\n",
      "Epoch 63/100, Training Loss: 72.0667495727539, Validation Loss: 211.56262355495144\n",
      "Epoch 64/100, Training Loss: 56.44398498535156, Validation Loss: 215.74758197681325\n",
      "Epoch 65/100, Training Loss: 88.9620132446289, Validation Loss: 210.8059231526143\n",
      "Epoch 66/100, Training Loss: 34.62974548339844, Validation Loss: 223.67759549939956\n",
      "Epoch 67/100, Training Loss: 68.4125747680664, Validation Loss: 212.2902480254302\n",
      "Epoch 68/100, Training Loss: 80.31668090820312, Validation Loss: 212.79978241791596\n",
      "Epoch 69/100, Training Loss: 60.52168655395508, Validation Loss: 215.0406156591467\n",
      "Epoch 70/100, Training Loss: 108.08919525146484, Validation Loss: 215.86706615138698\n",
      "Epoch 71/100, Training Loss: 62.48060989379883, Validation Loss: 214.49446171940983\n",
      "Epoch 72/100, Training Loss: 63.74812316894531, Validation Loss: 215.07127994846653\n",
      "Epoch 73/100, Training Loss: 73.5265121459961, Validation Loss: 216.57334536990604\n",
      "Epoch 74/100, Training Loss: 93.4451675415039, Validation Loss: 223.7976057310362\n",
      "Epoch 75/100, Training Loss: 125.44852447509766, Validation Loss: 220.4375833047403\n",
      "Epoch 76/100, Training Loss: 63.65861511230469, Validation Loss: 212.29998008109428\n",
      "Epoch 77/100, Training Loss: 70.80824279785156, Validation Loss: 211.53923529547615\n",
      "Epoch 78/100, Training Loss: 52.27487564086914, Validation Loss: 212.2455674970472\n",
      "Epoch 79/100, Training Loss: 67.53852081298828, Validation Loss: 215.29991230320286\n",
      "Epoch 80/100, Training Loss: 67.81652069091797, Validation Loss: 212.81749057254277\n",
      "Epoch 81/100, Training Loss: 48.36943435668945, Validation Loss: 224.9481238700248\n",
      "Epoch 82/100, Training Loss: 85.379150390625, Validation Loss: 213.42867444012617\n",
      "Epoch 83/100, Training Loss: 70.07113647460938, Validation Loss: 213.11666787637247\n",
      "Epoch 84/100, Training Loss: 78.00025177001953, Validation Loss: 216.9931388030181\n",
      "Epoch 85/100, Training Loss: 90.21602630615234, Validation Loss: 214.9241439613136\n",
      "Epoch 86/100, Training Loss: 47.5742301940918, Validation Loss: 216.72302969855232\n",
      "Epoch 87/100, Training Loss: 60.22378158569336, Validation Loss: 213.4319186648807\n",
      "Epoch 88/100, Training Loss: 59.324222564697266, Validation Loss: 215.7177619934082\n",
      "Epoch 89/100, Training Loss: 77.50032806396484, Validation Loss: 211.25953426876583\n",
      "Epoch 90/100, Training Loss: 119.7917251586914, Validation Loss: 220.337333967879\n",
      "Epoch 91/100, Training Loss: 99.86141967773438, Validation Loss: 214.15467894270614\n",
      "Epoch 92/100, Training Loss: 80.16543579101562, Validation Loss: 208.45207175177498\n",
      "Epoch 93/100, Training Loss: 54.827213287353516, Validation Loss: 212.8176647907979\n",
      "Epoch 94/100, Training Loss: 59.57404708862305, Validation Loss: 211.49546628384977\n",
      "Epoch 95/100, Training Loss: 53.41207504272461, Validation Loss: 212.25880269231024\n",
      "Epoch 96/100, Training Loss: 55.69236373901367, Validation Loss: 230.8341375608702\n",
      "Epoch 97/100, Training Loss: 50.74959945678711, Validation Loss: 221.02789442732526\n",
      "Epoch 98/100, Training Loss: 88.20696258544922, Validation Loss: 221.57916777327253\n",
      "Epoch 99/100, Training Loss: 60.28523254394531, Validation Loss: 209.17714866431984\n",
      "Epoch 100/100, Training Loss: 64.4151382446289, Validation Loss: 213.79924142167374\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "model = SolarPredictionNet(SEQUENCE_LENGTH, NUM_FEATURES)\n",
    "train_model(model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
