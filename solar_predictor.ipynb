{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_TO_KEEP = [\n",
    "    \"direct_rad:W\",\n",
    "    \"clear_sky_rad:W\",\n",
    "    \"diffuse_rad:W\",\n",
    "    \"direct_rad_1h:J\",\n",
    "    \"is_in_shadow:idx\",\n",
    "    \"clear_sky_energy_1h:J\",\n",
    "    \"diffuse_rad_1h:J\",\n",
    "    \"is_day:idx\",\n",
    "    \"sun_elevation:d\",\n",
    "    # \"ceiling_height_agl:m\",\n",
    "    # \"effective_cloud_cover:p\",\n",
    "    # \"visibility:m\",\n",
    "    # \"date_calc\",\n",
    "    \"pv_measurement\",\n",
    "]\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "NUM_FEATURES = len(COLUMNS_TO_KEEP) - 1  # -1 because pv_measurement is the target\n",
    "FEATURE_SIZE = 4  # 7 days of hourly data\n",
    "WEIGHT_DECAY = 0.01\n",
    "SEQUENCE_LENGTH = 7*24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, sequence_length):\n",
    "    \"\"\"\n",
    "    Converts time series data into overlapping sequences/windows.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    target_length = 1\n",
    "    for i in range(len(data) - sequence_length - target_length + 1):\n",
    "        seq = data[i : i + sequence_length]\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)\n",
    "\n",
    "class SolarPredictionNet(nn.Module):\n",
    "    def __init__(self, sequence_length, num_channels):\n",
    "        super(SolarPredictionNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(num_channels, 32, kernel_size=sequence_length, stride=1)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=1, stride=1)\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Parquet files\n",
    "df_data = pd.read_parquet(\"data/A/X_train_observed.parquet\")\n",
    "df_target = pd.read_parquet(\"data/A/train_targets.parquet\")\n",
    "\n",
    "# Merge the datasets\n",
    "df_merged = pd.merge(\n",
    "    df_data, df_target, left_on=\"date_forecast\", right_on=\"time\", how=\"inner\"\n",
    ")\n",
    "df_merged = df_merged[COLUMNS_TO_KEEP]\n",
    "\n",
    "# Separate the features and targets\n",
    "y = df_merged[\"pv_measurement\"]\n",
    "X = df_merged.drop(\"pv_measurement\", axis=1)\n",
    "\n",
    "# Convert dataframes to sequences\n",
    "X_sequences = create_sequences(X.values, SEQUENCE_LENGTH)\n",
    "# Adjust the sequence creation for y\n",
    "y_sequences = y.values[SEQUENCE_LENGTH-1:-1]  # Aligned with the end of each sequence and remove the last element\n",
    "\n",
    "# Create a custom dataset\n",
    "class SolarDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_sequences, y_sequences, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).transpose(1, 2)  # Adjust shape to [batch, channels, sequence]\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).transpose(1, 2)  # Adjust shape to [batch, channels, sequence]\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create datasets for training and validation\n",
    "train_dataset = SolarDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = SolarDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ... [Neural Network and Training code from previous messages]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader):\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Average validation loss\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Training Loss: {loss.item()}, Validation Loss: {val_loss}\"\n",
    "        )\n",
    "\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 184.2555694580078, Validation Loss: 278.23732110100826\n",
      "Epoch 2/100, Training Loss: 154.62493896484375, Validation Loss: 266.2208758379962\n",
      "Epoch 3/100, Training Loss: 238.4508056640625, Validation Loss: 296.5852339461043\n",
      "Epoch 4/100, Training Loss: 83.86500549316406, Validation Loss: 244.9417869774071\n",
      "Epoch 5/100, Training Loss: 69.76952362060547, Validation Loss: 246.78166905995963\n",
      "Epoch 6/100, Training Loss: 59.58860778808594, Validation Loss: 238.7703267277898\n",
      "Epoch 7/100, Training Loss: 79.11589813232422, Validation Loss: 248.5509638605891\n",
      "Epoch 8/100, Training Loss: 80.19047546386719, Validation Loss: 237.66624178499788\n",
      "Epoch 9/100, Training Loss: 110.00286865234375, Validation Loss: 259.6327291952597\n",
      "Epoch 10/100, Training Loss: 66.54064178466797, Validation Loss: 267.0486602577003\n",
      "Epoch 11/100, Training Loss: 97.6843490600586, Validation Loss: 263.64100828428525\n",
      "Epoch 12/100, Training Loss: 58.97007751464844, Validation Loss: 240.60029082427153\n",
      "Epoch 13/100, Training Loss: 31.06598663330078, Validation Loss: 248.23389428628457\n",
      "Epoch 14/100, Training Loss: 55.75119400024414, Validation Loss: 236.83748008109427\n",
      "Epoch 15/100, Training Loss: 57.5506591796875, Validation Loss: 243.1811904494827\n",
      "Epoch 16/100, Training Loss: 74.48992919921875, Validation Loss: 244.14361432049725\n",
      "Epoch 17/100, Training Loss: 41.1716423034668, Validation Loss: 241.0442833359177\n",
      "Epoch 18/100, Training Loss: 80.87430572509766, Validation Loss: 237.3793887988941\n",
      "Epoch 19/100, Training Loss: 65.13018798828125, Validation Loss: 257.09487453151394\n",
      "Epoch 20/100, Training Loss: 40.60845947265625, Validation Loss: 255.58995041718353\n",
      "Epoch 21/100, Training Loss: 69.29639434814453, Validation Loss: 248.1816244589316\n",
      "Epoch 22/100, Training Loss: 37.60445785522461, Validation Loss: 231.76756061863256\n",
      "Epoch 23/100, Training Loss: 67.86222839355469, Validation Loss: 233.77356596250792\n",
      "Epoch 24/100, Training Loss: 39.31563186645508, Validation Loss: 233.21306172963736\n",
      "Epoch 25/100, Training Loss: 99.15975189208984, Validation Loss: 237.2570108774546\n",
      "Epoch 26/100, Training Loss: 83.94574737548828, Validation Loss: 244.4593515756968\n",
      "Epoch 27/100, Training Loss: 40.4030647277832, Validation Loss: 229.2203074687236\n",
      "Epoch 28/100, Training Loss: 45.81916427612305, Validation Loss: 231.89376859922666\n",
      "Epoch 29/100, Training Loss: 22.969995498657227, Validation Loss: 233.37150717039367\n",
      "Epoch 30/100, Training Loss: 171.227783203125, Validation Loss: 261.21458653630435\n",
      "Epoch 31/100, Training Loss: 18.241775512695312, Validation Loss: 231.48154853614602\n",
      "Epoch 32/100, Training Loss: 46.23945999145508, Validation Loss: 234.92592004312053\n",
      "Epoch 33/100, Training Loss: 109.39115142822266, Validation Loss: 254.75830125550965\n",
      "Epoch 34/100, Training Loss: 34.57500076293945, Validation Loss: 233.4393827902304\n",
      "Epoch 35/100, Training Loss: 91.36065673828125, Validation Loss: 240.6239314723659\n",
      "Epoch 36/100, Training Loss: 26.434755325317383, Validation Loss: 232.19386636888657\n",
      "Epoch 37/100, Training Loss: 43.4710578918457, Validation Loss: 232.55798933699324\n",
      "Epoch 38/100, Training Loss: 54.0518684387207, Validation Loss: 230.81242906725083\n",
      "Epoch 39/100, Training Loss: 64.14791107177734, Validation Loss: 236.881788532154\n",
      "Epoch 40/100, Training Loss: 41.63392639160156, Validation Loss: 232.67354647662188\n",
      "Epoch 41/100, Training Loss: 28.23714256286621, Validation Loss: 239.34359745334933\n",
      "Epoch 42/100, Training Loss: 52.44862365722656, Validation Loss: 231.5726173194679\n",
      "Epoch 43/100, Training Loss: 42.820011138916016, Validation Loss: 235.37675383284287\n",
      "Epoch 44/100, Training Loss: 47.63532638549805, Validation Loss: 232.99307389130465\n",
      "Epoch 45/100, Training Loss: 74.7612533569336, Validation Loss: 236.1162872314453\n",
      "Epoch 46/100, Training Loss: 52.10552978515625, Validation Loss: 238.95167834307696\n",
      "Epoch 47/100, Training Loss: 51.97857666015625, Validation Loss: 229.32927206915778\n",
      "Epoch 48/100, Training Loss: 28.890913009643555, Validation Loss: 230.30831972070644\n",
      "Epoch 49/100, Training Loss: 35.62666702270508, Validation Loss: 227.7165612504289\n",
      "Epoch 50/100, Training Loss: 35.345497131347656, Validation Loss: 233.0776275016166\n",
      "Epoch 51/100, Training Loss: 21.640525817871094, Validation Loss: 229.13841259930584\n",
      "Epoch 52/100, Training Loss: 26.004365921020508, Validation Loss: 228.53067155786462\n",
      "Epoch 53/100, Training Loss: 78.09937286376953, Validation Loss: 247.57698753975532\n",
      "Epoch 54/100, Training Loss: 63.3736686706543, Validation Loss: 236.4364821356696\n",
      "Epoch 55/100, Training Loss: 50.09246826171875, Validation Loss: 236.34104373003984\n",
      "Epoch 56/100, Training Loss: 68.47881317138672, Validation Loss: 235.43643355498443\n",
      "Epoch 57/100, Training Loss: 36.44608688354492, Validation Loss: 237.11870814143\n",
      "Epoch 58/100, Training Loss: 75.80489349365234, Validation Loss: 239.37872157741236\n",
      "Epoch 59/100, Training Loss: 21.764795303344727, Validation Loss: 229.86774080637338\n",
      "Epoch 60/100, Training Loss: 36.30311965942383, Validation Loss: 230.83850262616133\n",
      "Epoch 61/100, Training Loss: 20.582664489746094, Validation Loss: 228.84364988997177\n",
      "Epoch 62/100, Training Loss: 44.057308197021484, Validation Loss: 230.20620238845413\n",
      "Epoch 63/100, Training Loss: 42.14409637451172, Validation Loss: 231.48096991874075\n",
      "Epoch 64/100, Training Loss: 115.06734466552734, Validation Loss: 246.2637989559689\n",
      "Epoch 65/100, Training Loss: 37.389671325683594, Validation Loss: 236.8798177976866\n",
      "Epoch 66/100, Training Loss: 56.61354064941406, Validation Loss: 235.35597884719436\n",
      "Epoch 67/100, Training Loss: 28.437231063842773, Validation Loss: 241.8326982549719\n",
      "Epoch 68/100, Training Loss: 111.0091781616211, Validation Loss: 254.02357942735827\n",
      "Epoch 69/100, Training Loss: 53.61552810668945, Validation Loss: 231.0317600353344\n",
      "Epoch 70/100, Training Loss: 28.38561248779297, Validation Loss: 234.03515519838075\n",
      "Epoch 71/100, Training Loss: 43.36942672729492, Validation Loss: 242.0185618013949\n",
      "Epoch 72/100, Training Loss: 71.37120819091797, Validation Loss: 235.9218615970096\n",
      "Epoch 73/100, Training Loss: 69.33574676513672, Validation Loss: 232.8653819934742\n",
      "Epoch 74/100, Training Loss: 44.545711517333984, Validation Loss: 234.95393416945998\n",
      "Epoch 75/100, Training Loss: 74.17493438720703, Validation Loss: 240.70652779244088\n",
      "Epoch 76/100, Training Loss: 75.3404312133789, Validation Loss: 237.6332823882232\n",
      "Epoch 77/100, Training Loss: 54.95158767700195, Validation Loss: 230.16462437397726\n",
      "Epoch 78/100, Training Loss: 17.75201416015625, Validation Loss: 227.04666075835357\n",
      "Epoch 79/100, Training Loss: 49.137516021728516, Validation Loss: 228.77144521764808\n",
      "Epoch 80/100, Training Loss: 17.45987892150879, Validation Loss: 225.2995132549389\n",
      "Epoch 81/100, Training Loss: 64.87945556640625, Validation Loss: 239.04059788471943\n",
      "Epoch 82/100, Training Loss: 52.07308578491211, Validation Loss: 236.5185789778426\n",
      "Epoch 83/100, Training Loss: 41.258548736572266, Validation Loss: 232.04107247429926\n",
      "Epoch 84/100, Training Loss: 29.896493911743164, Validation Loss: 227.0545040336815\n",
      "Epoch 85/100, Training Loss: 34.43299865722656, Validation Loss: 231.78366464150918\n",
      "Epoch 86/100, Training Loss: 24.416290283203125, Validation Loss: 227.51861141307933\n",
      "Epoch 87/100, Training Loss: 19.60340118408203, Validation Loss: 225.2984559342668\n",
      "Epoch 88/100, Training Loss: 64.60602569580078, Validation Loss: 242.9587829177444\n",
      "Epoch 89/100, Training Loss: 34.24685287475586, Validation Loss: 225.59489168734163\n",
      "Epoch 90/100, Training Loss: 44.06499099731445, Validation Loss: 235.84278199479385\n",
      "Epoch 91/100, Training Loss: 57.46889877319336, Validation Loss: 233.0843077582282\n",
      "Epoch 92/100, Training Loss: 29.06049346923828, Validation Loss: 235.03131955121015\n",
      "Epoch 93/100, Training Loss: 30.536191940307617, Validation Loss: 225.90957273534826\n",
      "Epoch 94/100, Training Loss: 82.26904296875, Validation Loss: 235.21893007433093\n",
      "Epoch 95/100, Training Loss: 54.70076370239258, Validation Loss: 229.5393897804054\n",
      "Epoch 96/100, Training Loss: 36.024436950683594, Validation Loss: 232.4123002542032\n",
      "Epoch 97/100, Training Loss: 22.450759887695312, Validation Loss: 228.4357234851734\n",
      "Epoch 98/100, Training Loss: 30.6923828125, Validation Loss: 229.1904660817739\n",
      "Epoch 99/100, Training Loss: 45.49042892456055, Validation Loss: 231.49119508073136\n",
      "Epoch 100/100, Training Loss: 61.42787551879883, Validation Loss: 237.802174934181\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "model = SolarPredictionNet(SEQUENCE_LENGTH, NUM_FEATURES)\n",
    "train_model(model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
