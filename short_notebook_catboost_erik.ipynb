{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in /opt/conda/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: autogluon in /opt/conda/lib/python3.10/site-packages (0.8.2)\n",
      "Requirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from catboost) (0.20.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from catboost) (3.7.3)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.10/site-packages (from catboost) (1.25.2)\n",
      "Requirement already satisfied: pandas>=0.24 in /opt/conda/lib/python3.10/site-packages (from catboost) (1.5.3)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from catboost) (1.11.3)\n",
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (from catboost) (5.18.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: autogluon.core==0.8.2 in /opt/conda/lib/python3.10/site-packages (from autogluon.core[all]==0.8.2->autogluon) (0.8.2)\n",
      "Requirement already satisfied: autogluon.features==0.8.2 in /opt/conda/lib/python3.10/site-packages (from autogluon) (0.8.2)\n",
      "Requirement already satisfied: autogluon.tabular==0.8.2 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular[all]==0.8.2->autogluon) (0.8.2)\n",
      "Requirement already satisfied: autogluon.multimodal==0.8.2 in /opt/conda/lib/python3.10/site-packages (from autogluon) (0.8.2)\n",
      "Requirement already satisfied: autogluon.timeseries==0.8.2 in /opt/conda/lib/python3.10/site-packages (from autogluon.timeseries[all]==0.8.2->autogluon) (0.8.2)\n",
      "Requirement already satisfied: networkx<4,>=3.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon) (3.2.1)\n",
      "Requirement already satisfied: tqdm<5,>=4.38 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon) (4.65.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon) (2.28.2)\n",
      "Requirement already satisfied: boto3<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon) (1.28.84)\n",
      "Requirement already satisfied: autogluon.common==0.8.2 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon) (0.8.2)\n",
      "Requirement already satisfied: hyperopt<0.2.8,>=0.2.7 in /opt/conda/lib/python3.10/site-packages (from autogluon.core[all]==0.8.2->autogluon) (0.2.7)\n",
      "Requirement already satisfied: ray<2.4,>=2.3 in /opt/conda/lib/python3.10/site-packages (from ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (2.3.1)\n",
      "Requirement already satisfied: pydantic<2.0,>=1.10.4 in /opt/conda/lib/python3.10/site-packages (from autogluon.core[all]==0.8.2->autogluon) (1.10.13)\n",
      "Requirement already satisfied: grpcio<=1.50.0,>=1.42.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.core[all]==0.8.2->autogluon) (1.50.0)\n",
      "Requirement already satisfied: Pillow<9.6,>=9.3 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==0.8.2->autogluon) (9.5.0)\n",
      "Requirement already satisfied: jsonschema<4.18,>=4.14 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==0.8.2->autogluon) (4.17.3)\n",
      "Requirement already satisfied: seqeval<1.3.0,>=1.2.2 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==0.8.2->autogluon) (1.2.2)\n",
      "Requirement already satisfied: evaluate<0.4.0,>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.3.0)\n",
      "Requirement already satisfied: accelerate<0.17,>=0.9 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.16.0)\n",
      "Requirement already satisfied: timm<0.10.0,>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.9.10)\n",
      "Requirement already satisfied: torch<1.14,>=1.9 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==0.8.2->autogluon) (1.13.1)\n",
      "Requirement already satisfied: torchvision<0.15.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.14.1)\n",
      "Requirement already satisfied: scikit-image<0.20.0,>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.19.3)\n",
      "Requirement already satisfied: pytorch-lightning<1.10.0,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==0.8.2->autogluon) (1.9.5)\n",
      "Requirement already satisfied: text-unidecode<1.4,>=1.3 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==0.8.2->autogluon) (1.3)\n",
      "Requirement already satisfied: torchmetrics<0.12.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.11.4)\n",
      "Requirement already satisfied: transformers<4.27.0,>=4.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.27.0,>=4.23.0->autogluon.multimodal==0.8.2->autogluon) (4.26.1)\n",
      "Requirement already satisfied: nptyping<2.5.0,>=1.4.4 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==0.8.2->autogluon) (2.4.1)\n",
      "Requirement already satisfied: omegaconf<2.3.0,>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==0.8.2->autogluon) (2.2.3)\n",
      "Requirement already satisfied: pytorch-metric-learning<2.0,>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==0.8.2->autogluon) (1.7.3)\n",
      "Requirement already satisfied: nlpaug<1.2.0,>=1.1.10 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==0.8.2->autogluon) (1.1.11)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.4.5 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==0.8.2->autogluon) (3.8.1)\n",
      "Requirement already satisfied: openmim<0.4.0,>=0.3.7 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.3.9)\n",
      "Requirement already satisfied: defusedxml<0.7.2,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.7.1)\n",
      "Requirement already satisfied: jinja2<3.2,>=3.0.3 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==0.8.2->autogluon) (3.1.2)\n",
      "Requirement already satisfied: tensorboard<3,>=2.9 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==0.8.2->autogluon) (2.15.1)\n",
      "Requirement already satisfied: pytesseract<0.3.11,>=0.3.9 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.3.10)\n",
      "Requirement already satisfied: lightgbm<3.4,>=3.3 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular[all]==0.8.2->autogluon) (3.3.5)\n",
      "Requirement already satisfied: xgboost<1.8,>=1.6 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular[all]==0.8.2->autogluon) (1.7.6)\n",
      "Requirement already satisfied: fastai<2.8,>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular[all]==0.8.2->autogluon) (2.7.13)\n",
      "Requirement already satisfied: statsmodels<0.15,>=0.13.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.timeseries==0.8.2->autogluon.timeseries[all]==0.8.2->autogluon) (0.14.0)\n",
      "Requirement already satisfied: gluonts<0.14,>=0.13.1 in /opt/conda/lib/python3.10/site-packages (from autogluon.timeseries==0.8.2->autogluon.timeseries[all]==0.8.2->autogluon) (0.13.7)\n",
      "Requirement already satisfied: statsforecast<1.5,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.timeseries==0.8.2->autogluon.timeseries[all]==0.8.2->autogluon) (1.4.0)\n",
      "Requirement already satisfied: mlforecast<0.7.4,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.timeseries==0.8.2->autogluon.timeseries[all]==0.8.2->autogluon) (0.7.3)\n",
      "Requirement already satisfied: ujson<6,>=5 in /opt/conda/lib/python3.10/site-packages (from autogluon.timeseries==0.8.2->autogluon.timeseries[all]==0.8.2->autogluon) (5.8.0)\n",
      "Requirement already satisfied: psutil<6,>=5.7.3 in /opt/conda/lib/python3.10/site-packages (from autogluon.common==0.8.2->autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon) (5.9.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from autogluon.common==0.8.2->autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon) (60.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2023.3.post1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (4.44.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (3.1.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly->catboost) (8.2.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate<0.17,>=0.9->autogluon.multimodal==0.8.2->autogluon) (6.0.1)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.84 in /opt/conda/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon) (1.31.84)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon) (0.7.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (2.14.6)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (0.3.7)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (0.19.0)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (0.18.0)\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (23.3.1)\n",
      "Requirement already satisfied: fastdownload<2,>=0.0.5 in /opt/conda/lib/python3.10/site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (0.0.7)\n",
      "Requirement already satisfied: fastcore<1.6,>=1.5.29 in /opt/conda/lib/python3.10/site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (1.5.29)\n",
      "Requirement already satisfied: fastprogress>=0.2.4 in /opt/conda/lib/python3.10/site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (1.0.3)\n",
      "Requirement already satisfied: spacy<4 in /opt/conda/lib/python3.10/site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (3.7.2)\n",
      "Requirement already satisfied: toolz~=0.10 in /opt/conda/lib/python3.10/site-packages (from gluonts<0.14,>=0.13.1->autogluon.timeseries==0.8.2->autogluon.timeseries[all]==0.8.2->autogluon) (0.12.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gluonts<0.14,>=0.13.1->autogluon.timeseries==0.8.2->autogluon.timeseries[all]==0.8.2->autogluon) (4.8.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.10/site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==0.8.2->autogluon) (0.18.3)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==0.8.2->autogluon) (3.0.0)\n",
      "Requirement already satisfied: py4j in /opt/conda/lib/python3.10/site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==0.8.2->autogluon) (0.10.9.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2<3.2,>=3.0.3->autogluon.multimodal==0.8.2->autogluon) (2.1.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema<4.18,>=4.14->autogluon.multimodal==0.8.2->autogluon) (23.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema<4.18,>=4.14->autogluon.multimodal==0.8.2->autogluon) (0.20.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from lightgbm<3.4,>=3.3->autogluon.tabular[all]==0.8.2->autogluon) (0.41.3)\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from mlforecast<0.7.4,>=0.7.0->autogluon.timeseries==0.8.2->autogluon.timeseries[all]==0.8.2->autogluon) (0.58.1)\n",
      "Requirement already satisfied: window-ops in /opt/conda/lib/python3.10/site-packages (from mlforecast<0.7.4,>=0.7.0->autogluon.timeseries==0.8.2->autogluon.timeseries[all]==0.8.2->autogluon) (0.0.14)\n",
      "Requirement already satisfied: gdown>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==0.8.2->autogluon) (4.7.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.4.5->autogluon.multimodal==0.8.2->autogluon) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.4.5->autogluon.multimodal==0.8.2->autogluon) (2023.10.3)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /opt/conda/lib/python3.10/site-packages (from omegaconf<2.3.0,>=2.1.1->autogluon.multimodal==0.8.2->autogluon) (4.9.3)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (0.4.6)\n",
      "Requirement already satisfied: model-index in /opt/conda/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (0.1.11)\n",
      "Requirement already satisfied: opendatalab in /opt/conda/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (0.0.10)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (13.4.2)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (0.9.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.6.0.post0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning<1.10.0,>=1.9.0->autogluon.multimodal==0.8.2->autogluon) (0.9.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from ray<2.4,>=2.3->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (3.13.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ray<2.4,>=2.3->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (1.0.7)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/lib/python3.10/site-packages (from ray<2.4,>=2.3->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (3.20.2)\n",
      "Requirement already satisfied: aiosignal in /opt/conda/lib/python3.10/site-packages (from ray<2.4,>=2.3->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (1.3.1)\n",
      "Requirement already satisfied: frozenlist in /opt/conda/lib/python3.10/site-packages (from ray<2.4,>=2.3->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (1.4.0)\n",
      "Requirement already satisfied: virtualenv>=20.0.24 in /opt/conda/lib/python3.10/site-packages (from ray<2.4,>=2.3->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (20.21.0)\n",
      "Requirement already satisfied: aiohttp>=3.7 in /opt/conda/lib/python3.10/site-packages (from ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (3.8.6)\n",
      "Requirement already satisfied: aiohttp-cors in /opt/conda/lib/python3.10/site-packages (from ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (0.7.0)\n",
      "Requirement already satisfied: colorful in /opt/conda/lib/python3.10/site-packages (from ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (0.5.5)\n",
      "Requirement already satisfied: py-spy>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (0.3.14)\n",
      "Requirement already satisfied: gpustat>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (1.0.0)\n",
      "Requirement already satisfied: opencensus in /opt/conda/lib/python3.10/site-packages (from ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (0.11.3)\n",
      "Requirement already satisfied: prometheus-client>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (0.18.0)\n",
      "Requirement already satisfied: smart-open in /opt/conda/lib/python3.10/site-packages (from ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (6.4.0)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in /opt/conda/lib/python3.10/site-packages (from ray[tune]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (2.6.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==0.8.2->autogluon.core[all]==0.8.2->autogluon) (2023.7.22)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image<0.20.0,>=0.19.1->autogluon.multimodal==0.8.2->autogluon) (2.32.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.10/site-packages (from scikit-image<0.20.0,>=0.19.1->autogluon.multimodal==0.8.2->autogluon) (2023.9.26)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image<0.20.0,>=0.19.1->autogluon.multimodal==0.8.2->autogluon) (1.4.1)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /opt/conda/lib/python3.10/site-packages (from statsmodels<0.15,>=0.13.0->autogluon.timeseries==0.8.2->autogluon.timeseries[all]==0.8.2->autogluon) (0.5.3)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (2.0.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (2.23.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (1.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (3.5.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (3.0.1)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm<0.10.0,>=0.9.2->autogluon.multimodal==0.8.2->autogluon) (0.4.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch<1.14,>=1.9->autogluon.multimodal==0.8.2->autogluon) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch<1.14,>=1.9->autogluon.multimodal==0.8.2->autogluon) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch<1.14,>=1.9->autogluon.multimodal==0.8.2->autogluon) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch<1.14,>=1.9->autogluon.multimodal==0.8.2->autogluon) (11.7.99)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers<4.27.0,>=4.23.0->transformers[sentencepiece]<4.27.0,>=4.23.0->autogluon.multimodal==0.8.2->autogluon) (0.13.3)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.27.0,>=4.23.0->autogluon.multimodal==0.8.2->autogluon) (0.1.99)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp>=3.7->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp>=3.7->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp>=3.7->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (1.9.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (14.0.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==0.8.2->autogluon) (4.12.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (1.3.1)\n",
      "Requirement already satisfied: nvidia-ml-py<=11.495.46,>=11.450.129 in /opt/conda/lib/python3.10/site-packages (from gpustat>=1.0.0->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (11.495.46)\n",
      "Requirement already satisfied: blessed>=1.17.1 in /opt/conda/lib/python3.10/site-packages (from gpustat>=1.0.0->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (1.20.0)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->mlforecast<0.7.4,>=0.7.0->autogluon.timeseries==0.8.2->autogluon.timeseries[all]==0.8.2->autogluon) (0.41.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (0.9.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (3.3.0)\n",
      "Requirement already satisfied: distlib<1,>=0.3.6 in /opt/conda/lib/python3.10/site-packages (from virtualenv>=20.0.24->ray<2.4,>=2.3->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (0.3.7)\n",
      "Requirement already satisfied: platformdirs<4,>=2.4 in /opt/conda/lib/python3.10/site-packages (from virtualenv>=20.0.24->ray<2.4,>=2.3->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (3.11.0)\n",
      "Requirement already satisfied: ordered-set in /opt/conda/lib/python3.10/site-packages (from model-index->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (4.1.0)\n",
      "Requirement already satisfied: opencensus-context>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from opencensus->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (0.1.3)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from opencensus->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (1.34.0)\n",
      "Requirement already satisfied: pycryptodome in /opt/conda/lib/python3.10/site-packages (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (3.19.0)\n",
      "Requirement already satisfied: openxlab in /opt/conda/lib/python3.10/site-packages (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (0.0.28)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (2.16.1)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in /opt/conda/lib/python3.10/site-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (0.2.9)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]<2.4,>=2.3; extra == \"all\"->autogluon.core[all]==0.8.2->autogluon) (1.61.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (3.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (0.1.3)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (0.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==0.8.2->autogluon) (2.5)\n",
      "Requirement already satisfied: oss2~=2.17.0 in /opt/conda/lib/python3.10/site-packages (from openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (2.17.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==0.8.2->autogluon) (1.7.1)\n",
      "Requirement already satisfied: crcmod>=1.7 in /opt/conda/lib/python3.10/site-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (1.7)\n",
      "Requirement already satisfied: aliyun-python-sdk-kms>=2.4.1 in /opt/conda/lib/python3.10/site-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (2.16.2)\n",
      "Requirement already satisfied: aliyun-python-sdk-core>=2.13.12 in /opt/conda/lib/python3.10/site-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (2.14.0)\n",
      "Requirement already satisfied: cryptography>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (41.0.5)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=2.6.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.6.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install catboost scikit-learn autogluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autogluon Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import autogluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constans and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = [\"A\", \"B\", \"C\"]\n",
    "features_order = []\n",
    "\n",
    "LAGGED_COLUMNS_TO_KEEP = [\n",
    "    # 'direct_rad:W_lag_1h', \n",
    "    'direct_rad:W_lag_forward_1h', \n",
    "    # 'clear_sky_rad:W_lag_1h', \n",
    "    'clear_sky_rad:W_lag_forward_1h', \n",
    "    # 'diffuse_rad:W_lag_1h', \n",
    "    'diffuse_rad:W_lag_forward_1h', \n",
    "    # 'direct_rad_1h:J_lag_1h', \n",
    "    'direct_rad_1h:J_lag_forward_1h', \n",
    "    # 'is_in_shadow:idx_lag_1h', \n",
    "    'is_in_shadow:idx_lag_forward_1h', \n",
    "    # 'clear_sky_energy_1h:J_lag_1h', \n",
    "    'clear_sky_energy_1h:J_lag_forward_1h', \n",
    "    # 'effective_cloud_cover:p_lag_1h', \n",
    "    'effective_cloud_cover:p_lag_forward_1h', \n",
    "    # 'visibility:m_lag_1h', \n",
    "    'visibility:m_lag_forward_1h', \n",
    "    # 'total_cloud_cover:p_lag_1h', \n",
    "    'total_cloud_cover:p_lag_forward_1h', \n",
    "\n",
    "\n",
    "    # 'direct_rad:W_lag_2h', \n",
    "    # 'direct_rad:W_lag_forward_2h', \n",
    "    # 'clear_sky_rad:W_lag_2h', \n",
    "    # 'clear_sky_rad:W_lag_forward_2h', \n",
    "    # 'diffuse_rad:W_lag_2h', \n",
    "    # 'diffuse_rad:W_lag_forward_2h', \n",
    "    # 'direct_rad_1h:J_lag_2h', \n",
    "    # 'direct_rad_1h:J_lag_forward_2h', \n",
    "    # 'is_in_shadow:idx_lag_2h', \n",
    "    # 'is_in_shadow:idx_lag_forward_2h', \n",
    "    # 'clear_sky_energy_1h:J_lag_2h', \n",
    "    # 'clear_sky_energy_1h:J_lag_forward_2h', \n",
    "    # 'effective_cloud_cover:p_lag_2h', \n",
    "    # 'effective_cloud_cover:p_lag_forward_2h', \n",
    "    # 'visibility:m_lag_2h', \n",
    "    # 'visibility:m_lag_forward_2h', \n",
    "    # 'total_cloud_cover:p_lag_2h', \n",
    "    # 'total_cloud_cover:p_lag_forward_2h', \n",
    "\n",
    "    # 'direct_rad:W_lag_3h', \n",
    "    # 'direct_rad:W_lag_forward_3h', \n",
    "    # 'clear_sky_rad:W_lag_3h', \n",
    "    # 'clear_sky_rad:W_lag_forward_3h', \n",
    "    # 'diffuse_rad:W_lag_3h', \n",
    "    # 'diffuse_rad:W_lag_forward_3h', \n",
    "    # 'direct_rad_1h:J_lag_3h', \n",
    "    # 'direct_rad_1h:J_lag_forward_3h', \n",
    "    # 'is_in_shadow:idx_lag_3h', \n",
    "    # 'is_in_shadow:idx_lag_forward_3h', \n",
    "    # 'clear_sky_energy_1h:J_lag_3h', \n",
    "    # 'clear_sky_energy_1h:J_lag_forward_3h', \n",
    "    # 'effective_cloud_cover:p_lag_3h', \n",
    "    # 'effective_cloud_cover:p_lag_forward_3h', \n",
    "    # 'visibility:m_lag_3h', \n",
    "    # 'visibility:m_lag_forward_3h', \n",
    "    # 'total_cloud_cover:p_lag_3h', \n",
    "    # 'total_cloud_cover:p_lag_forward_3h'\n",
    "]\n",
    "\n",
    "CUSTOM_COLUMNS_TO_KEEP = [\n",
    "    \"hour_cos\",\n",
    "    \"hour_sin\",\n",
    "    \"month_sin\",\n",
    "    \"month_cos\",\n",
    "    \"day-of-year\",\n",
    "    \"hours_since_forecast\"\n",
    "]\n",
    "\n",
    "WEATHER_FEATURES = [\n",
    "    \"direct_rad:W\",\n",
    "    \"clear_sky_rad:W\",\n",
    "    \"diffuse_rad:W\",\n",
    "    \"direct_rad_1h:J\",\n",
    "    \"is_in_shadow:idx\",\n",
    "    \"clear_sky_energy_1h:J\",\n",
    "    \"effective_cloud_cover:p\",\n",
    "    \"visibility:m\",\n",
    "    \"total_cloud_cover:p\",\n",
    "]\n",
    "\n",
    "\n",
    "TEST_COLUMNS_TO_KEEP = [\n",
    "    \"direct_rad:W\",\n",
    "    \"clear_sky_rad:W\",\n",
    "    \"diffuse_rad:W\",\n",
    "    \"direct_rad_1h:J\",\n",
    "    \"is_in_shadow:idx\",\n",
    "    \"clear_sky_energy_1h:J\",\n",
    "    \"diffuse_rad_1h:J\",\n",
    "    \"is_day:idx\",\n",
    "    \"sun_elevation:d\",\n",
    "    \"ceiling_height_agl:m\",\n",
    "    \"effective_cloud_cover:p\",\n",
    "    \"visibility:m\",\n",
    "    \"total_cloud_cover:p\",\n",
    "    \"air_density_2m:kgm3\",\n",
    "    \"wind_speed_v_10m:ms\",\n",
    "    \"dew_point_2m:K\",\n",
    "    \"wind_speed_u_10m:ms\",\n",
    "    \"t_1000hPa:K\",\n",
    "    \"absolute_humidity_2m:gm3\",\n",
    "    \"snow_water:kgm2\",\n",
    "    \"relative_humidity_1000hPa:p\",\n",
    "    \"fresh_snow_24h:cm\",\n",
    "    \"cloud_base_agl:m\",\n",
    "    \"fresh_snow_12h:cm\",\n",
    "    \"snow_depth:cm\",\n",
    "    \"dew_or_rime:idx\",\n",
    "    \"fresh_snow_6h:cm\",\n",
    "    \"super_cooled_liquid_water:kgm2\",\n",
    "    \"fresh_snow_3h:cm\",\n",
    "    \"rain_water:kgm2\",\n",
    "    \"precip_type_5min:idx\",\n",
    "    \"precip_5min:mm\",\n",
    "    \"fresh_snow_1h:cm\",\n",
    "    \"sun_azimuth:d\",\n",
    "    \"msl_pressure:hPa\",\n",
    "    \"pressure_100m:hPa\",\n",
    "    \"pressure_50m:hPa\",\n",
    "    \"sfc_pressure:hPa\",\n",
    "    \"prob_rime:p\",\n",
    "    \"wind_speed_10m:ms\",\n",
    "    # \"elevation:m\",\n",
    "    # \"snow_density:kgm3\",\n",
    "    # \"snow_drift:idx\",\n",
    "    \"snow_melt_10min:mm\",\n",
    "    \"wind_speed_w_1000hPa:ms\",\n",
    "    \"observed_or_estimated\"\n",
    "    # \"location_A\",\n",
    "    # \"location_B\",\n",
    "    # \"location_C\",\n",
    "    # \"date_calc\",\n",
    "] + CUSTOM_COLUMNS_TO_KEEP  +  LAGGED_COLUMNS_TO_KEEP\n",
    "\n",
    "COLUMNS_TO_KEEP = TEST_COLUMNS_TO_KEEP + [\"pv_measurement\"]\n",
    "\n",
    "\n",
    "\n",
    "def create_weather_lagged_features(df, weather_features):\n",
    "    # Choose the weather features for which you want to create lagged versions\n",
    "    for feature in weather_features:\n",
    "        # Assuming hourly data, adjust the lags for your specific dataset\n",
    "        # Creating lagged features for 1 hour, 1 day, and 1 week\n",
    "        # df[f'{feature}_lag_1h'] = df[feature].shift(1)\n",
    "        # df[f'{feature}_lag_2h'] = df[feature].shift(2)\n",
    "        # df[f'{feature}_lag_3h'] = df[feature].shift(3)\n",
    "\n",
    "        df[f'{feature}_lag_forward_1h'] = df[feature].shift(-1)\n",
    "        # df[f'{feature}_lag_forward_2h'] = df[feature].shift(-2)\n",
    "        # df[f'{feature}_lag_forward_3h'] = df[feature].shift(-3)\n",
    "        # df[f'{feature}_lag_24h'] = df[feature].shift(24*4)\n",
    "        # df[f'{feature}_lag_168h'] = df[feature].shift(24 * 7 * 4 * 365)\n",
    "        # df[f'{feature}_front_lag_1h'] = df[feature].shift(-4)\n",
    "        # df[f'{feature}_front_lag_24h'] = df[feature].shift(-24*4)\n",
    "\n",
    "\n",
    "    # Handling edges by filling NaNs with appropriate values or dropping them\n",
    "    # You may choose to fill with zeroes or interpolate, based on what makes more sense for your data\n",
    "    # df.fillna(method='ffill', inplace=True)  # Forward fill  # Autogluon should handle this for us.\n",
    "    # df.fillna(method='bfill', inplace=True)  # Backward fill  # Autogluon should handle this for us.\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "B_SCALE_VALUE = 6.3\n",
    "C_SCALE_VALUE = 8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_custom_fields(df):\n",
    "     df['hour_sin'] = np.sin(2 * np.pi * df['date_forecast'].dt.hour / 24)\n",
    "     df['hour_cos'] = np.cos(2 * np.pi * df['date_forecast'].dt.hour / 24)\n",
    "\n",
    "     df['month_sin'] = np.sin(2 * np.pi * df['date_forecast'].dt.month / 12)\n",
    "     df['month_cos'] = np.cos(2 * np.pi * df['date_forecast'].dt.month / 12)\n",
    "     df['day-of-year'] = df['date_forecast'].dt.dayofyear\n",
    "     return df\n",
    "\n",
    "def add_calc_date(df_observed, df_estimated, df_test):\n",
    "    # Function to calculate the difference in hours\n",
    "    def calculate_hour_difference(row):\n",
    "        diff = row['date_calc'] - row['date_forecast']\n",
    "        return diff.total_seconds() / 3600  # Convert difference to hours\n",
    "\n",
    "    # Apply the function to calculate the hour difference for df_estimated and df_test\n",
    "    df_estimated['hours_since_forecast'] = df_estimated.apply(calculate_hour_difference, axis=1)\n",
    "    df_test['hours_since_forecast'] = df_test.apply(calculate_hour_difference, axis=1)\n",
    "\n",
    "    # Fill in zero for df_observed\n",
    "    df_observed['hours_since_forecast'] = 0\n",
    "\n",
    "    return df_observed, df_estimated, df_test\n",
    "\n",
    "\n",
    "def remove_outliers(df):\n",
    "    # Use a mask to filter out the rows where rolling std is zero but keep the rows where the value itself is zero\n",
    "    # Because some places in the data, the pv-measurements are messed up and are repeating.\n",
    "    mask = (df['pv_measurement'].rolling(2).std() == 0) & (df['pv_measurement'] != 0)\n",
    "    df[mask] = np.NaN  # Put this to NaN and hope autoGluon Handles.\n",
    "    return df\n",
    "\n",
    "def resample_add_data(df, is_test_data):\n",
    "    df = add_custom_fields(df)\n",
    "    df.set_index('date_forecast', inplace=True)\n",
    "    df = df.resample('1H').mean()\n",
    "    \n",
    "    # Remove empty dates if test data\n",
    "    if is_test_data:\n",
    "        non_nan_threshold = len(df.columns) // 2\n",
    "        df.dropna(thresh=non_nan_threshold, inplace=True)\n",
    "\n",
    "    # df.interpolate(method=\"linear\", inplace=True)  # Autogluon should handle this for us.\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_location_feature(X, location):\n",
    "      # Treat location as a categorical feature by converting it to a category type\n",
    "    X['location'] = location\n",
    "    X['location'] = X['location'].astype(str)  # Convert to string if 'location' is not an int\n",
    "    # X['dew_or_rime:idx'] = X['dew_or_rime:idx'].astype(str)\n",
    "    # X['is_day:idx'] = X['is_day:idx'].astype(str)\n",
    "    # X['is_in_shadow:idx'] = X['is_in_shadow:idx'].astype(str)\n",
    "    # categorical_columns = ['location', 'dew_or_rime:idx', 'is_day:idx', 'is_in_shadow:idx']\n",
    "\n",
    "    # # Before filling NaN values, add 'missing' as a category for each categorical column.\n",
    "    # for column in categorical_columns:\n",
    "    #     X[column] = X[column].astype('category')  # Ensure the column is of type 'category'.\n",
    "    #     if 'missing' not in X[column].cat.categories:\n",
    "    #         X[column] = X[column].cat.add_categories(['missing'])  # Add 'missing' as a new category.\n",
    "    #     X[column] = X[column].fillna('missing')\n",
    "    # X['location'] = X['location'].astype('category')\n",
    "    # X['dew_or_rime:idx'] = X['dew_or_rime:idx'].astype('category')\n",
    "    # X['is_day:idx'] = X['is_day:idx'].astype('category')\n",
    "    # X['is_in_shadow:idx'] = X['is_in_shadow:idx'].astype('category')\n",
    "    \n",
    "    return X\n",
    "\n",
    "def add_scaling(X_test, X_training, location):\n",
    "    global scalers\n",
    "    continuous_columns = X_training.select_dtypes(include=['float32', 'int32']).columns\n",
    "    if location not in scalers:\n",
    "        scalers[location] = MinMaxScaler()\n",
    "    X_training[continuous_columns] = scalers[location].fit_transform(X_training[continuous_columns])\n",
    "    \n",
    "    X_test[continuous_columns] = scalers[location].transform(X_test[continuous_columns])\n",
    "\n",
    "    return X_test, X_training\n",
    "\n",
    "# Skip this as we have hours since forecast as a feature.\n",
    "# Deprecated as the concat is moved to main function.\n",
    "def make_observed_and_estimated_category(df_observed, df_estimated, df_test):\n",
    "     # Hot encode in wether observed or estimated\n",
    "    df_observed['observed_or_estimated'] = 'observed'\n",
    "    df_estimated['observed_or_estimated'] = 'estimated'\n",
    "    df_test['observed_or_estimated'] = 'estimated'\n",
    "    # Concatenate observed and estimated\n",
    "    df_training = pd.concat([df_observed, df_estimated], axis=0).sort_values(by=\"date_forecast\")\n",
    "    df_training['observed_or_estimated'] = df_training['observed_or_estimated'].astype('category')\n",
    "    df_test['observed_or_estimated'] = df_test['observed_or_estimated'].astype('category')\n",
    "\n",
    "    return df_training, df_test\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_101452/2117605410.py:36: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.resample('1H').mean()\n",
      "/var/tmp/ipykernel_101452/2117605410.py:36: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.resample('1H').mean()\n",
      "/var/tmp/ipykernel_101452/2117605410.py:36: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.resample('1H').mean()\n",
      "/var/tmp/ipykernel_101452/2117605410.py:36: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.resample('1H').mean()\n",
      "/var/tmp/ipykernel_101452/2117605410.py:36: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.resample('1H').mean()\n",
      "/var/tmp/ipykernel_101452/2117605410.py:36: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.resample('1H').mean()\n",
      "/var/tmp/ipykernel_101452/2117605410.py:36: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.resample('1H').mean()\n",
      "/var/tmp/ipykernel_101452/2117605410.py:36: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.resample('1H').mean()\n",
      "/var/tmp/ipykernel_101452/2117605410.py:36: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.resample('1H').mean()\n",
      "/var/tmp/ipykernel_101452/2117605410.py:36: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.resample('1H').mean()\n",
      "/var/tmp/ipykernel_101452/2117605410.py:36: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.resample('1H').mean()\n",
      "/var/tmp/ipykernel_101452/2117605410.py:36: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.resample('1H').mean()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize a dictionary to hold the scalers for each location\n",
    "\n",
    "scalers = {}\n",
    "\n",
    "\n",
    "def prepare_data(location):\n",
    "    # Load data\n",
    "    scaling = False  # Set scaling to True to enable individual scaling for each location\n",
    "    global scalers\n",
    "    global scale_target \n",
    "    scale_target = False\n",
    "\n",
    "    # Load training data\n",
    "    df_observed = pd.read_parquet(f\"data/{location}/X_train_observed.parquet\")\n",
    "    df_estimated = pd.read_parquet(f\"data/{location}/X_train_estimated.parquet\")\n",
    "    df_target = pd.read_parquet(f\"data/{location}/train_targets.parquet\")\n",
    "    # drop nan values in target data, pv measurement\n",
    "    df_target.dropna(inplace=True)\n",
    "\n",
    "   \n",
    "    # Load test data\n",
    "    df_test = pd.read_parquet(f\"data/{location}/X_test_estimated.parquet\")\n",
    "\n",
    "   \n",
    "    # Add calculated date\n",
    "    df_observed, df_estimated, df_test = add_calc_date(df_observed, df_estimated, df_test)\n",
    "    \n",
    "\n",
    "    # Resample and add custom fields\n",
    "\n",
    "    df_observed = resample_add_data(df_observed, False)\n",
    "    df_estimated = resample_add_data(df_estimated, False)\n",
    "    df_test = resample_add_data(df_test, True)\n",
    "\n",
    "    df_training, df_test = make_observed_and_estimated_category(df_observed, df_estimated, df_test)\n",
    "    \n",
    "\n",
    "    # Autogluon should scale for us.\n",
    "    if scale_target:\n",
    "        if location == \"B\":\n",
    "            df_target[\"pv_measurement\"] = df_target[\"pv_measurement\"] * B_SCALE_VALUE\n",
    "        elif location == \"C\":\n",
    "            df_target[\"pv_measurement\"] = df_target[\"pv_measurement\"] * C_SCALE_VALUE\n",
    "    \n",
    "    # Merge training with target data\n",
    "    df_training = pd.merge(df_training, df_target, left_on=\"date_forecast\", right_on=\"time\", how=\"inner\")\n",
    "    \n",
    "    # Create lagged features and remove outliers training\n",
    "    df_training = create_weather_lagged_features(df_training, WEATHER_FEATURES)\n",
    "    df_training = df_training[COLUMNS_TO_KEEP]\n",
    "    df_training = remove_outliers(df_training)\n",
    "\n",
    "    df_test = create_weather_lagged_features(df_test, WEATHER_FEATURES)\n",
    "    df_test = df_test[TEST_COLUMNS_TO_KEEP]\n",
    "\n",
    "\n",
    "    # Add categories\n",
    "    df_training = add_location_feature(df_training, location)\n",
    "    X_test = add_location_feature(df_test, location)\n",
    "    \n",
    "    # Add scaling\n",
    "    if scaling:\n",
    "        X_test, X_training = add_scaling(X_test, X_training, location)\n",
    "    df_test.reset_index(inplace=True)\n",
    "    df_test.drop(columns=[\"date_forecast\"], inplace=True)\n",
    "    # y_training = np.log1p(y_training)\n",
    "    return df_training, X_test\n",
    "\n",
    "\n",
    "\n",
    "# Use prepare_data function\n",
    "\n",
    "combined_df_train = []\n",
    "combined_df_test = []\n",
    "combined_df_validation = []\n",
    "\n",
    "for location in locations:\n",
    "    # Prepare the training data\n",
    "    X_training, X_test = prepare_data(location)\n",
    "\n",
    "    df_training, df_test = prepare_data(location)\n",
    "    # split df training at \n",
    "    X_training, X_validation = train_test_split(df_training[df_training[\"observed_or_estimated\"] == \"estimated\"], test_size=1440, shuffle=True)\n",
    "    X_training = pd.concat([X_training, df_training[df_training[\"observed_or_estimated\"] == \"observed\"]])\n",
    "    X_training = shuffle(X_training)\n",
    "    \n",
    "    combined_df_train.append(X_training)\n",
    "    combined_df_validation.append(X_validation)\n",
    "\n",
    "    combined_df_test.append(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of these hyperparameters have been found by experimenting with some standard parameters in AutoGluon, and then only using the best ones for each location to make the train time shorter\n",
    "lgbmXT = {'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}\n",
    "customlgbm = {'extra_trees': True, 'feature_fraction': 0.8, 'learning_rate': 0.02, 'min_data_in_leaf': 3, 'num_leaves': 21, 'ag_args': {'name_suffix': 'custom2'}}\n",
    "\n",
    "custom_fastai = {'bs': 1024, 'emb_drop': 0.6, 'epochs': 48, 'layers': [200, 100, 50], 'lr': 0.008, 'ps': 0.09, 'ag_args': {'name_suffix': '_custom'}}\n",
    "\n",
    "hyperparameters_a = {\n",
    "    'NN_TORCH': {},\n",
    "    'GBM': [lgbmXT, customlgbm, 'GBMLarge'],\n",
    "    'FASTAI': [custom_fastai],\n",
    "    'CAT': {},\n",
    "}\n",
    "hyperparameters_b_c = {\n",
    "    'NN_TORCH': {},\n",
    "    'GBM': [customlgbm],\n",
    "    'KNN': [{'weights': 'uniform'}],\n",
    "    'FASTAI': [{}, custom_fastai],\n",
    "    'CAT': {},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Presets specified: ['experimental_zeroshot_hpo_hybrid']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=2, num_bag_sets=2\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"autogluon_models/shortnotebook_modelA_gluon/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Debian 5.10.197-1 (2023-09-29)\n",
      "Disk Space Avail:   97.09 GB / 105.09 GB (92.4%)\n",
      "Train Data Rows:    32643\n",
      "Train Data Columns: 59\n",
      "Tuning Data Rows:    1440\n",
      "Tuning Data Columns: 59\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (5733.42, 0.0, 644.11247, 1175.38916)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5508.78 MB\n",
      "\tTrain Data (Original)  Memory Usage: 10.6 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['location']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  1 | ['observed_or_estimated']\n",
      "\t\t('float', [])    : 57 | ['direct_rad:W', 'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad_1h:J', 'is_in_shadow:idx', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 57 | ['direct_rad:W', 'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad_1h:J', 'is_in_shadow:idx', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['observed_or_estimated']\n",
      "\t0.2s = Fit runtime\n",
      "\t58 features in original data used to generate 58 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 8.62 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.18s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "use_bag_holdout=True, will use tuning_data as holdout (will not be used for early stopping).\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {'extra_trees': True, 'feature_fraction': 0.8, 'learning_rate': 0.02, 'min_data_in_leaf': 3, 'num_leaves': 21, 'ag_args': {'name_suffix': 'custom2'}}, 'GBMLarge'],\n",
      "\t'FASTAI': [{'bs': 1024, 'emb_drop': 0.6, 'epochs': 48, 'layers': [200, 100, 50], 'lr': 0.008, 'ps': 0.09, 'ag_args': {'name_suffix': '_custom'}}],\n",
      "\t'CAT': {},\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 6 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-92.2755\t = Validation score   (-mean_absolute_error)\n",
      "\t40.98s\t = Training   runtime\n",
      "\t71.25s\t = Validation runtime\n",
      "Fitting model: LightGBMcustom2_BAG_L1 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-92.7779\t = Validation score   (-mean_absolute_error)\n",
      "\t26.06s\t = Training   runtime\n",
      "\t38.18s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-100.9965\t = Validation score   (-mean_absolute_error)\n",
      "\t234.0s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_custom_BAG_L1 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-104.4104\t = Validation score   (-mean_absolute_error)\n",
      "\t31.2s\t = Training   runtime\n",
      "\t0.57s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-94.0872\t = Validation score   (-mean_absolute_error)\n",
      "\t75.54s\t = Training   runtime\n",
      "\t0.71s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-100.7055\t = Validation score   (-mean_absolute_error)\n",
      "\t170.35s\t = Training   runtime\n",
      "\t104.46s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-88.9446\t = Validation score   (-mean_absolute_error)\n",
      "\t0.18s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 6 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-90.7169\t = Validation score   (-mean_absolute_error)\n",
      "\t2.49s\t = Training   runtime\n",
      "\t0.53s\t = Validation runtime\n",
      "Fitting model: LightGBMcustom2_BAG_L2 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-89.3018\t = Validation score   (-mean_absolute_error)\n",
      "\t10.04s\t = Training   runtime\n",
      "\t3.36s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-92.1657\t = Validation score   (-mean_absolute_error)\n",
      "\t5.35s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_custom_BAG_L2 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-96.3479\t = Validation score   (-mean_absolute_error)\n",
      "\t30.27s\t = Training   runtime\n",
      "\t0.5s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-85.6316\t = Validation score   (-mean_absolute_error)\n",
      "\t40.46s\t = Training   runtime\n",
      "\t0.75s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-90.4232\t = Validation score   (-mean_absolute_error)\n",
      "\t11.17s\t = Training   runtime\n",
      "\t1.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ...\n",
      "\t-85.1565\t = Validation score   (-mean_absolute_error)\n",
      "\t0.18s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 782.4s ... Best model: \"WeightedEnsemble_L3\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"autogluon_models/shortnotebook_modelA_gluon/\")\n",
      "Presets specified: ['experimental_zeroshot_hpo_hybrid']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=2, num_bag_sets=2\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"autogluon_models/shortnotebook_modelB_gluon/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Debian 5.10.197-1 (2023-09-29)\n",
      "Disk Space Avail:   96.36 GB / 105.09 GB (91.7%)\n",
      "Train Data Rows:    28151\n",
      "Train Data Columns: 59\n",
      "Tuning Data Rows:    1440\n",
      "Tuning Data Columns: 59\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (1152.3, -0.0, 95.72913, 203.98474)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4485.91 MB\n",
      "\tTrain Data (Original)  Memory Usage: 9.2 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['location']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  1 | ['observed_or_estimated']\n",
      "\t\t('float', [])    : 57 | ['direct_rad:W', 'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad_1h:J', 'is_in_shadow:idx', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 57 | ['direct_rad:W', 'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad_1h:J', 'is_in_shadow:idx', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['observed_or_estimated']\n",
      "\t0.1s = Fit runtime\n",
      "\t58 features in original data used to generate 58 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 7.49 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.18s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "use_bag_holdout=True, will use tuning_data as holdout (will not be used for early stopping).\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'feature_fraction': 0.8, 'learning_rate': 0.02, 'min_data_in_leaf': 3, 'num_leaves': 21, 'ag_args': {'name_suffix': 'custom2'}}],\n",
      "\t'KNN': [{'weights': 'uniform'}],\n",
      "\t'FASTAI': [{}, {'bs': 1024, 'emb_drop': 0.6, 'epochs': 48, 'layers': [200, 100, 50], 'lr': 0.008, 'ps': 0.09, 'ag_args': {'name_suffix': '_custom'}}],\n",
      "\t'CAT': {},\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 6 L1 models ...\n",
      "Fitting model: KNeighbors_BAG_L1 ...\n",
      "\t-18.394\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t1.12s\t = Validation runtime\n",
      "Fitting model: LightGBMcustom2_BAG_L1 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-11.5903\t = Validation score   (-mean_absolute_error)\n",
      "\t25.68s\t = Training   runtime\n",
      "\t30.18s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.0229\t = Validation score   (-mean_absolute_error)\n",
      "\t227.96s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-14.0381\t = Validation score   (-mean_absolute_error)\n",
      "\t31.57s\t = Training   runtime\n",
      "\t0.73s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_custom_BAG_L1 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.6593\t = Validation score   (-mean_absolute_error)\n",
      "\t25.95s\t = Training   runtime\n",
      "\t0.52s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-11.4783\t = Validation score   (-mean_absolute_error)\n",
      "\t97.17s\t = Training   runtime\n",
      "\t0.56s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-11.02\t = Validation score   (-mean_absolute_error)\n",
      "\t0.16s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 5 L2 models ...\n",
      "Fitting model: LightGBMcustom2_BAG_L2 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-10.6463\t = Validation score   (-mean_absolute_error)\n",
      "\t26.69s\t = Training   runtime\n",
      "\t28.05s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-11.6173\t = Validation score   (-mean_absolute_error)\n",
      "\t10.37s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-11.6015\t = Validation score   (-mean_absolute_error)\n",
      "\t31.42s\t = Training   runtime\n",
      "\t0.8s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_custom_BAG_L2 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-11.9488\t = Validation score   (-mean_absolute_error)\n",
      "\t26.87s\t = Training   runtime\n",
      "\t0.49s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-11.1203\t = Validation score   (-mean_absolute_error)\n",
      "\t51.58s\t = Training   runtime\n",
      "\t0.68s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ...\n",
      "\t-10.5983\t = Validation score   (-mean_absolute_error)\n",
      "\t0.14s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 590.21s ... Best model: \"WeightedEnsemble_L3\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"autogluon_models/shortnotebook_modelB_gluon/\")\n",
      "Presets specified: ['experimental_zeroshot_hpo_hybrid']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=2, num_bag_sets=2\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"autogluon_models/shortnotebook_modelC_gluon/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Debian 5.10.197-1 (2023-09-29)\n",
      "Disk Space Avail:   96.13 GB / 105.09 GB (91.5%)\n",
      "Train Data Rows:    24195\n",
      "Train Data Columns: 59\n",
      "Tuning Data Rows:    1440\n",
      "Tuning Data Columns: 59\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and label-values can't be converted to int).\n",
      "\tLabel info (max, min, mean, stddev): (999.6, 0.0, 79.15182, 168.14557)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4490.41 MB\n",
      "\tTrain Data (Original)  Memory Usage: 7.97 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['location']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  1 | ['observed_or_estimated']\n",
      "\t\t('float', [])    : 57 | ['direct_rad:W', 'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad_1h:J', 'is_in_shadow:idx', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 57 | ['direct_rad:W', 'clear_sky_rad:W', 'diffuse_rad:W', 'direct_rad_1h:J', 'is_in_shadow:idx', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['observed_or_estimated']\n",
      "\t0.1s = Fit runtime\n",
      "\t58 features in original data used to generate 58 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 6.49 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.14s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "use_bag_holdout=True, will use tuning_data as holdout (will not be used for early stopping).\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'feature_fraction': 0.8, 'learning_rate': 0.02, 'min_data_in_leaf': 3, 'num_leaves': 21, 'ag_args': {'name_suffix': 'custom2'}}],\n",
      "\t'KNN': [{'weights': 'uniform'}],\n",
      "\t'FASTAI': [{}, {'bs': 1024, 'emb_drop': 0.6, 'epochs': 48, 'layers': [200, 100, 50], 'lr': 0.008, 'ps': 0.09, 'ag_args': {'name_suffix': '_custom'}}],\n",
      "\t'CAT': {},\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 6 L1 models ...\n",
      "Fitting model: KNeighbors_BAG_L1 ...\n",
      "\t-20.5902\t = Validation score   (-mean_absolute_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.73s\t = Validation runtime\n",
      "Fitting model: LightGBMcustom2_BAG_L1 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-10.8928\t = Validation score   (-mean_absolute_error)\n",
      "\t25.4s\t = Training   runtime\n",
      "\t30.39s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-12.2812\t = Validation score   (-mean_absolute_error)\n",
      "\t224.38s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.2119\t = Validation score   (-mean_absolute_error)\n",
      "\t26.51s\t = Training   runtime\n",
      "\t0.66s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_custom_BAG_L1 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-13.1842\t = Validation score   (-mean_absolute_error)\n",
      "\t23.02s\t = Training   runtime\n",
      "\t0.42s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-11.4252\t = Validation score   (-mean_absolute_error)\n",
      "\t66.3s\t = Training   runtime\n",
      "\t0.49s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-10.5896\t = Validation score   (-mean_absolute_error)\n",
      "\t0.18s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 5 L2 models ...\n",
      "Fitting model: LightGBMcustom2_BAG_L2 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-9.9257\t = Validation score   (-mean_absolute_error)\n",
      "\t23.84s\t = Training   runtime\n",
      "\t25.82s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-10.9414\t = Validation score   (-mean_absolute_error)\n",
      "\t11.64s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-11.2648\t = Validation score   (-mean_absolute_error)\n",
      "\t28.08s\t = Training   runtime\n",
      "\t1.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_custom_BAG_L2 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-11.3337\t = Validation score   (-mean_absolute_error)\n",
      "\t23.27s\t = Training   runtime\n",
      "\t0.44s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
      "\tFitting 4 child models (S1F1 - S2F2) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-11.5588\t = Validation score   (-mean_absolute_error)\n",
      "\t31.63s\t = Training   runtime\n",
      "\t0.61s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ...\n",
      "\t-9.9257\t = Validation score   (-mean_absolute_error)\n",
      "\t0.16s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 517.91s ... Best model: \"WeightedEnsemble_L3\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"autogluon_models/shortnotebook_modelC_gluon/\")\n"
     ]
    }
   ],
   "source": [
    "def train_model(dataset):\n",
    "    # Define the path where the AutoGluon models will be saved\n",
    "    # enumerate all the locations\n",
    "    for index, location in enumerate(locations):\n",
    "        save_path = f\"autogluon_models/shortnotebook_model{location}_gluon\"\n",
    "\n",
    "        if location == \"A\":\n",
    "            hyperparameters = hyperparameters_a\n",
    "        else:\n",
    "            hyperparameters = hyperparameters_b_c\n",
    "\n",
    "        # Initialize the TabularPredictor object\n",
    "        model = TabularPredictor(\n",
    "            label=\"pv_measurement\", path=save_path, eval_metric=\"mae\"\n",
    "        )\n",
    "        model.fit(\n",
    "            train_data=combined_df_train[index].dropna(subset=[\"pv_measurement\"]),\n",
    "            tuning_data=combined_df_validation[index].dropna(subset=[\"pv_measurement\"]),\n",
    "            presets=\"experimental_zeroshot_hpo_hybrid\",\n",
    "            use_bag_holdout=True,\n",
    "            hyperparameters=hyperparameters,\n",
    "            # Got from the example in the docs\n",
    "            # time_limit=60*60*2, # It did not use up the time\n",
    "            num_bag_folds=2,\n",
    "            num_bag_sets=2,\n",
    "            num_stack_levels=1,\n",
    "        )\n",
    "\n",
    "\n",
    "def evaluate_model(X_val, y_val, location, model=None):\n",
    "    if model is None:\n",
    "        # If no model is passed, we assume the model has been previously saved and needs to be loaded\n",
    "        save_path = f\"autogluon_models_location_{location}\"\n",
    "        # model = TabularPredictor.load(save_path)\n",
    "        model = autogluon.multimodal(save_path)\n",
    "\n",
    "    # Predictions are made on the non-transformed validation data\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    # If y_val was transformed (e.g., log1p), then apply the inverse transformation\n",
    "    # y_val = np.expm1(y_val)\n",
    "    # y_pred = np.expm1(y_pred)\n",
    "\n",
    "    # Calculate the MAE\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    print(f\"Location {location}, Mean Absolute Error: {mae}\")\n",
    "\n",
    "\n",
    "# Train the model using all available training data and the initial validation set for early stopping\n",
    "train_model(combined_df_train)\n",
    "\n",
    "# Evaluate the model using the same validation set\n",
    "# evaluate_model(combined_X_val, combined_Y_val, location, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.leaderboard(combined_df_validation, silent=True)\n",
    "# model.feature_importance(combined_df_validation, time_limit=120)\n",
    "\n",
    "# predictior = TabularPredictor.load(f\"autogluon_models/test_modelA\", require_version_match=False)\n",
    "\n",
    "\n",
    "# # Get the leaderboard\n",
    "# print(\"leaderboard\")\n",
    "# leaderboard = predictior.leaderboard(silent=True)\n",
    "\n",
    "# # Retrieve the best model's name\n",
    "# best_model = leaderboard.iloc[0]['model']\n",
    "# print(\"Predictor info:\")\n",
    "# predictior.info()\n",
    "\n",
    "# # Retrieve hyperparameters of the best model\n",
    "# best_model_info = predictior.get_model_info(model=best_model)\n",
    "# best_hyperparameters = best_model_info['hyperparameters']\n",
    "\n",
    "# # Optionally, save or print these hyperparameters\n",
    "# print(\"Best Model Hyperparameters:\", best_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming you have defined WEATHER_FEATURES, TEST_COLUMNS_TO_KEEP, and other functions previously\n",
    "\n",
    "def make_predictions(df_test_pred, location):\n",
    "    eval_model = TabularPredictor.load(f\"autogluon_models/shortnotebook_model{location}_gluon\", require_version_match=False)\n",
    "    preds = eval_model.predict(df_test_pred)\n",
    "    return preds\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate locally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def evaluate_model_locally(location, scalers):\n",
    "    # Load the test data\n",
    "    target_df = pd.read_parquet(f\"data/{location}/train_targets.parquet\")\n",
    "    \n",
    "    # Make predictions\n",
    "    # filter x_validate to only include values from location\n",
    "    pred_dataset = combined_df_validation[combined_df_validation[\"location\"] == location]\n",
    "    pred_dataset.reset_index(inplace=True)\n",
    "\n",
    "    preds = make_predictions(pred_dataset.drop(\"pv_measurement\", axis=1), location)[-720:]\n",
    "    target = target_df.tail(720)[\"pv_measurement\"].to_numpy()\n",
    "    \n",
    "    differences = preds - target\n",
    "    # Count predictions lower than the actual\n",
    "    lower_predictions = (differences < 0) & (target != 0)\n",
    "    # Count predictions higher than the actual\n",
    "    higher_predictions = (differences > 0) & (target != 0)\n",
    "\n",
    "    # Biggest misreads\n",
    "    absolute_differences = abs(differences)\n",
    "    max_diff_index = absolute_differences.argmax()  # Index of the biggest difference\n",
    "    # max_diff_value = absolute_differences[max_diff_index]  # Value of the biggest difference\n",
    "    print(f\"Number of predictions that are a lower value than the actual, given that the actual is not 0: {lower_predictions.sum()}\")\n",
    "    print(f\"Number of predictions that are larger than the target, given that the target is not 0: {higher_predictions.sum()}\")\n",
    "    # print(f\"The biggest misread is at index {max_diff_index} with a difference of {max_diff_value}\")\n",
    "    \n",
    "    index = target_df.index[-720:]\n",
    "    print(f'location: {location}')\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(60,6))\n",
    "    plt.plot(index, target, label=\"Target\")\n",
    "    plt.plot(index, preds, label=\"Predictions\")\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Target vs Predictions')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "    plt.show()\n",
    "\n",
    "for loc in locations:\n",
    "    # evaluate_model_locally(loc, scalers)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit to csv 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "for index, location in enumerate(locations): \n",
    "    preds = make_predictions(combined_df_test[index], location)\n",
    "\n",
    "    # Assign the predictions to df_submission for the current location\n",
    "    mask = df_submission[\"location\"] == location\n",
    "    # Add a check to make sure the lengths match\n",
    "    if len(preds) != mask.sum():\n",
    "        raise ValueError(f\"Mismatch in length of predictions and submission entries for location {location}.\")\n",
    "\n",
    "    df_submission.loc[mask, \"prediction\"] = preds.to_numpy()\n",
    "\n",
    "\n",
    "# Save the results to a new submission file\n",
    "df_submission[[\"id\", \"prediction\"]].to_csv(\"Short-notebook-Autogluon-erik.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constans and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = [\"A\", \"B\", \"C\"]\n",
    "features_order = []\n",
    "A_B_ratio = 6.73\n",
    "A_C_ratio = 8.17\n",
    "\n",
    "LAGGED_COLUMNS_TO_KEEP = [\n",
    "    'direct_rad:W_lag_1h', \n",
    "    'direct_rad:W_lag_forward_1h', \n",
    "    'clear_sky_rad:W_lag_1h', \n",
    "    'clear_sky_rad:W_lag_forward_1h', \n",
    "    'diffuse_rad:W_lag_1h', \n",
    "    'diffuse_rad:W_lag_forward_1h', \n",
    "    'direct_rad_1h:J_lag_1h', \n",
    "    'direct_rad_1h:J_lag_forward_1h', \n",
    "    'is_in_shadow:idx_lag_1h', \n",
    "    'is_in_shadow:idx_lag_forward_1h', \n",
    "    'clear_sky_energy_1h:J_lag_1h', \n",
    "    'clear_sky_energy_1h:J_lag_forward_1h', \n",
    "    'effective_cloud_cover:p_lag_1h', \n",
    "    'effective_cloud_cover:p_lag_forward_1h', \n",
    "    'visibility:m_lag_1h', \n",
    "    'visibility:m_lag_forward_1h', \n",
    "    'total_cloud_cover:p_lag_1h', \n",
    "    'total_cloud_cover:p_lag_forward_1h', \n",
    "\n",
    "\n",
    "    # 'direct_rad:W_lag_2h', \n",
    "    # 'direct_rad:W_lag_forward_2h', \n",
    "    # 'clear_sky_rad:W_lag_2h', \n",
    "    # 'clear_sky_rad:W_lag_forward_2h', \n",
    "    # 'diffuse_rad:W_lag_2h', \n",
    "    # 'diffuse_rad:W_lag_forward_2h', \n",
    "    # 'direct_rad_1h:J_lag_2h', \n",
    "    # 'direct_rad_1h:J_lag_forward_2h', \n",
    "    # 'is_in_shadow:idx_lag_2h', \n",
    "    # 'is_in_shadow:idx_lag_forward_2h', \n",
    "    # 'clear_sky_energy_1h:J_lag_2h', \n",
    "    # 'clear_sky_energy_1h:J_lag_forward_2h', \n",
    "    # 'effective_cloud_cover:p_lag_2h', \n",
    "    # 'effective_cloud_cover:p_lag_forward_2h', \n",
    "    # 'visibility:m_lag_2h', \n",
    "    # 'visibility:m_lag_forward_2h', \n",
    "    # 'total_cloud_cover:p_lag_2h', \n",
    "    # 'total_cloud_cover:p_lag_forward_2h', \n",
    "\n",
    "    # 'direct_rad:W_lag_3h', \n",
    "    # 'direct_rad:W_lag_forward_3h', \n",
    "    # 'clear_sky_rad:W_lag_3h', \n",
    "    # 'clear_sky_rad:W_lag_forward_3h', \n",
    "    # 'diffuse_rad:W_lag_3h', \n",
    "    # 'diffuse_rad:W_lag_forward_3h', \n",
    "    # 'direct_rad_1h:J_lag_3h', \n",
    "    # 'direct_rad_1h:J_lag_forward_3h', \n",
    "    # 'is_in_shadow:idx_lag_3h', \n",
    "    # 'is_in_shadow:idx_lag_forward_3h', \n",
    "    # 'clear_sky_energy_1h:J_lag_3h', \n",
    "    # 'clear_sky_energy_1h:J_lag_forward_3h', \n",
    "    # 'effective_cloud_cover:p_lag_3h', \n",
    "    # 'effective_cloud_cover:p_lag_forward_3h', \n",
    "    # 'visibility:m_lag_3h', \n",
    "    # 'visibility:m_lag_forward_3h', \n",
    "    # 'total_cloud_cover:p_lag_3h', \n",
    "    # 'total_cloud_cover:p_lag_forward_3h'\n",
    "]\n",
    "\n",
    "CUSTOM_COLUMNS_TO_KEEP = [\n",
    "    \"hour_cos\",\n",
    "    \"hour_sin\",\n",
    "    \"month_sin\",\n",
    "    \"month_cos\",\n",
    "    \"day-of-year\",\n",
    "    # \"hours_since_forecast\",\n",
    "    # \"sun_azimuth_cos\",\n",
    "    # \"sun_azimuth_sin\",\n",
    "]\n",
    "\n",
    "WEATHER_FEATURES = [\n",
    "    \"direct_rad:W\",\n",
    "    \"clear_sky_rad:W\",\n",
    "    \"diffuse_rad:W\",\n",
    "    \"direct_rad_1h:J\",\n",
    "    \"is_in_shadow:idx\",\n",
    "    \"clear_sky_energy_1h:J\",\n",
    "    \"effective_cloud_cover:p\",\n",
    "    \"visibility:m\",\n",
    "    \"total_cloud_cover:p\",\n",
    "]\n",
    "\n",
    "\n",
    "TEST_COLUMNS_TO_KEEP = [\n",
    "    \"direct_rad:W\",\n",
    "    \"clear_sky_rad:W\",\n",
    "    \"diffuse_rad:W\",\n",
    "    \"direct_rad_1h:J\",\n",
    "    \"is_in_shadow:idx\",\n",
    "    \"clear_sky_energy_1h:J\",\n",
    "    \"diffuse_rad_1h:J\",\n",
    "    \"is_day:idx\",\n",
    "    \"sun_elevation:d\",\n",
    "    \"ceiling_height_agl:m\",\n",
    "    \"effective_cloud_cover:p\",\n",
    "    \"visibility:m\",\n",
    "    \"total_cloud_cover:p\",\n",
    "    \"air_density_2m:kgm3\",\n",
    "    \"wind_speed_v_10m:ms\",\n",
    "    \"dew_point_2m:K\",\n",
    "    \"wind_speed_u_10m:ms\",\n",
    "    \"t_1000hPa:K\",\n",
    "    \"absolute_humidity_2m:gm3\",\n",
    "    \"snow_water:kgm2\",\n",
    "    \"relative_humidity_1000hPa:p\",\n",
    "    \"fresh_snow_24h:cm\",\n",
    "    \"cloud_base_agl:m\",\n",
    "    \"fresh_snow_12h:cm\",\n",
    "    \"snow_depth:cm\",\n",
    "    \"dew_or_rime:idx\",\n",
    "    \"fresh_snow_6h:cm\",\n",
    "    \"super_cooled_liquid_water:kgm2\",\n",
    "    \"fresh_snow_3h:cm\",\n",
    "    \"rain_water:kgm2\",\n",
    "    \"precip_type_5min:idx\",\n",
    "    \"precip_5min:mm\",\n",
    "    \"fresh_snow_1h:cm\",\n",
    "    \"sun_azimuth:d\",\n",
    "    \"msl_pressure:hPa\",\n",
    "    \"pressure_100m:hPa\",\n",
    "    \"pressure_50m:hPa\",\n",
    "    \"sfc_pressure:hPa\",\n",
    "    \"prob_rime:p\",\n",
    "    \"wind_speed_10m:ms\",\n",
    "    \"elevation:m\",\n",
    "    # \"snow_density:kgm3\",\n",
    "    \"snow_drift:idx\",\n",
    "    \"snow_melt_10min:mm\",\n",
    "    \"wind_speed_w_1000hPa:ms\",\n",
    "    \"observed_or_estimated\",\n",
    "    # \"location_A\",\n",
    "    # \"location_B\",\n",
    "    # \"location_C\",\n",
    "    # \"date_calc\",\n",
    "] + CUSTOM_COLUMNS_TO_KEEP  +  LAGGED_COLUMNS_TO_KEEP\n",
    "\n",
    "COLUMNS_TO_KEEP = TEST_COLUMNS_TO_KEEP + [\"pv_measurement\"]\n",
    "\n",
    "\n",
    "\n",
    "def create_weather_lagged_features(df, weather_features):\n",
    "    # Choose the weather features for which you want to create lagged versions\n",
    "    for feature in weather_features:\n",
    "        # Assuming hourly data, adjust the lags for your specific dataset\n",
    "        # Creating lagged features for 1 hour, 1 day, and 1 week\n",
    "        df[f'{feature}_lag_1h'] = df[feature].shift(1)\n",
    "        # df[f'{feature}_lag_2h'] = df[feature].shift(2)\n",
    "        # df[f'{feature}_lag_3h'] = df[feature].shift(3)\n",
    "\n",
    "        df[f'{feature}_lag_forward_1h'] = df[feature].shift(-1)\n",
    "        # df[f'{feature}_lag_forward_2h'] = df[feature].shift(-2)\n",
    "        # df[f'{feature}_lag_forward_3h'] = df[feature].shift(-3)\n",
    "        # df[f'{feature}_lag_24h'] = df[feature].shift(24*4)\n",
    "        # df[f'{feature}_lag_168h'] = df[feature].shift(24 * 7 * 4 * 365)\n",
    "        # df[f'{feature}_front_lag_1h'] = df[feature].shift(-4)\n",
    "        # df[f'{feature}_front_lag_24h'] = df[feature].shift(-24*4)\n",
    "\n",
    "\n",
    "    # Handling edges by filling NaNs with appropriate values or dropping them\n",
    "    # You may choose to fill with zeroes or interpolate, based on what makes more sense for your data\n",
    "    df.fillna(method='ffill', inplace=True)  # Forward fill\n",
    "    df.fillna(method='bfill', inplace=True)  # Backward fill\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_lagged_features(df, column_name='pv_measurement'):\n",
    "    # Assuming 'date_forecast' is the datetime column used for sorting\n",
    "\n",
    "    df[f'{column_name}_prev_month'] = df[column_name].shift(24*7) # previous week\n",
    "\n",
    "    # For yearly lag, you would need to calculate the number of observations per year\n",
    "    # If the data is not consistent (leap years, etc.), you may need a more complex method\n",
    "    # Here's a simple version assuming 365 days a year:\n",
    "    df[f'{column_name}_prev_year'] = df[column_name].shift(24*365) # previous year\n",
    "    df[f'{column_name}_2years_ago'] = df[column_name].shift(24*365*2) # next year\n",
    "\n",
    "    # Handling edges by filling NaNs with appropriate values or dropping them\n",
    "    df.fillna(method='ffill', inplace=True)  # Forward fill\n",
    "    df.fillna(method='bfill', inplace=True)  # Backward fill\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_custom_fields(df):\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['date_forecast'].dt.hour / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['date_forecast'].dt.hour / 24)\n",
    "\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['date_forecast'].dt.month / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['date_forecast'].dt.month / 12)\n",
    "    df['day-of-year'] = df['date_forecast'].dt.dayofyear\n",
    "\n",
    "    # df['sun_azimuth_cos'] = np.cos(np.radians(df['sun_azimuth:d']))\n",
    "    # df['sun_azimuth_sin'] = np.sin(np.radians(df['sun_azimuth:d']))\n",
    "    # df.drop(columns=['sun_azimuth:d'], inplace=True)\n",
    "   \n",
    "    return df\n",
    "\n",
    "def add_calc_date(df_observed, df_estimated, df_test):\n",
    "    # Function to calculate the difference in hours\n",
    "    def calculate_hour_difference(row):\n",
    "        diff = row['date_calc'] - row['date_forecast']\n",
    "        return diff.total_seconds() / 3600  # Convert difference to hours\n",
    "\n",
    "    # Apply the function to calculate the hour difference for df_estimated and df_test\n",
    "    df_estimated['hours_since_forecast'] = df_estimated.apply(calculate_hour_difference, axis=1)\n",
    "    df_test['hours_since_forecast'] = df_test.apply(calculate_hour_difference, axis=1)\n",
    "\n",
    "    # Fill in zero for df_observed\n",
    "    df_observed['hours_since_forecast'] = 0\n",
    "\n",
    "    return df_observed, df_estimated, df_test\n",
    "\n",
    "\n",
    "def remove_outliers(df):\n",
    "    # Use a mask to filter out the rows where rolling std is zero but keep the rows where the value itself is zero\n",
    "    mask = (df['pv_measurement'].rolling(5).std() == 0) & (df['pv_measurement'] != 0)\n",
    "    df = df[~mask]\n",
    "    return df\n",
    "\n",
    "def resample_add_data(df, is_test_data):\n",
    "    df = add_custom_fields(df)\n",
    "    df.set_index('date_forecast', inplace=True)\n",
    "    df = df.resample('1H').mean()\n",
    "    \n",
    "    # Remove empty dates if test data\n",
    "    if is_test_data:\n",
    "        non_nan_threshold = len(df.columns) // 2  \n",
    "        df.dropna(thresh=non_nan_threshold, inplace=True)\n",
    "    df.interpolate(method=\"linear\", inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_location_feature(X, location):\n",
    "      # Treat location as a categorical feature by converting it to a category type\n",
    "    \n",
    "    # bins = [-90, 0, 45, 90]\n",
    "    # labels = ['Low', 'Medium', 'High']\n",
    "    # X['sun_elevation:d'] = pd.cut(X['sun_elevation:d'], bins=bins, labels=labels, include_lowest=True)\n",
    "    X['location'] = location\n",
    "    X['location'] = X['location'].astype(str)  # Convert to string if 'location' is not an int\n",
    "    X['dew_or_rime:idx'] = X['dew_or_rime:idx'].astype(str)\n",
    "    X['is_day:idx'] = X['is_day:idx'].astype(str)\n",
    "    X['is_in_shadow:idx'] = X['is_in_shadow:idx'].astype(str)\n",
    "    categorical_columns = ['location', 'dew_or_rime:idx', 'is_day:idx', 'is_in_shadow:idx']\n",
    "\n",
    "    # Before filling NaN values, add 'missing' as a category for each categorical column.\n",
    "    for column in categorical_columns:\n",
    "        X[column] = X[column].astype('category')  # Ensure the column is of type 'category'.\n",
    "        if 'missing' not in X[column].cat.categories:\n",
    "            X[column] = X[column].cat.add_categories(['missing'])  # Add 'missing' as a new category.\n",
    "        X[column] = X[column].fillna('missing')\n",
    "    X['location'] = X['location'].astype('category')\n",
    "    X['dew_or_rime:idx'] = X['dew_or_rime:idx'].astype('category')\n",
    "    X['is_day:idx'] = X['is_day:idx'].astype('category')\n",
    "    X['is_in_shadow:idx'] = X['is_in_shadow:idx'].astype('category')\n",
    "    # X['sun_elevation:d'] = X['sun_elevation:d'].astype('category')\n",
    "    \n",
    "    return X\n",
    "\n",
    "def add_scaling(X_test, X_training, location):\n",
    "    global scalers\n",
    "    continuous_columns = X_training.select_dtypes(include=['float32', 'int32']).columns\n",
    "    if location not in scalers:\n",
    "        scalers[location] = MinMaxScaler()\n",
    "    X_training[continuous_columns] = scalers[location].fit_transform(X_training[continuous_columns])\n",
    "    \n",
    "    X_test[continuous_columns] = scalers[location].transform(X_test[continuous_columns])\n",
    "\n",
    "    return X_test, X_training\n",
    "\n",
    "def make_observed_and_estimated_category(df_observed, df_estimated, df_test):\n",
    "     # Hot encode in wether observed or estimated\n",
    "    df_observed['observed_or_estimated'] = 'observed'\n",
    "    df_estimated['observed_or_estimated'] = 'estimated'\n",
    "    df_test['observed_or_estimated'] = 'estimated'\n",
    "    # Concatenate observed and estimated\n",
    "    df_training = pd.concat([df_observed, df_estimated], axis=0).sort_values(by=\"date_forecast\")\n",
    "    df_training['observed_or_estimated'] = df_training['observed_or_estimated'].astype('category')\n",
    "    df_test['observed_or_estimated'] = df_test['observed_or_estimated'].astype('category')\n",
    "\n",
    "    return df_training, df_test\n",
    "\n",
    "def analyze_pv_measurements(dataframe):\n",
    "    # Filter the DataFrame for rows where 'is_day' is 0\n",
    "    night_data = dataframe[dataframe['is_day:idx'] == 0]\n",
    "\n",
    "    # Analysis of 'pv_measurements' during night time\n",
    "    pv_stats = night_data['pv_measurement'].describe()\n",
    "\n",
    "    return pv_stats\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_101452/4272354047.py:40: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.resample('1H').mean()\n",
      "/var/tmp/ipykernel_101452/4272354047.py:40: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.resample('1H').mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    15976.000000\n",
      "mean         0.146713\n",
      "std          3.232574\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max        206.580000\n",
      "Name: pv_measurement, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_101452/4272354047.py:40: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.resample('1H').mean()\n",
      "/var/tmp/ipykernel_101452/4272354047.py:40: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.resample('1H').mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    14821.000000\n",
      "mean         0.257302\n",
      "std          6.118606\n",
      "min         -0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max        273.412500\n",
      "Name: pv_measurement, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_101452/4272354047.py:40: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.resample('1H').mean()\n",
      "/var/tmp/ipykernel_101452/4272354047.py:40: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.resample('1H').mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    13238.000000\n",
      "mean         0.088243\n",
      "std          2.489824\n",
      "min         -0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max        137.200000\n",
      "Name: pv_measurement, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_101452/4272354047.py:40: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.resample('1H').mean()\n",
      "/var/tmp/ipykernel_101452/4272354047.py:40: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.resample('1H').mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    15976.000000\n",
      "mean         0.146713\n",
      "std          3.232574\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max        206.580000\n",
      "Name: pv_measurement, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_101452/4272354047.py:40: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.resample('1H').mean()\n",
      "/var/tmp/ipykernel_101452/4272354047.py:40: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.resample('1H').mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    14821.000000\n",
      "mean         0.257302\n",
      "std          6.118606\n",
      "min         -0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max        273.412500\n",
      "Name: pv_measurement, dtype: float64\n",
      "count    13238.000000\n",
      "mean         0.088243\n",
      "std          2.489824\n",
      "min         -0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max        137.200000\n",
      "Name: pv_measurement, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_101452/4272354047.py:40: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.resample('1H').mean()\n",
      "/var/tmp/ipykernel_101452/4272354047.py:40: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df = df.resample('1H').mean()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize a dictionary to hold the scalers for each location\n",
    "\n",
    "scalers = {}\n",
    "scaling_target = False\n",
    "\n",
    "def prepare_data(location):\n",
    "    # Load data\n",
    "\n",
    "    scaling = True  # Set scaling to True to enable individual scaling for each location\n",
    "    global scalers\n",
    "    global scaling_target\n",
    "    # Load training data\n",
    "    df_observed = pd.read_parquet(f\"data/{location}/X_train_observed.parquet\")\n",
    "    df_estimated = pd.read_parquet(f\"data/{location}/X_train_estimated.parquet\")\n",
    "    df_target = pd.read_parquet(f\"data/{location}/train_targets.parquet\")\n",
    "\n",
    "\n",
    "    # Load test data\n",
    "    df_test = pd.read_parquet(f\"data/{location}/X_test_estimated.parquet\")\n",
    "\n",
    "    # Add calculated date\n",
    "    df_observed, df_estimated, df_test = add_calc_date(df_observed, df_estimated, df_test)\n",
    "    # Hot encode in wether observed or estimated\n",
    "\n",
    "    # Resample and add custom fields\n",
    "\n",
    "    df_observed = resample_add_data(df_observed, False)\n",
    "    df_estimated = resample_add_data(df_estimated, False)\n",
    "    df_test = resample_add_data(df_test, True)\n",
    "\n",
    "    df_training, df_test = make_observed_and_estimated_category(df_observed, df_estimated, df_test)\n",
    "\n",
    "    # Merge training with target data\n",
    "    df_training = pd.merge(df_training, df_target, left_on=\"date_forecast\", right_on=\"time\", how=\"inner\")\n",
    "    \n",
    "    # Create lagged features and remove outliers training\n",
    "    df_training = create_weather_lagged_features(df_training, WEATHER_FEATURES)\n",
    "    df_training = df_training[COLUMNS_TO_KEEP]\n",
    "    df_training = remove_outliers(df_training)\n",
    "\n",
    "    # Create lagged features test\n",
    "    df_test = create_weather_lagged_features(df_test, WEATHER_FEATURES)\n",
    "    df_test = df_test[TEST_COLUMNS_TO_KEEP]\n",
    "\n",
    "    print(analyze_pv_measurements(df_training))\n",
    "    # Make training x and y\n",
    "    y_training = df_training[\"pv_measurement\"]\n",
    "    X_training = df_training.drop(\"pv_measurement\", axis=1)\n",
    "\n",
    "    # Add categories\n",
    "    X_training = add_location_feature(X_training, location)\n",
    "    X_test = add_location_feature(df_test, location)\n",
    "    \n",
    "    # Add scaling\n",
    "    if scaling:\n",
    "        X_test, X_training = add_scaling(X_test, X_training, location)\n",
    "    X_test.reset_index(inplace=True)\n",
    "    X_test.drop(\"date_forecast\", axis=1, inplace=True)\n",
    "    # y_training = np.log1p(y_training)\n",
    "    if scaling_target:\n",
    "        if location == \"B\": \n",
    "            y_training = A_B_ratio*(y_training)\n",
    "        if location == \"C\":\n",
    "            y_training = A_C_ratio*(y_training)\n",
    "    return X_training, X_test, y_training\n",
    "\n",
    "\n",
    "\n",
    "# Use prepare_data function\n",
    "\n",
    "combined_X_train = pd.DataFrame()\n",
    "combined_X_val = pd.DataFrame()\n",
    "combined_Y_train = pd.DataFrame()\n",
    "combined_Y_val = pd.DataFrame()\n",
    "combined_X_test = pd.DataFrame()\n",
    "\n",
    "# For validation locally \n",
    "for location in locations:\n",
    "    # Prepare the training data\n",
    "    X_training, X_test, y_training = prepare_data(location)\n",
    "    \n",
    "    # Split and concatenate the training data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_training, y_training, test_size=0.0001, random_state=42)\n",
    "    combined_X_train = pd.concat([combined_X_train, X_train])\n",
    "    combined_X_val = pd.concat([combined_X_val, X_val])\n",
    "    combined_Y_train = pd.concat([combined_Y_train, y_train])\n",
    "    combined_Y_val = pd.concat([combined_Y_val, y_val])\n",
    "    combined_X_test = pd.concat([combined_X_test, X_test])\n",
    "\n",
    "    # Spl\n",
    "\n",
    "combined_X_test\n",
    "combined_X_train, combined_Y_train = shuffle(combined_X_train, combined_Y_train, random_state=42)\n",
    "combined_X_val, combined_Y_val = shuffle(combined_X_val, combined_Y_val, random_state=42)\n",
    "\n",
    "for location in locations: \n",
    "    prepare_data(location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 291.1747725\ttest: 748.0749860\tbest: 748.0749860 (0)\ttotal: 264ms\tremaining: 26m 24s\n",
      "200:\tlearn: 120.4355877\ttest: 340.3530055\tbest: 340.3530055 (200)\ttotal: 39.1s\tremaining: 18m 47s\n",
      "400:\tlearn: 87.9791543\ttest: 258.5880338\tbest: 258.4648335 (399)\ttotal: 1m 17s\tremaining: 18m 2s\n",
      "600:\tlearn: 77.9862214\ttest: 231.5580906\tbest: 231.5580906 (600)\ttotal: 1m 56s\tremaining: 17m 23s\n",
      "800:\tlearn: 73.9952100\ttest: 219.2543227\tbest: 219.2543227 (800)\ttotal: 2m 34s\tremaining: 16m 42s\n",
      "1000:\tlearn: 70.9904915\ttest: 214.3781165\tbest: 214.3781165 (1000)\ttotal: 3m 13s\tremaining: 16m 7s\n",
      "1200:\tlearn: 69.7845306\ttest: 214.7317033\tbest: 214.0832978 (1128)\ttotal: 3m 52s\tremaining: 15m 30s\n",
      "1400:\tlearn: 68.8523475\ttest: 214.5354891\tbest: 214.0832978 (1128)\ttotal: 4m 31s\tremaining: 14m 50s\n",
      "1600:\tlearn: 67.5184148\ttest: 213.3025232\tbest: 213.2961239 (1598)\ttotal: 5m 10s\tremaining: 14m 11s\n",
      "1800:\tlearn: 65.9367814\ttest: 212.6226062\tbest: 212.5849724 (1799)\ttotal: 5m 49s\tremaining: 13m 34s\n",
      "2000:\tlearn: 64.5477038\ttest: 210.6184980\tbest: 210.5779418 (1984)\ttotal: 6m 27s\tremaining: 12m 55s\n",
      "2200:\tlearn: 63.5823349\ttest: 210.5102013\tbest: 210.4371593 (2171)\ttotal: 7m 6s\tremaining: 12m 16s\n",
      "2400:\tlearn: 62.4426302\ttest: 210.0582024\tbest: 210.0259646 (2396)\ttotal: 7m 45s\tremaining: 11m 37s\n",
      "2600:\tlearn: 61.2460544\ttest: 209.0466412\tbest: 209.0346514 (2599)\ttotal: 8m 24s\tremaining: 10m 59s\n",
      "2800:\tlearn: 60.3771031\ttest: 208.4823475\tbest: 208.3892060 (2681)\ttotal: 9m 3s\tremaining: 10m 20s\n",
      "3000:\tlearn: 59.2792646\ttest: 208.5747306\tbest: 208.3892060 (2681)\ttotal: 9m 42s\tremaining: 9m 41s\n",
      "3200:\tlearn: 58.1997882\ttest: 207.5298793\tbest: 207.5208626 (3197)\ttotal: 10m 21s\tremaining: 9m 3s\n",
      "3400:\tlearn: 57.1938073\ttest: 207.3101884\tbest: 206.9104722 (3348)\ttotal: 11m\tremaining: 8m 24s\n",
      "3600:\tlearn: 56.0766838\ttest: 205.8996209\tbest: 205.8763439 (3596)\ttotal: 11m 39s\tremaining: 7m 45s\n",
      "3800:\tlearn: 55.0455358\ttest: 204.6698232\tbest: 204.3385891 (3796)\ttotal: 12m 17s\tremaining: 7m 6s\n",
      "4000:\tlearn: 54.1956812\ttest: 204.9475649\tbest: 204.3385891 (3796)\ttotal: 12m 56s\tremaining: 6m 28s\n",
      "4200:\tlearn: 53.3266474\ttest: 204.2184840\tbest: 204.1772173 (4177)\ttotal: 13m 35s\tremaining: 5m 49s\n",
      "4400:\tlearn: 52.5834212\ttest: 203.6567340\tbest: 203.6356193 (4356)\ttotal: 14m 14s\tremaining: 5m 10s\n",
      "4600:\tlearn: 51.8641305\ttest: 203.6481464\tbest: 203.0538063 (4536)\ttotal: 14m 52s\tremaining: 4m 31s\n",
      "4800:\tlearn: 51.2328246\ttest: 203.5731603\tbest: 203.0538063 (4536)\ttotal: 15m 31s\tremaining: 3m 52s\n",
      "5000:\tlearn: 50.5869984\ttest: 203.4510246\tbest: 203.0538063 (4536)\ttotal: 16m 10s\tremaining: 3m 13s\n",
      "5200:\tlearn: 50.2631691\ttest: 203.2469832\tbest: 203.0538063 (4536)\ttotal: 16m 49s\tremaining: 2m 35s\n",
      "5400:\tlearn: 50.0617474\ttest: 203.2767836\tbest: 203.0538063 (4536)\ttotal: 17m 27s\tremaining: 1m 56s\n",
      "5600:\tlearn: 49.8364993\ttest: 203.0269690\tbest: 202.9809362 (5505)\ttotal: 18m 6s\tremaining: 1m 17s\n",
      "5800:\tlearn: 49.6145011\ttest: 203.1481166\tbest: 202.9204233 (5721)\ttotal: 18m 44s\tremaining: 38.6s\n",
      "5999:\tlearn: 49.3233244\ttest: 203.4878996\tbest: 202.9204233 (5721)\ttotal: 19m 23s\tremaining: 0us\n",
      "\n",
      "bestTest = 202.9204233\n",
      "bestIteration = 5721\n",
      "\n",
      "Location C, Mean Absolute Error: 10.931930181606651\n",
      "Location A, Mean Absolute Error: 496.5297191197997\n",
      "Location B, Mean Absolute Error: 5.321513121540725\n"
     ]
    }
   ],
   "source": [
    "def train_model(X_train, y_train, X_val, y_val, location):\n",
    "    cat_features = [index for index, col in enumerate(X_train.columns) if col in ['location', 'dew_or_rime:idx', 'is_day:idx', 'is_in_shadow:idx', 'observed_or_estimated']]\n",
    "    \n",
    "    model = CatBoostRegressor(\n",
    "        iterations=6000,\n",
    "        learning_rate=0.007,\n",
    "        depth=12,  # assuming you decided to keep the depth reduced\n",
    "        loss_function='MAE',\n",
    "        verbose=200,\n",
    "        # per_float_feature_quantization='65:border_count=1024',\n",
    "        cat_features=cat_features,\n",
    "    )\n",
    "\n",
    "    # Use the provided validation set for early stopping\n",
    "    \n",
    "    model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=False,)\n",
    "    model.save_model(f\"catboost_model_merged.cbm\")\n",
    "\n",
    "def evaluate_model(X_val, y_val, location):\n",
    "    global scaling_target  \n",
    "    model = CatBoostRegressor()\n",
    "    model.load_model(f\"catboost_model_merged.cbm\")\n",
    "    \n",
    "    # Unique locations in the X_val dataframe\n",
    "    locations = X_val['location'].unique()\n",
    "    \n",
    "    for location in locations:\n",
    "        # Filter X_val and y_val for the current location\n",
    "        X_val_loc = X_val[X_val['location'] == location]\n",
    "        y_val_loc = y_val[X_val['location'] == location]\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred_loc = model.predict(X_val_loc)\n",
    "        \n",
    "        # Apply the transformation if needed (assuming y_val_loc and y_pred_loc are log1p transformed)\n",
    "        # y_val_loc = np.expm1(y_val_loc)\n",
    "        # y_pred_loc = np.expm1(y_pred_loc)\n",
    "        \n",
    "        # Calculate MAE for the current location\n",
    "        mae = mean_absolute_error(y_val_loc, y_pred_loc)\n",
    "\n",
    "        if scaling_target: \n",
    "            if location == \"B\": \n",
    "                mae = mae/A_B_ratio\n",
    "            if location == \"C\":\n",
    "                mae = mae/A_C_ratio\n",
    "                \n",
    "        print(f'Location {location}, Mean Absolute Error: {mae}')\n",
    "\n",
    "\n",
    "# Train the model using all available training data and the initial validation set for early stopping\n",
    "train_model(combined_X_train, combined_Y_train, combined_X_val, combined_Y_val, location)\n",
    "# Evaluate the model using the same validation set\n",
    "evaluate_model(combined_X_val, combined_Y_val, location)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance():\n",
    "    model = CatBoostRegressor()\n",
    "    model.load_model(f\"catboost_model_merged.cbm\")\n",
    "\n",
    "    # Getting feature importances\n",
    "    feature_importances = model.get_feature_importance()\n",
    "    feature_names = model.feature_names_\n",
    "\n",
    "    # Creating a DataFrame from feature importances\n",
    "    df_feature_importances = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importances\n",
    "    })\n",
    "\n",
    "    # Sorting the DataFrame by importance in descending order\n",
    "    df_feature_importances = df_feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    return df_feature_importances\n",
    "\n",
    "\n",
    "# feature_importances = feature_importance()\n",
    "# Calling the function\n",
    "# print(feature_importances.head(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming you have defined WEATHER_FEATURES, TEST_COLUMNS_TO_KEEP, and other functions previously\n",
    "\n",
    "def make_predictions(df_test_pred):\n",
    "    model = CatBoostRegressor()\n",
    "    model.load_model(f\"catboost_model_merged.cbm\")\n",
    "    # Load model \n",
    "    model = CatBoostRegressor()\n",
    "    model.load_model(f\"catboost_model_merged.cbm\")\n",
    "    \n",
    "    preds = model.predict(df_test_pred)\n",
    "    \n",
    "    # Inverse transform the predictions\n",
    "    # preds = np.expm1(preds)\n",
    "    return preds\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate locally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model_locally(location, X_val_loc, y_val_loc):\n",
    "    # Load the test data\n",
    "    global scaling_target\n",
    "    \n",
    "    # Make predictions\n",
    "    preds = make_predictions(X_val_loc)[-200:]\n",
    "    target = y_val_loc.values[-200:]\n",
    "    if scaling_target:\n",
    "        if location == \"B\": \n",
    "            preds = (preds)/A_B_ratio\n",
    "        \n",
    "        if location == \"C\":\n",
    "            preds = (preds)/A_C_ratio\n",
    "\n",
    "    # differences = preds - target\n",
    "\n",
    "    # # Count predictions lower than the actual\n",
    "    # lower_predictions = (differences < 0) & (target != 0)\n",
    "    # # Count predictions higher than the actual\n",
    "    # higher_predictions = (differences > 0) & (target != 0)\n",
    "\n",
    "    # # Biggest misreads\n",
    "    # absolute_differences = abs(differences)\n",
    "    # max_diff_index = absolute_differences.argmax()  # Index of the biggest difference\n",
    "    # max_diff_value = absolute_differences[max_diff_index]  # Value of the biggest difference\n",
    "    # print(f\"Number of predictions that are a lower value than the actual, given that the actual is not 0: {lower_predictions.sum()}\")\n",
    "    # print(f\"Number of predictions that are larger than the target, given that the target is not 0: {higher_predictions.sum()}\")\n",
    "    # print(f\"The biggest misread is at index {max_diff_index} with a difference of {max_diff_value}\")\n",
    "\n",
    "    # write preds to a csv file\n",
    "    # df = pd.DataFrame(preds)\n",
    "    # df.to_csv(f\"best_preds_{location}.csv\")\n",
    "    # make array of indices of numpy array target \n",
    "\n",
    "    # plot the preds from the csv file \"best_preds_{location}.csv\"\n",
    "    df = pd.read_csv(f\"best_preds_{location}.csv\")[-200:]\n",
    "    # df.reset_index(inplace=True)\n",
    "    best_preds = df[\"0\"]\n",
    "    print(preds)\n",
    "    index = np.arange(len(target))\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(60,6))\n",
    "    plt.plot(index, target, label=\"Target\")\n",
    "    plt.plot(index, preds, label=\"Predictions\")\n",
    "    plt.plot(index, best_preds.values, label=\"Best predictions\")\n",
    "\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Target vs Predictions')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "    plt.show()\n",
    "\n",
    "for location in locations:\n",
    "    X_val_loc = combined_X_val[combined_X_val['location'] == location].sort_index()\n",
    "    y_val_loc = combined_Y_val[combined_X_val['location'] == location].sort_index()\n",
    "    # evaluate_model_locally(location, X_val_loc, y_val_loc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit to csv 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160\n",
      "2160\n"
     ]
    }
   ],
   "source": [
    "df_submission = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "preds = make_predictions(combined_X_test)\n",
    "print(len(preds))\n",
    "print(len(df_submission))\n",
    "df_submission[\"prediction\"] = preds\n",
    "\n",
    "# Save the results to a new submission file\n",
    "df_submission[[\"id\", \"prediction\"]].to_csv(\"Short-notebook-Catboost-145.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensamble average Autogluon and Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "df1 = pd.read_csv(\"Short-notebook-Catboost-145.csv\")\n",
    "df2 = pd.read_csv(\"Short-notebook-Autogluon-erik.csv\")\n",
    "# Verify that the IDs match in all DataFrames (assuming they are sorted and have the same length)\n",
    "# if not (df1['id'].equals(df2['id']) and df1['id'].equals(df3['id']) and df1['id']):\n",
    "#     raise ValueError(\"The IDs in the CSV files do not match.\")\n",
    "\n",
    "# Calculate the average of the predictions for all rows\n",
    "df_average = pd.DataFrame({\n",
    "    'id': df1['id'],\n",
    "    'prediction': (df1['prediction'] + df2['prediction'])/2\n",
    "})\n",
    "\n",
    "df = df_average\n",
    "\n",
    "# Ensure there are no negative predictions\n",
    "df['prediction'] = df['prediction'].apply(lambda x: max(x, 0))\n",
    "\n",
    "df['prediction'] = df['prediction'].apply(lambda x: 0 if x < 1.5 else x)\n",
    "\n",
    "\n",
    "# Set predictions to zero where up to three non-zero predictions are surrounded by zeros\n",
    "for i in range(1, len(df) - 1):\n",
    "    # Check single non-zero prediction surrounded by zeros\n",
    "    if df.loc[i - 1, 'prediction'] == 0 and df.loc[i + 1, 'prediction'] == 0:\n",
    "        df.loc[i, 'prediction'] = 0\n",
    "    # Check two consecutive non-zero predictions surrounded by zeros\n",
    "    if i < len(df) - 2 and df.loc[i - 1, 'prediction'] == 0 and df.loc[i + 2, 'prediction'] == 0:\n",
    "        df.loc[i, 'prediction'] = 0\n",
    "        df.loc[i + 1, 'prediction'] = 0\n",
    "    # Check three consecutive non-zero predictions surrounded by zeros\n",
    "\n",
    "# Save the averaged predictions to a new CSV file\n",
    "df.to_csv('predictions/short_notebook_cat_autogluon.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
